{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:09:44.313156Z",
     "start_time": "2017-06-16T19:09:44.307461Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T18:43:03.931476Z",
     "start_time": "2017-06-16T18:43:03.772569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "\n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T18:43:03.938649Z",
     "start_time": "2017-06-16T18:43:03.933104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T18:43:03.949810Z",
     "start_time": "2017-06-16T18:43:03.940168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T18:43:04.653934Z",
     "start_time": "2017-06-16T18:43:03.951239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 122)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "preprocess.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T18:43:05.739437Z",
     "start_time": "2017-06-16T18:43:04.655492Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T18:43:05.925845Z",
     "start_time": "2017-06-16T18:43:05.741108Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 18\n",
    "\n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "            #hidden_encoder = tf.layers.dense(self.x, latent_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            #hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(hidden_encoder, classes, activation=tf.nn.softmax)\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-1, 1e-1)\n",
    "            #loss = tf.where(tf.is_nan(loss), 1e-1, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-1), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-1), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = loss\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T18:43:06.156898Z",
     "start_time": "2017-06-16T18:43:05.927489Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'test_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "    \n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_dense_only_nsl_kdd/hidden layers_{}_features count_{}\".format(epochs,h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        def train_batch():\n",
    "                            nonlocal train_loss\n",
    "                            _, train_loss = sess.run([net.train_op, \n",
    "                                                               net.regularized_loss, \n",
    "                                                               ], #net.summary_op\n",
    "                                                              feed_dict={net.x: x_train[i,:], \n",
    "                                                                         net.y_: y_train[i,:], \n",
    "                                                                         net.keep_prob:0.5, net.lr:lr})\n",
    "\n",
    "                        train_batch()\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        while((train_loss > 1e4 or np.isnan(train_loss)) and epoch > 1):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                            net.saver.restore(sess, \n",
    "                                              tf.train.latest_checkpoint('dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}'\n",
    "                                                                         .format(epochs,h,f)))\n",
    "                            train_batch()\n",
    "\n",
    "\n",
    "                    valid_accuracy = sess.run(net.tf_accuracy, #net.summary_op \n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                             net.y_: preprocess.y_test, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    \n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x: preprocess.x_test_, \n",
    "                                                                             net.y_: preprocess.y_test_, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Validation Accuracy: {:.6f}\".format(epoch, train_loss, valid_accuracy))\n",
    "                    print(\"Accuracy on Test data: {}, {}\".format(accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                        Train.best_acc_global = accuracy\n",
    "                        Train.pred_value = pred_value\n",
    "                        Train.actual_value = actual_value\n",
    "                        Train.pred_value_ = pred_value_\n",
    "                        Train.actual_value_ = actual_value_\n",
    "                        Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "                        Train.best_acc = accuracy\n",
    "\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_dense_only_nsl_kdd/hidden_layers_{}_features_count_{}\".format(h,f),\n",
    "                                        global_step = epochs)\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epoch+1)*(c+1),f,h):(curr_pred, \n",
    "                                                   Train.result((epoch+1)*(c+1), f, h, valid_accuracy, accuracy, accuracy_, time.perf_counter() - start_time))})\n",
    "\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:36.607425Z",
     "start_time": "2017-06-16T18:43:06.158546Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:20 hidden layers:1 features count:1\n",
      "Step 1 | Training Loss: 0.777004 | Validation Accuracy: 0.563582\n",
      "Accuracy on Test data: 0.44144782423973083, 0.22092826664447784\n",
      "Step 2 | Training Loss: 0.728217 | Validation Accuracy: 0.593269\n",
      "Accuracy on Test data: 0.47901880741119385, 0.2433755248785019\n",
      "Step 3 | Training Loss: 0.660577 | Validation Accuracy: 0.634386\n",
      "Accuracy on Test data: 0.5603708028793335, 0.2653164565563202\n",
      "Step 4 | Training Loss: 0.616754 | Validation Accuracy: 0.704477\n",
      "Accuracy on Test data: 0.6015791296958923, 0.32877635955810547\n",
      "Step 5 | Training Loss: 0.572547 | Validation Accuracy: 0.824178\n",
      "Accuracy on Test data: 0.6676720976829529, 0.38472574949264526\n",
      "Step 6 | Training Loss: 0.534059 | Validation Accuracy: 0.876568\n",
      "Accuracy on Test data: 0.692689836025238, 0.42742615938186646\n",
      "Step 7 | Training Loss: 0.519222 | Validation Accuracy: 0.903794\n",
      "Accuracy on Test data: 0.7143807411193848, 0.4680168628692627\n",
      "Step 8 | Training Loss: 0.489144 | Validation Accuracy: 0.919590\n",
      "Accuracy on Test data: 0.729817271232605, 0.4962025284767151\n",
      "Step 9 | Training Loss: 0.459408 | Validation Accuracy: 0.936577\n",
      "Accuracy on Test data: 0.7267565727233887, 0.4860759377479553\n",
      "Step 10 | Training Loss: 0.455298 | Validation Accuracy: 0.948166\n",
      "Accuracy on Test data: 0.7388218641281128, 0.5078480839729309\n",
      "Step 11 | Training Loss: 0.434487 | Validation Accuracy: 0.960549\n",
      "Accuracy on Test data: 0.7428140640258789, 0.5147679448127747\n",
      "Step 12 | Training Loss: 0.416765 | Validation Accuracy: 0.963248\n",
      "Accuracy on Test data: 0.7463626861572266, 0.5195780396461487\n",
      "Step 13 | Training Loss: 0.423132 | Validation Accuracy: 0.965153\n",
      "Accuracy on Test data: 0.7456973195075989, 0.5190717577934265\n",
      "Step 14 | Training Loss: 0.415189 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7440117001533508, 0.5158649682998657\n",
      "Step 15 | Training Loss: 0.397081 | Validation Accuracy: 0.966661\n",
      "Accuracy on Test data: 0.7441891431808472, 0.5157805681228638\n",
      "Step 16 | Training Loss: 0.389092 | Validation Accuracy: 0.966503\n",
      "Accuracy on Test data: 0.7443665862083435, 0.5156962275505066\n",
      "Step 17 | Training Loss: 0.383104 | Validation Accuracy: 0.967535\n",
      "Accuracy on Test data: 0.7443665862083435, 0.5156962275505066\n",
      "Step 18 | Training Loss: 0.387084 | Validation Accuracy: 0.965074\n",
      "Accuracy on Test data: 0.7437899112701416, 0.5145991444587708\n",
      "Step 19 | Training Loss: 0.370290 | Validation Accuracy: 0.965391\n",
      "Accuracy on Test data: 0.7425035238265991, 0.5121518969535828\n",
      "Step 20 | Training Loss: 0.374035 | Validation Accuracy: 0.967455\n",
      "Accuracy on Test data: 0.7423261404037476, 0.5117299556732178\n",
      "Step 1 | Training Loss: 0.390024 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7423261404037476, 0.5117299556732178\n",
      "Step 2 | Training Loss: 0.370773 | Validation Accuracy: 0.966820\n",
      "Accuracy on Test data: 0.7423261404037476, 0.5117299556732178\n",
      "Step 3 | Training Loss: 0.370834 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7423261404037476, 0.5117299556732178\n",
      "Step 4 | Training Loss: 0.371141 | Validation Accuracy: 0.968249\n",
      "Accuracy on Test data: 0.7423261404037476, 0.5117299556732178\n",
      "Step 5 | Training Loss: 0.380251 | Validation Accuracy: 0.966741\n",
      "Accuracy on Test data: 0.7423261404037476, 0.5117299556732178\n",
      "Step 6 | Training Loss: 0.383584 | Validation Accuracy: 0.968566\n",
      "Accuracy on Test data: 0.7423261404037476, 0.5117299556732178\n",
      "Step 7 | Training Loss: 0.370354 | Validation Accuracy: 0.971742\n",
      "Accuracy on Test data: 0.7421486973762512, 0.5113924145698547\n",
      "Step 8 | Training Loss: 0.363091 | Validation Accuracy: 0.972456\n",
      "Accuracy on Test data: 0.7420156002044678, 0.5111392140388489\n",
      "Step 9 | Training Loss: 0.378364 | Validation Accuracy: 0.966900\n",
      "Accuracy on Test data: 0.7420156002044678, 0.5111392140388489\n",
      "Step 10 | Training Loss: 0.363414 | Validation Accuracy: 0.965550\n",
      "Accuracy on Test data: 0.741926908493042, 0.5109704732894897\n",
      "Step 11 | Training Loss: 0.377408 | Validation Accuracy: 0.968566\n",
      "Accuracy on Test data: 0.7417051196098328, 0.5105485320091248\n",
      "Step 12 | Training Loss: 0.360574 | Validation Accuracy: 0.968170\n",
      "Accuracy on Test data: 0.7416163682937622, 0.5103797316551208\n",
      "Step 13 | Training Loss: 0.370595 | Validation Accuracy: 0.967852\n",
      "Accuracy on Test data: 0.7415720224380493, 0.5102953314781189\n",
      "Step 14 | Training Loss: 0.371762 | Validation Accuracy: 0.966264\n",
      "Accuracy on Test data: 0.7414833307266235, 0.5101265907287598\n",
      "Step 15 | Training Loss: 0.364756 | Validation Accuracy: 0.968170\n",
      "Accuracy on Test data: 0.741394579410553, 0.5099577903747559\n",
      "Step 16 | Training Loss: 0.374415 | Validation Accuracy: 0.969916\n",
      "Accuracy on Test data: 0.7413502335548401, 0.5098733901977539\n",
      "Step 17 | Training Loss: 0.372561 | Validation Accuracy: 0.969281\n",
      "Accuracy on Test data: 0.7412615418434143, 0.5097046494483948\n",
      "Step 18 | Training Loss: 0.364275 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.741084098815918, 0.5093671083450317\n",
      "Step 19 | Training Loss: 0.371128 | Validation Accuracy: 0.971424\n",
      "Accuracy on Test data: 0.7411284446716309, 0.5094514489173889\n",
      "Step 20 | Training Loss: 0.362356 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.741084098815918, 0.5093671083450317\n",
      "Step 1 | Training Loss: 0.360611 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.741084098815918, 0.5093671083450317\n",
      "Step 2 | Training Loss: 0.374099 | Validation Accuracy: 0.970233\n",
      "Accuracy on Test data: 0.741084098815918, 0.5093671083450317\n",
      "Step 3 | Training Loss: 0.354643 | Validation Accuracy: 0.968646\n",
      "Accuracy on Test data: 0.7410397529602051, 0.5092827081680298\n",
      "Step 4 | Training Loss: 0.360132 | Validation Accuracy: 0.967535\n",
      "Accuracy on Test data: 0.7410397529602051, 0.5092827081680298\n",
      "Step 5 | Training Loss: 0.357024 | Validation Accuracy: 0.967773\n",
      "Accuracy on Test data: 0.7410397529602051, 0.5092827081680298\n",
      "Step 6 | Training Loss: 0.374601 | Validation Accuracy: 0.968566\n",
      "Accuracy on Test data: 0.7409954071044922, 0.5091983079910278\n",
      "Step 7 | Training Loss: 0.363514 | Validation Accuracy: 0.970630\n",
      "Accuracy on Test data: 0.7409510016441345, 0.5091139078140259\n",
      "Step 8 | Training Loss: 0.372980 | Validation Accuracy: 0.970233\n",
      "Accuracy on Test data: 0.7409066557884216, 0.5090295076370239\n",
      "Step 9 | Training Loss: 0.370176 | Validation Accuracy: 0.968408\n",
      "Accuracy on Test data: 0.7409066557884216, 0.5090295076370239\n",
      "Step 10 | Training Loss: 0.373241 | Validation Accuracy: 0.969281\n",
      "Accuracy on Test data: 0.7409066557884216, 0.5090295076370239\n",
      "Step 11 | Training Loss: 0.375112 | Validation Accuracy: 0.966106\n",
      "Accuracy on Test data: 0.7408179640769958, 0.5088607668876648\n",
      "Step 12 | Training Loss: 0.364114 | Validation Accuracy: 0.969201\n",
      "Accuracy on Test data: 0.7408179640769958, 0.5088607668876648\n",
      "Step 13 | Training Loss: 0.366550 | Validation Accuracy: 0.968011\n",
      "Accuracy on Test data: 0.7408179640769958, 0.5088607668876648\n",
      "Step 14 | Training Loss: 0.356897 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.7408179640769958, 0.5088607668876648\n",
      "Step 15 | Training Loss: 0.369340 | Validation Accuracy: 0.966106\n",
      "Accuracy on Test data: 0.7408179640769958, 0.5088607668876648\n",
      "Step 16 | Training Loss: 0.374746 | Validation Accuracy: 0.967614\n",
      "Accuracy on Test data: 0.740773618221283, 0.5087763667106628\n",
      "Step 17 | Training Loss: 0.367804 | Validation Accuracy: 0.969519\n",
      "Accuracy on Test data: 0.7407292127609253, 0.5086919665336609\n",
      "Step 18 | Training Loss: 0.364192 | Validation Accuracy: 0.969598\n",
      "Accuracy on Test data: 0.7406848669052124, 0.5086075663566589\n",
      "Step 19 | Training Loss: 0.369700 | Validation Accuracy: 0.968725\n",
      "Accuracy on Test data: 0.7406848669052124, 0.5086075663566589\n",
      "Step 20 | Training Loss: 0.367174 | Validation Accuracy: 0.967773\n",
      "Accuracy on Test data: 0.7406848669052124, 0.5086075663566589\n",
      "Current Layer Attributes - epochs:20 hidden layers:1 features count:4\n",
      "Step 1 | Training Loss: 0.656217 | Validation Accuracy: 0.693761\n",
      "Accuracy on Test data: 0.7211231589317322, 0.7079324722290039\n",
      "Step 2 | Training Loss: 0.629948 | Validation Accuracy: 0.834498\n",
      "Accuracy on Test data: 0.8492282032966614, 0.7324050664901733\n",
      "Step 3 | Training Loss: 0.549399 | Validation Accuracy: 0.858628\n",
      "Accuracy on Test data: 0.860849916934967, 0.7494514584541321\n",
      "Step 4 | Training Loss: 0.533532 | Validation Accuracy: 0.885299\n",
      "Accuracy on Test data: 0.8657292127609253, 0.7563713192939758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.500190 | Validation Accuracy: 0.898873\n",
      "Accuracy on Test data: 0.8661727905273438, 0.7550210952758789\n",
      "Step 6 | Training Loss: 0.472174 | Validation Accuracy: 0.913081\n",
      "Accuracy on Test data: 0.8595191836357117, 0.7410126328468323\n",
      "Step 7 | Training Loss: 0.475786 | Validation Accuracy: 0.944118\n",
      "Accuracy on Test data: 0.8691447973251343, 0.7586497664451599\n",
      "Step 8 | Training Loss: 0.453214 | Validation Accuracy: 0.946738\n",
      "Accuracy on Test data: 0.870120644569397, 0.7602531909942627\n",
      "Step 9 | Training Loss: 0.446380 | Validation Accuracy: 0.949278\n",
      "Accuracy on Test data: 0.8694552779197693, 0.7589873671531677\n",
      "Step 10 | Training Loss: 0.420007 | Validation Accuracy: 0.949992\n",
      "Accuracy on Test data: 0.8685681223869324, 0.7568776607513428\n",
      "Step 11 | Training Loss: 0.419335 | Validation Accuracy: 0.952215\n",
      "Accuracy on Test data: 0.8667938113212585, 0.753248929977417\n",
      "Step 12 | Training Loss: 0.410896 | Validation Accuracy: 0.954040\n",
      "Accuracy on Test data: 0.8648420572280884, 0.749113917350769\n",
      "Step 13 | Training Loss: 0.396258 | Validation Accuracy: 0.954993\n",
      "Accuracy on Test data: 0.8638218641281128, 0.7465822696685791\n",
      "Step 14 | Training Loss: 0.415641 | Validation Accuracy: 0.959359\n",
      "Accuracy on Test data: 0.8627129197120667, 0.744303822517395\n",
      "Step 15 | Training Loss: 0.385725 | Validation Accuracy: 0.956183\n",
      "Accuracy on Test data: 0.8592086434364319, 0.7372151613235474\n",
      "Step 16 | Training Loss: 0.378207 | Validation Accuracy: 0.959438\n",
      "Accuracy on Test data: 0.853087306022644, 0.7255696058273315\n",
      "Step 17 | Training Loss: 0.377876 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.8297995328903198, 0.6811814308166504\n",
      "Step 18 | Training Loss: 0.376021 | Validation Accuracy: 0.960470\n",
      "Accuracy on Test data: 0.819419801235199, 0.6613501906394958\n",
      "Step 19 | Training Loss: 0.376252 | Validation Accuracy: 0.961740\n",
      "Accuracy on Test data: 0.813342809677124, 0.649789035320282\n",
      "Step 20 | Training Loss: 0.370232 | Validation Accuracy: 0.959597\n",
      "Accuracy on Test data: 0.8100603222846985, 0.6432911157608032\n",
      "Step 1 | Training Loss: 0.382822 | Validation Accuracy: 0.959994\n",
      "Accuracy on Test data: 0.8100603222846985, 0.643206775188446\n",
      "Step 2 | Training Loss: 0.371101 | Validation Accuracy: 0.961899\n",
      "Accuracy on Test data: 0.8096610903739929, 0.6424472332000732\n",
      "Step 3 | Training Loss: 0.364700 | Validation Accuracy: 0.963883\n",
      "Accuracy on Test data: 0.8095280528068542, 0.6421940922737122\n",
      "Step 4 | Training Loss: 0.377339 | Validation Accuracy: 0.963407\n",
      "Accuracy on Test data: 0.8092618584632874, 0.6416877508163452\n",
      "Step 5 | Training Loss: 0.379721 | Validation Accuracy: 0.965629\n",
      "Accuracy on Test data: 0.8091731667518616, 0.6415190100669861\n",
      "Step 6 | Training Loss: 0.361445 | Validation Accuracy: 0.965629\n",
      "Accuracy on Test data: 0.8089070320129395, 0.6410126686096191\n",
      "Step 7 | Training Loss: 0.382642 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.8086408972740173, 0.6403375267982483\n",
      "Step 8 | Training Loss: 0.379596 | Validation Accuracy: 0.964359\n",
      "Accuracy on Test data: 0.8085078001022339, 0.6400843858718872\n",
      "Step 9 | Training Loss: 0.376705 | Validation Accuracy: 0.963010\n",
      "Accuracy on Test data: 0.8081973195075989, 0.6394936442375183\n",
      "Step 10 | Training Loss: 0.361802 | Validation Accuracy: 0.962851\n",
      "Accuracy on Test data: 0.8080198764801025, 0.6391561031341553\n",
      "Step 11 | Training Loss: 0.365201 | Validation Accuracy: 0.964836\n",
      "Accuracy on Test data: 0.8077980875968933, 0.6387341618537903\n",
      "Step 12 | Training Loss: 0.375425 | Validation Accuracy: 0.963963\n",
      "Accuracy on Test data: 0.807310163974762, 0.6378058791160583\n",
      "Step 13 | Training Loss: 0.376381 | Validation Accuracy: 0.966344\n",
      "Accuracy on Test data: 0.8068222403526306, 0.6368776559829712\n",
      "Step 14 | Training Loss: 0.353978 | Validation Accuracy: 0.963169\n",
      "Accuracy on Test data: 0.8063342571258545, 0.6359493732452393\n",
      "Step 15 | Training Loss: 0.374407 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.8058463335037231, 0.6350210905075073\n",
      "Step 16 | Training Loss: 0.358799 | Validation Accuracy: 0.962772\n",
      "Accuracy on Test data: 0.8040720224380493, 0.6316455602645874\n",
      "Step 17 | Training Loss: 0.356240 | Validation Accuracy: 0.967773\n",
      "Accuracy on Test data: 0.8022090196609497, 0.6281012892723083\n",
      "Step 18 | Training Loss: 0.361819 | Validation Accuracy: 0.964042\n",
      "Accuracy on Test data: 0.8008782863616943, 0.6255696415901184\n",
      "Step 19 | Training Loss: 0.375803 | Validation Accuracy: 0.965074\n",
      "Accuracy on Test data: 0.8005234003067017, 0.6248944997787476\n",
      "Step 20 | Training Loss: 0.366202 | Validation Accuracy: 0.964280\n",
      "Accuracy on Test data: 0.8000798225402832, 0.6240506172180176\n",
      "Step 1 | Training Loss: 0.361946 | Validation Accuracy: 0.964518\n",
      "Accuracy on Test data: 0.8000354766845703, 0.6239662170410156\n",
      "Step 2 | Training Loss: 0.356923 | Validation Accuracy: 0.963566\n",
      "Accuracy on Test data: 0.7999911308288574, 0.6238818764686584\n",
      "Step 3 | Training Loss: 0.361447 | Validation Accuracy: 0.965233\n",
      "Accuracy on Test data: 0.7999467849731445, 0.6237974762916565\n",
      "Step 4 | Training Loss: 0.370994 | Validation Accuracy: 0.964994\n",
      "Accuracy on Test data: 0.7999024391174316, 0.6237130761146545\n",
      "Step 5 | Training Loss: 0.358716 | Validation Accuracy: 0.963963\n",
      "Accuracy on Test data: 0.7999024391174316, 0.6237130761146545\n",
      "Step 6 | Training Loss: 0.363319 | Validation Accuracy: 0.966344\n",
      "Accuracy on Test data: 0.799858033657074, 0.6236286759376526\n",
      "Step 7 | Training Loss: 0.351459 | Validation Accuracy: 0.963963\n",
      "Accuracy on Test data: 0.7997693419456482, 0.6234599351882935\n",
      "Step 8 | Training Loss: 0.362952 | Validation Accuracy: 0.964677\n",
      "Accuracy on Test data: 0.7997693419456482, 0.6234599351882935\n",
      "Step 9 | Training Loss: 0.379539 | Validation Accuracy: 0.964518\n",
      "Accuracy on Test data: 0.7997249960899353, 0.6233755350112915\n",
      "Step 10 | Training Loss: 0.359783 | Validation Accuracy: 0.964359\n",
      "Accuracy on Test data: 0.7997249960899353, 0.6233755350112915\n",
      "Step 11 | Training Loss: 0.359606 | Validation Accuracy: 0.963566\n",
      "Accuracy on Test data: 0.7997693419456482, 0.6234599351882935\n",
      "Step 12 | Training Loss: 0.369384 | Validation Accuracy: 0.962851\n",
      "Accuracy on Test data: 0.7997693419456482, 0.6234599351882935\n",
      "Step 13 | Training Loss: 0.359970 | Validation Accuracy: 0.964915\n",
      "Accuracy on Test data: 0.7997693419456482, 0.6234599351882935\n",
      "Step 14 | Training Loss: 0.361656 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.7997249960899353, 0.6233755350112915\n",
      "Step 15 | Training Loss: 0.363186 | Validation Accuracy: 0.966026\n",
      "Accuracy on Test data: 0.7997249960899353, 0.6233755350112915\n",
      "Step 16 | Training Loss: 0.374880 | Validation Accuracy: 0.963963\n",
      "Accuracy on Test data: 0.7997249960899353, 0.6233755350112915\n",
      "Step 17 | Training Loss: 0.368779 | Validation Accuracy: 0.966741\n",
      "Accuracy on Test data: 0.7995918989181519, 0.6231223344802856\n",
      "Step 18 | Training Loss: 0.364066 | Validation Accuracy: 0.966582\n",
      "Accuracy on Test data: 0.7995918989181519, 0.6231223344802856\n",
      "Step 19 | Training Loss: 0.363773 | Validation Accuracy: 0.964836\n",
      "Accuracy on Test data: 0.7995918989181519, 0.6231223344802856\n",
      "Step 20 | Training Loss: 0.367268 | Validation Accuracy: 0.963804\n",
      "Accuracy on Test data: 0.7995918989181519, 0.6231223344802856\n",
      "Current Layer Attributes - epochs:20 hidden layers:1 features count:16\n",
      "Step 1 | Training Loss: 0.627020 | Validation Accuracy: 0.698762\n",
      "Accuracy on Test data: 0.4991128444671631, 0.22497890889644623\n",
      "Step 2 | Training Loss: 0.567122 | Validation Accuracy: 0.773774\n",
      "Accuracy on Test data: 0.6016678214073181, 0.26059073209762573\n",
      "Step 3 | Training Loss: 0.531363 | Validation Accuracy: 0.812907\n",
      "Accuracy on Test data: 0.6488201022148132, 0.34607595205307007\n",
      "Step 4 | Training Loss: 0.526012 | Validation Accuracy: 0.854183\n",
      "Accuracy on Test data: 0.6778743863105774, 0.39864978194236755\n",
      "Step 5 | Training Loss: 0.484638 | Validation Accuracy: 0.882204\n",
      "Accuracy on Test data: 0.7143807411193848, 0.46405062079429626\n",
      "Step 6 | Training Loss: 0.484225 | Validation Accuracy: 0.908081\n",
      "Accuracy on Test data: 0.7322125434875488, 0.49468353390693665\n",
      "Step 7 | Training Loss: 0.450852 | Validation Accuracy: 0.930703\n",
      "Accuracy on Test data: 0.7252040505409241, 0.48016878962516785\n",
      "Step 8 | Training Loss: 0.440440 | Validation Accuracy: 0.925703\n",
      "Accuracy on Test data: 0.73012775182724, 0.4892826974391937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.428129 | Validation Accuracy: 0.948960\n",
      "Accuracy on Test data: 0.7372693419456482, 0.502869188785553\n",
      "Step 10 | Training Loss: 0.429529 | Validation Accuracy: 0.958168\n",
      "Accuracy on Test data: 0.7429915070533752, 0.5137552618980408\n",
      "Step 11 | Training Loss: 0.412351 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.7476934194564819, 0.5227004289627075\n",
      "Step 12 | Training Loss: 0.398463 | Validation Accuracy: 0.961581\n",
      "Accuracy on Test data: 0.7481813430786133, 0.5237130522727966\n",
      "Step 13 | Training Loss: 0.399832 | Validation Accuracy: 0.968011\n",
      "Accuracy on Test data: 0.7481369972229004, 0.5236287117004395\n",
      "Step 14 | Training Loss: 0.384146 | Validation Accuracy: 0.965709\n",
      "Accuracy on Test data: 0.7481813430786133, 0.5237130522727966\n",
      "Step 15 | Training Loss: 0.386666 | Validation Accuracy: 0.965312\n",
      "Accuracy on Test data: 0.747427225112915, 0.5222784876823425\n",
      "Step 16 | Training Loss: 0.376054 | Validation Accuracy: 0.968725\n",
      "Accuracy on Test data: 0.7465400695800781, 0.5205907225608826\n",
      "Step 17 | Training Loss: 0.374086 | Validation Accuracy: 0.969757\n",
      "Accuracy on Test data: 0.7452980875968933, 0.5181434750556946\n",
      "Step 18 | Training Loss: 0.365230 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.7431245446205139, 0.5138396620750427\n",
      "Step 19 | Training Loss: 0.375924 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7435237765312195, 0.5136708617210388\n",
      "Step 20 | Training Loss: 0.378583 | Validation Accuracy: 0.971265\n",
      "Accuracy on Test data: 0.7433019876480103, 0.5131645798683167\n",
      "Step 1 | Training Loss: 0.361418 | Validation Accuracy: 0.968328\n",
      "Accuracy on Test data: 0.7433463335037231, 0.5131645798683167\n",
      "Step 2 | Training Loss: 0.367114 | Validation Accuracy: 0.969598\n",
      "Accuracy on Test data: 0.7433463335037231, 0.5131645798683167\n",
      "Step 3 | Training Loss: 0.357124 | Validation Accuracy: 0.968646\n",
      "Accuracy on Test data: 0.743390679359436, 0.5132489204406738\n",
      "Step 4 | Training Loss: 0.359223 | Validation Accuracy: 0.968566\n",
      "Accuracy on Test data: 0.7433463335037231, 0.5131645798683167\n",
      "Step 5 | Training Loss: 0.362105 | Validation Accuracy: 0.968249\n",
      "Accuracy on Test data: 0.7433463335037231, 0.5131645798683167\n",
      "Step 6 | Training Loss: 0.372693 | Validation Accuracy: 0.969757\n",
      "Accuracy on Test data: 0.7432576417922974, 0.5129957795143127\n",
      "Step 7 | Training Loss: 0.373584 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7432132959365845, 0.5129113793373108\n",
      "Step 8 | Training Loss: 0.365102 | Validation Accuracy: 0.969598\n",
      "Accuracy on Test data: 0.7432132959365845, 0.5129113793373108\n",
      "Step 9 | Training Loss: 0.363454 | Validation Accuracy: 0.969519\n",
      "Accuracy on Test data: 0.7431688904762268, 0.5128269791603088\n",
      "Step 10 | Training Loss: 0.365609 | Validation Accuracy: 0.969440\n",
      "Accuracy on Test data: 0.7432132959365845, 0.5129113793373108\n",
      "Step 11 | Training Loss: 0.352463 | Validation Accuracy: 0.969916\n",
      "Accuracy on Test data: 0.7432576417922974, 0.5129957795143127\n",
      "Step 12 | Training Loss: 0.367304 | Validation Accuracy: 0.972138\n",
      "Accuracy on Test data: 0.7432132959365845, 0.5129113793373108\n",
      "Step 13 | Training Loss: 0.366193 | Validation Accuracy: 0.970313\n",
      "Accuracy on Test data: 0.7432132959365845, 0.5129113793373108\n",
      "Step 14 | Training Loss: 0.364690 | Validation Accuracy: 0.967614\n",
      "Accuracy on Test data: 0.7431245446205139, 0.5127426385879517\n",
      "Step 15 | Training Loss: 0.361454 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7431245446205139, 0.5127426385879517\n",
      "Step 16 | Training Loss: 0.365949 | Validation Accuracy: 0.968408\n",
      "Accuracy on Test data: 0.743080198764801, 0.5126582384109497\n",
      "Step 17 | Training Loss: 0.365799 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.7429915070533752, 0.5124894380569458\n",
      "Step 18 | Training Loss: 0.360641 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 19 | Training Loss: 0.371631 | Validation Accuracy: 0.972615\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 20 | Training Loss: 0.360895 | Validation Accuracy: 0.968725\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 1 | Training Loss: 0.364503 | Validation Accuracy: 0.970868\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 2 | Training Loss: 0.362974 | Validation Accuracy: 0.971821\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 3 | Training Loss: 0.361697 | Validation Accuracy: 0.968408\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 4 | Training Loss: 0.371263 | Validation Accuracy: 0.969440\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 5 | Training Loss: 0.352363 | Validation Accuracy: 0.969995\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 6 | Training Loss: 0.366036 | Validation Accuracy: 0.967376\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 7 | Training Loss: 0.357771 | Validation Accuracy: 0.968884\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 8 | Training Loss: 0.367316 | Validation Accuracy: 0.969757\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 9 | Training Loss: 0.364515 | Validation Accuracy: 0.969360\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 10 | Training Loss: 0.355541 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 11 | Training Loss: 0.363914 | Validation Accuracy: 0.970154\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 12 | Training Loss: 0.364919 | Validation Accuracy: 0.970154\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5123206973075867\n",
      "Step 13 | Training Loss: 0.369443 | Validation Accuracy: 0.970392\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 14 | Training Loss: 0.355235 | Validation Accuracy: 0.969440\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 15 | Training Loss: 0.369413 | Validation Accuracy: 0.971107\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 16 | Training Loss: 0.357197 | Validation Accuracy: 0.972059\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 17 | Training Loss: 0.362967 | Validation Accuracy: 0.968884\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 18 | Training Loss: 0.360249 | Validation Accuracy: 0.970551\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 19 | Training Loss: 0.362923 | Validation Accuracy: 0.969281\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Step 20 | Training Loss: 0.357683 | Validation Accuracy: 0.971107\n",
      "Accuracy on Test data: 0.7429471015930176, 0.5124050378799438\n",
      "Current Layer Attributes - epochs:20 hidden layers:1 features count:32\n",
      "Step 1 | Training Loss: 0.592598 | Validation Accuracy: 0.822115\n",
      "Accuracy on Test data: 0.7750177383422852, 0.6038818359375\n",
      "Step 2 | Training Loss: 0.554565 | Validation Accuracy: 0.857835\n",
      "Accuracy on Test data: 0.8089513778686523, 0.6616877913475037\n",
      "Step 3 | Training Loss: 0.533371 | Validation Accuracy: 0.885934\n",
      "Accuracy on Test data: 0.8373846411705017, 0.7098734378814697\n",
      "Step 4 | Training Loss: 0.501727 | Validation Accuracy: 0.906652\n",
      "Accuracy on Test data: 0.848207950592041, 0.7246413230895996\n",
      "Step 5 | Training Loss: 0.464581 | Validation Accuracy: 0.915780\n",
      "Accuracy on Test data: 0.8516234755516052, 0.7289451360702515\n",
      "Step 6 | Training Loss: 0.464567 | Validation Accuracy: 0.918082\n",
      "Accuracy on Test data: 0.8500709533691406, 0.7230379581451416\n",
      "Step 7 | Training Loss: 0.438049 | Validation Accuracy: 0.929671\n",
      "Accuracy on Test data: 0.8411107063293457, 0.7043882012367249\n",
      "Step 8 | Training Loss: 0.445480 | Validation Accuracy: 0.941816\n",
      "Accuracy on Test data: 0.8354772925376892, 0.6924894452095032\n",
      "Step 9 | Training Loss: 0.423546 | Validation Accuracy: 0.954199\n",
      "Accuracy on Test data: 0.8278034329414368, 0.6758649945259094\n",
      "Step 10 | Training Loss: 0.395482 | Validation Accuracy: 0.958247\n",
      "Accuracy on Test data: 0.8249645233154297, 0.6692826747894287\n",
      "Step 11 | Training Loss: 0.404249 | Validation Accuracy: 0.961581\n",
      "Accuracy on Test data: 0.8223917484283447, 0.6635442972183228\n",
      "Step 12 | Training Loss: 0.388825 | Validation Accuracy: 0.961105\n",
      "Accuracy on Test data: 0.8207505345344543, 0.6603375673294067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13 | Training Loss: 0.402723 | Validation Accuracy: 0.964280\n",
      "Accuracy on Test data: 0.817689836025238, 0.654599130153656\n",
      "Step 14 | Training Loss: 0.392659 | Validation Accuracy: 0.965074\n",
      "Accuracy on Test data: 0.8138307332992554, 0.647257387638092\n",
      "Step 15 | Training Loss: 0.374696 | Validation Accuracy: 0.965233\n",
      "Accuracy on Test data: 0.7925834059715271, 0.6068354249000549\n",
      "Step 16 | Training Loss: 0.365829 | Validation Accuracy: 0.967376\n",
      "Accuracy on Test data: 0.7843772172927856, 0.5911392569541931\n",
      "Step 17 | Training Loss: 0.378761 | Validation Accuracy: 0.969440\n",
      "Accuracy on Test data: 0.7826915979385376, 0.5878481268882751\n",
      "Step 18 | Training Loss: 0.362303 | Validation Accuracy: 0.968090\n",
      "Accuracy on Test data: 0.7811834812164307, 0.5849788784980774\n",
      "Step 19 | Training Loss: 0.362318 | Validation Accuracy: 0.971424\n",
      "Accuracy on Test data: 0.7806955575942993, 0.5839662551879883\n",
      "Step 20 | Training Loss: 0.366252 | Validation Accuracy: 0.969281\n",
      "Accuracy on Test data: 0.779675304889679, 0.5820252895355225\n",
      "Step 1 | Training Loss: 0.357381 | Validation Accuracy: 0.968884\n",
      "Accuracy on Test data: 0.7795866131782532, 0.5818565487861633\n",
      "Step 2 | Training Loss: 0.358196 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7795866131782532, 0.5818565487861633\n",
      "Step 3 | Training Loss: 0.369036 | Validation Accuracy: 0.968805\n",
      "Accuracy on Test data: 0.7794535160064697, 0.5816033482551575\n",
      "Step 4 | Training Loss: 0.368815 | Validation Accuracy: 0.970551\n",
      "Accuracy on Test data: 0.7794091701507568, 0.5815190076828003\n",
      "Step 5 | Training Loss: 0.365713 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7794091701507568, 0.5815190076828003\n",
      "Step 6 | Training Loss: 0.349640 | Validation Accuracy: 0.971583\n",
      "Accuracy on Test data: 0.7793204188346863, 0.5813502073287964\n",
      "Step 7 | Training Loss: 0.366052 | Validation Accuracy: 0.968487\n",
      "Accuracy on Test data: 0.7793204188346863, 0.5813502073287964\n",
      "Step 8 | Training Loss: 0.375602 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7792317271232605, 0.5811814069747925\n",
      "Step 9 | Training Loss: 0.359484 | Validation Accuracy: 0.970154\n",
      "Accuracy on Test data: 0.7791430354118347, 0.5810126662254333\n",
      "Step 10 | Training Loss: 0.362303 | Validation Accuracy: 0.971186\n",
      "Accuracy on Test data: 0.779098629951477, 0.5809282660484314\n",
      "Step 11 | Training Loss: 0.376181 | Validation Accuracy: 0.970789\n",
      "Accuracy on Test data: 0.7789655923843384, 0.5806751251220703\n",
      "Step 12 | Training Loss: 0.361860 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7786107063293457, 0.5799999833106995\n",
      "Step 13 | Training Loss: 0.357461 | Validation Accuracy: 0.968566\n",
      "Accuracy on Test data: 0.778477668762207, 0.5797468423843384\n",
      "Step 14 | Training Loss: 0.370536 | Validation Accuracy: 0.967058\n",
      "Accuracy on Test data: 0.7784332633018494, 0.5796624422073364\n",
      "Step 15 | Training Loss: 0.361567 | Validation Accuracy: 0.970472\n",
      "Accuracy on Test data: 0.7783889174461365, 0.5795780420303345\n",
      "Step 16 | Training Loss: 0.358709 | Validation Accuracy: 0.969440\n",
      "Accuracy on Test data: 0.7782558798789978, 0.5793249011039734\n",
      "Step 17 | Training Loss: 0.366972 | Validation Accuracy: 0.970710\n",
      "Accuracy on Test data: 0.7780340909957886, 0.5789029598236084\n",
      "Step 18 | Training Loss: 0.357987 | Validation Accuracy: 0.967852\n",
      "Accuracy on Test data: 0.7779896855354309, 0.5788185596466064\n",
      "Step 19 | Training Loss: 0.356544 | Validation Accuracy: 0.970233\n",
      "Accuracy on Test data: 0.7777678966522217, 0.5783966183662415\n",
      "Step 20 | Training Loss: 0.377514 | Validation Accuracy: 0.969281\n",
      "Accuracy on Test data: 0.777634859085083, 0.5781434774398804\n",
      "Step 1 | Training Loss: 0.356861 | Validation Accuracy: 0.971424\n",
      "Accuracy on Test data: 0.777634859085083, 0.5781434774398804\n",
      "Step 2 | Training Loss: 0.362301 | Validation Accuracy: 0.969519\n",
      "Accuracy on Test data: 0.777634859085083, 0.5781434774398804\n",
      "Step 3 | Training Loss: 0.363433 | Validation Accuracy: 0.971821\n",
      "Accuracy on Test data: 0.777634859085083, 0.5781434774398804\n",
      "Step 4 | Training Loss: 0.368869 | Validation Accuracy: 0.971662\n",
      "Accuracy on Test data: 0.7775905132293701, 0.5780590772628784\n",
      "Step 5 | Training Loss: 0.359692 | Validation Accuracy: 0.967773\n",
      "Accuracy on Test data: 0.7775905132293701, 0.5780590772628784\n",
      "Step 6 | Training Loss: 0.374673 | Validation Accuracy: 0.969995\n",
      "Accuracy on Test data: 0.7775461077690125, 0.5779746770858765\n",
      "Step 7 | Training Loss: 0.362033 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7775017619132996, 0.5778902769088745\n",
      "Step 8 | Training Loss: 0.370268 | Validation Accuracy: 0.970472\n",
      "Accuracy on Test data: 0.7775017619132996, 0.5778902769088745\n",
      "Step 9 | Training Loss: 0.346743 | Validation Accuracy: 0.969440\n",
      "Accuracy on Test data: 0.7775017619132996, 0.5778902769088745\n",
      "Step 10 | Training Loss: 0.369000 | Validation Accuracy: 0.972456\n",
      "Accuracy on Test data: 0.7774574160575867, 0.5778059363365173\n",
      "Step 11 | Training Loss: 0.369146 | Validation Accuracy: 0.971503\n",
      "Accuracy on Test data: 0.7774130702018738, 0.5777215361595154\n",
      "Step 12 | Training Loss: 0.353460 | Validation Accuracy: 0.970075\n",
      "Accuracy on Test data: 0.7773687243461609, 0.5776371359825134\n",
      "Step 13 | Training Loss: 0.364004 | Validation Accuracy: 0.972297\n",
      "Accuracy on Test data: 0.7773687243461609, 0.5776371359825134\n",
      "Step 14 | Training Loss: 0.353447 | Validation Accuracy: 0.971900\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5775527358055115\n",
      "Step 15 | Training Loss: 0.357162 | Validation Accuracy: 0.970392\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5775527358055115\n",
      "Step 16 | Training Loss: 0.342974 | Validation Accuracy: 0.971186\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5775527358055115\n",
      "Step 17 | Training Loss: 0.363738 | Validation Accuracy: 0.972694\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5775527358055115\n",
      "Step 18 | Training Loss: 0.368328 | Validation Accuracy: 0.970948\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5775527358055115\n",
      "Step 19 | Training Loss: 0.355204 | Validation Accuracy: 0.972059\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5775527358055115\n",
      "Step 20 | Training Loss: 0.351287 | Validation Accuracy: 0.970551\n",
      "Accuracy on Test data: 0.7773243188858032, 0.5775527358055115\n",
      "Current Layer Attributes - epochs:20 hidden layers:1 features count:64\n",
      "Step 1 | Training Loss: 0.596146 | Validation Accuracy: 0.792189\n",
      "Accuracy on Test data: 0.5971876978874207, 0.4325738251209259\n",
      "Step 2 | Training Loss: 0.566766 | Validation Accuracy: 0.857200\n",
      "Accuracy on Test data: 0.6497959494590759, 0.47637131810188293\n",
      "Step 3 | Training Loss: 0.521244 | Validation Accuracy: 0.901016\n",
      "Accuracy on Test data: 0.7413058876991272, 0.5225316286087036\n",
      "Step 4 | Training Loss: 0.514816 | Validation Accuracy: 0.927687\n",
      "Accuracy on Test data: 0.7584279775619507, 0.5521519184112549\n",
      "Step 5 | Training Loss: 0.462447 | Validation Accuracy: 0.935863\n",
      "Accuracy on Test data: 0.7749290466308594, 0.5822784900665283\n",
      "Step 6 | Training Loss: 0.462390 | Validation Accuracy: 0.938879\n",
      "Accuracy on Test data: 0.7842885255813599, 0.5992404818534851\n",
      "Step 7 | Training Loss: 0.443915 | Validation Accuracy: 0.945547\n",
      "Accuracy on Test data: 0.7914301156997681, 0.6113923788070679\n",
      "Step 8 | Training Loss: 0.418798 | Validation Accuracy: 0.951659\n",
      "Accuracy on Test data: 0.798349916934967, 0.6237130761146545\n",
      "Step 9 | Training Loss: 0.413422 | Validation Accuracy: 0.955628\n",
      "Accuracy on Test data: 0.8017654418945312, 0.6287763714790344\n",
      "Step 10 | Training Loss: 0.409683 | Validation Accuracy: 0.958485\n",
      "Accuracy on Test data: 0.8011887669563293, 0.6257383823394775\n",
      "Step 11 | Training Loss: 0.392607 | Validation Accuracy: 0.960232\n",
      "Accuracy on Test data: 0.7913857102394104, 0.6066666841506958\n",
      "Step 12 | Training Loss: 0.399166 | Validation Accuracy: 0.963486\n",
      "Accuracy on Test data: 0.7873491644859314, 0.598565399646759\n",
      "Step 13 | Training Loss: 0.391781 | Validation Accuracy: 0.960311\n",
      "Accuracy on Test data: 0.779364824295044, 0.5830379724502563\n",
      "Step 14 | Training Loss: 0.390137 | Validation Accuracy: 0.963566\n",
      "Accuracy on Test data: 0.7762597799301147, 0.5764557123184204\n",
      "Step 15 | Training Loss: 0.381291 | Validation Accuracy: 0.966741\n",
      "Accuracy on Test data: 0.7741749286651611, 0.5722362995147705\n",
      "Step 16 | Training Loss: 0.376210 | Validation Accuracy: 0.966026\n",
      "Accuracy on Test data: 0.7687633037567139, 0.5616877675056458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17 | Training Loss: 0.379719 | Validation Accuracy: 0.966344\n",
      "Accuracy on Test data: 0.7680979371070862, 0.5602531433105469\n",
      "Step 18 | Training Loss: 0.369693 | Validation Accuracy: 0.964994\n",
      "Accuracy on Test data: 0.7674769163131714, 0.5589873194694519\n",
      "Step 19 | Training Loss: 0.366651 | Validation Accuracy: 0.970710\n",
      "Accuracy on Test data: 0.7662792801856995, 0.5566244721412659\n",
      "Step 20 | Training Loss: 0.370362 | Validation Accuracy: 0.969043\n",
      "Accuracy on Test data: 0.765835702419281, 0.5557805895805359\n",
      "Step 1 | Training Loss: 0.359450 | Validation Accuracy: 0.969281\n",
      "Accuracy on Test data: 0.7655695676803589, 0.555274248123169\n",
      "Step 2 | Training Loss: 0.374335 | Validation Accuracy: 0.971027\n",
      "Accuracy on Test data: 0.7653477787971497, 0.554852306842804\n",
      "Step 3 | Training Loss: 0.360776 | Validation Accuracy: 0.968328\n",
      "Accuracy on Test data: 0.7652146816253662, 0.5545991659164429\n",
      "Step 4 | Training Loss: 0.365722 | Validation Accuracy: 0.966344\n",
      "Accuracy on Test data: 0.7651259899139404, 0.554430365562439\n",
      "Step 5 | Training Loss: 0.356175 | Validation Accuracy: 0.969360\n",
      "Accuracy on Test data: 0.764992892742157, 0.5541772246360779\n",
      "Step 6 | Training Loss: 0.372335 | Validation Accuracy: 0.969598\n",
      "Accuracy on Test data: 0.7648154497146606, 0.5538396835327148\n",
      "Step 7 | Training Loss: 0.369927 | Validation Accuracy: 0.969043\n",
      "Accuracy on Test data: 0.7647711038589478, 0.5537552833557129\n",
      "Step 8 | Training Loss: 0.362821 | Validation Accuracy: 0.968725\n",
      "Accuracy on Test data: 0.764682412147522, 0.553586483001709\n",
      "Step 9 | Training Loss: 0.364646 | Validation Accuracy: 0.966979\n",
      "Accuracy on Test data: 0.7645049691200256, 0.553248941898346\n",
      "Step 10 | Training Loss: 0.364227 | Validation Accuracy: 0.971186\n",
      "Accuracy on Test data: 0.7644606232643127, 0.553164541721344\n",
      "Step 11 | Training Loss: 0.357159 | Validation Accuracy: 0.968487\n",
      "Accuracy on Test data: 0.764150083065033, 0.5525738596916199\n",
      "Step 12 | Training Loss: 0.353820 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.7640170454978943, 0.552320659160614\n",
      "Step 13 | Training Loss: 0.360954 | Validation Accuracy: 0.972535\n",
      "Accuracy on Test data: 0.7640170454978943, 0.552320659160614\n",
      "Step 14 | Training Loss: 0.369773 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.763839602470398, 0.551983118057251\n",
      "Step 15 | Training Loss: 0.357276 | Validation Accuracy: 0.969201\n",
      "Accuracy on Test data: 0.7636621594429016, 0.5516455769538879\n",
      "Step 16 | Training Loss: 0.364010 | Validation Accuracy: 0.970075\n",
      "Accuracy on Test data: 0.7634847164154053, 0.5513080358505249\n",
      "Step 17 | Training Loss: 0.365163 | Validation Accuracy: 0.970313\n",
      "Accuracy on Test data: 0.7633073329925537, 0.5509704351425171\n",
      "Step 18 | Training Loss: 0.362465 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.763262927532196, 0.5508860945701599\n",
      "Step 19 | Training Loss: 0.360959 | Validation Accuracy: 0.970551\n",
      "Accuracy on Test data: 0.7630855441093445, 0.5505484938621521\n",
      "Step 20 | Training Loss: 0.373328 | Validation Accuracy: 0.968328\n",
      "Accuracy on Test data: 0.7628193497657776, 0.5500422120094299\n",
      "Step 1 | Training Loss: 0.358150 | Validation Accuracy: 0.966820\n",
      "Accuracy on Test data: 0.7628193497657776, 0.5500422120094299\n",
      "Step 2 | Training Loss: 0.360611 | Validation Accuracy: 0.966741\n",
      "Accuracy on Test data: 0.7628193497657776, 0.5500422120094299\n",
      "Step 3 | Training Loss: 0.358184 | Validation Accuracy: 0.969201\n",
      "Accuracy on Test data: 0.7628193497657776, 0.5500422120094299\n",
      "Step 4 | Training Loss: 0.374440 | Validation Accuracy: 0.967058\n",
      "Accuracy on Test data: 0.7628193497657776, 0.5500422120094299\n",
      "Step 5 | Training Loss: 0.350506 | Validation Accuracy: 0.969757\n",
      "Accuracy on Test data: 0.7627750039100647, 0.549957811832428\n",
      "Step 6 | Training Loss: 0.348084 | Validation Accuracy: 0.969598\n",
      "Accuracy on Test data: 0.7627750039100647, 0.549957811832428\n",
      "Step 7 | Training Loss: 0.363054 | Validation Accuracy: 0.966503\n",
      "Accuracy on Test data: 0.7627306580543518, 0.549873411655426\n",
      "Step 8 | Training Loss: 0.355489 | Validation Accuracy: 0.968566\n",
      "Accuracy on Test data: 0.762641966342926, 0.5497046709060669\n",
      "Step 9 | Training Loss: 0.367835 | Validation Accuracy: 0.969836\n",
      "Accuracy on Test data: 0.7625975608825684, 0.5496202707290649\n",
      "Step 10 | Training Loss: 0.361346 | Validation Accuracy: 0.970313\n",
      "Accuracy on Test data: 0.7625532150268555, 0.549535870552063\n",
      "Step 11 | Training Loss: 0.354255 | Validation Accuracy: 0.967693\n",
      "Accuracy on Test data: 0.7625532150268555, 0.549535870552063\n",
      "Step 12 | Training Loss: 0.362049 | Validation Accuracy: 0.967535\n",
      "Accuracy on Test data: 0.7625532150268555, 0.549535870552063\n",
      "Step 13 | Training Loss: 0.354014 | Validation Accuracy: 0.969598\n",
      "Accuracy on Test data: 0.7625088691711426, 0.549451470375061\n",
      "Step 14 | Training Loss: 0.360807 | Validation Accuracy: 0.967455\n",
      "Accuracy on Test data: 0.7624645233154297, 0.5493670701980591\n",
      "Step 15 | Training Loss: 0.348801 | Validation Accuracy: 0.967217\n",
      "Accuracy on Test data: 0.7624645233154297, 0.5493670701980591\n",
      "Step 16 | Training Loss: 0.362223 | Validation Accuracy: 0.971186\n",
      "Accuracy on Test data: 0.7624645233154297, 0.5493670701980591\n",
      "Step 17 | Training Loss: 0.362002 | Validation Accuracy: 0.970472\n",
      "Accuracy on Test data: 0.7624645233154297, 0.5493670701980591\n",
      "Step 18 | Training Loss: 0.359988 | Validation Accuracy: 0.969995\n",
      "Accuracy on Test data: 0.7624645233154297, 0.5493670701980591\n",
      "Step 19 | Training Loss: 0.355744 | Validation Accuracy: 0.968487\n",
      "Accuracy on Test data: 0.7624201774597168, 0.5492827296257019\n",
      "Step 20 | Training Loss: 0.371876 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7624201774597168, 0.5492827296257019\n",
      "Current Layer Attributes - epochs:20 hidden layers:1 features count:122\n",
      "Step 1 | Training Loss: 0.750674 | Validation Accuracy: 0.369106\n",
      "Accuracy on Test data: 0.23704755306243896, 0.3168776333332062\n",
      "Step 2 | Training Loss: 0.688687 | Validation Accuracy: 0.631608\n",
      "Accuracy on Test data: 0.5208480954170227, 0.3732489347457886\n",
      "Step 3 | Training Loss: 0.638445 | Validation Accuracy: 0.808700\n",
      "Accuracy on Test data: 0.6637242436408997, 0.3850632905960083\n",
      "Step 4 | Training Loss: 0.593914 | Validation Accuracy: 0.857358\n",
      "Accuracy on Test data: 0.6944641470909119, 0.4301265776157379\n",
      "Step 5 | Training Loss: 0.568811 | Validation Accuracy: 0.901651\n",
      "Accuracy on Test data: 0.7179293632507324, 0.4720675051212311\n",
      "Step 6 | Training Loss: 0.538049 | Validation Accuracy: 0.926099\n",
      "Accuracy on Test data: 0.7320795059204102, 0.4982278347015381\n",
      "Step 7 | Training Loss: 0.510770 | Validation Accuracy: 0.935069\n",
      "Accuracy on Test data: 0.735849916934967, 0.5050632953643799\n",
      "Step 8 | Training Loss: 0.479815 | Validation Accuracy: 0.946103\n",
      "Accuracy on Test data: 0.7395759224891663, 0.5113924145698547\n",
      "Step 9 | Training Loss: 0.461242 | Validation Accuracy: 0.944356\n",
      "Accuracy on Test data: 0.7419712543487549, 0.5157805681228638\n",
      "Step 10 | Training Loss: 0.444763 | Validation Accuracy: 0.950310\n",
      "Accuracy on Test data: 0.7439673542976379, 0.5191560983657837\n",
      "Step 11 | Training Loss: 0.423457 | Validation Accuracy: 0.956104\n",
      "Accuracy on Test data: 0.745963454246521, 0.5218565464019775\n",
      "Step 12 | Training Loss: 0.420854 | Validation Accuracy: 0.956342\n",
      "Accuracy on Test data: 0.7494233250617981, 0.5273417830467224\n",
      "Step 13 | Training Loss: 0.425223 | Validation Accuracy: 0.960073\n",
      "Accuracy on Test data: 0.7500886917114258, 0.5286076068878174\n",
      "Step 14 | Training Loss: 0.406456 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.750842809677124, 0.5303797721862793\n",
      "Step 15 | Training Loss: 0.395462 | Validation Accuracy: 0.965312\n",
      "Accuracy on Test data: 0.7514638304710388, 0.5320675373077393\n",
      "Step 16 | Training Loss: 0.403890 | Validation Accuracy: 0.965788\n",
      "Accuracy on Test data: 0.7530163526535034, 0.5330801606178284\n",
      "Step 17 | Training Loss: 0.384152 | Validation Accuracy: 0.965550\n",
      "Accuracy on Test data: 0.7542139887809753, 0.5345991849899292\n",
      "Step 18 | Training Loss: 0.384081 | Validation Accuracy: 0.968487\n",
      "Accuracy on Test data: 0.7545688152313232, 0.5351054668426514\n",
      "Step 19 | Training Loss: 0.367513 | Validation Accuracy: 0.964518\n",
      "Accuracy on Test data: 0.7545688152313232, 0.5347679257392883\n",
      "Step 20 | Training Loss: 0.377377 | Validation Accuracy: 0.969360\n",
      "Accuracy on Test data: 0.7543026804924011, 0.5341772437095642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.366589 | Validation Accuracy: 0.968328\n",
      "Accuracy on Test data: 0.7542139887809753, 0.5340084433555603\n",
      "Step 2 | Training Loss: 0.380237 | Validation Accuracy: 0.967693\n",
      "Accuracy on Test data: 0.7539921998977661, 0.5335021018981934\n",
      "Step 3 | Training Loss: 0.370951 | Validation Accuracy: 0.969043\n",
      "Accuracy on Test data: 0.7539478540420532, 0.5334177017211914\n",
      "Step 4 | Training Loss: 0.375410 | Validation Accuracy: 0.970233\n",
      "Accuracy on Test data: 0.7538591027259827, 0.5332489609718323\n",
      "Step 5 | Training Loss: 0.375739 | Validation Accuracy: 0.969598\n",
      "Accuracy on Test data: 0.7537704110145569, 0.5330801606178284\n",
      "Step 6 | Training Loss: 0.381182 | Validation Accuracy: 0.970233\n",
      "Accuracy on Test data: 0.7535486221313477, 0.5326582193374634\n",
      "Step 7 | Training Loss: 0.372525 | Validation Accuracy: 0.968011\n",
      "Accuracy on Test data: 0.7536816596984863, 0.5328270196914673\n",
      "Step 8 | Training Loss: 0.374072 | Validation Accuracy: 0.967931\n",
      "Accuracy on Test data: 0.7535929679870605, 0.5326582193374634\n",
      "Step 9 | Training Loss: 0.366557 | Validation Accuracy: 0.968963\n",
      "Accuracy on Test data: 0.7535042762756348, 0.5324894785881042\n",
      "Step 10 | Training Loss: 0.369122 | Validation Accuracy: 0.967455\n",
      "Accuracy on Test data: 0.7532824873924255, 0.5320675373077393\n",
      "Step 11 | Training Loss: 0.367334 | Validation Accuracy: 0.970710\n",
      "Accuracy on Test data: 0.7532824873924255, 0.5320675373077393\n",
      "Step 12 | Training Loss: 0.361524 | Validation Accuracy: 0.968725\n",
      "Accuracy on Test data: 0.7531050443649292, 0.5317299365997314\n",
      "Step 13 | Training Loss: 0.376783 | Validation Accuracy: 0.970392\n",
      "Accuracy on Test data: 0.7530606985092163, 0.5316455960273743\n",
      "Step 14 | Training Loss: 0.382935 | Validation Accuracy: 0.967852\n",
      "Accuracy on Test data: 0.7530163526535034, 0.5315611958503723\n",
      "Step 15 | Training Loss: 0.360858 | Validation Accuracy: 0.968805\n",
      "Accuracy on Test data: 0.7527945637702942, 0.5311392545700073\n",
      "Step 16 | Training Loss: 0.363367 | Validation Accuracy: 0.971900\n",
      "Accuracy on Test data: 0.7527501583099365, 0.5310548543930054\n",
      "Step 17 | Training Loss: 0.373590 | Validation Accuracy: 0.968805\n",
      "Accuracy on Test data: 0.7526171207427979, 0.5308017134666443\n",
      "Step 18 | Training Loss: 0.365923 | Validation Accuracy: 0.970472\n",
      "Accuracy on Test data: 0.752572774887085, 0.5307173132896423\n",
      "Step 19 | Training Loss: 0.357275 | Validation Accuracy: 0.968646\n",
      "Accuracy on Test data: 0.7524396777153015, 0.5304641127586365\n",
      "Step 20 | Training Loss: 0.368569 | Validation Accuracy: 0.968963\n",
      "Accuracy on Test data: 0.7522178888320923, 0.5300421714782715\n",
      "Step 1 | Training Loss: 0.376694 | Validation Accuracy: 0.970868\n",
      "Accuracy on Test data: 0.7521735429763794, 0.5299578309059143\n",
      "Step 2 | Training Loss: 0.369924 | Validation Accuracy: 0.968487\n",
      "Accuracy on Test data: 0.7520847916603088, 0.5297890305519104\n",
      "Step 3 | Training Loss: 0.370818 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7520847916603088, 0.5297890305519104\n",
      "Step 4 | Training Loss: 0.364648 | Validation Accuracy: 0.965391\n",
      "Accuracy on Test data: 0.7520847916603088, 0.5297890305519104\n",
      "Step 5 | Training Loss: 0.366527 | Validation Accuracy: 0.967773\n",
      "Accuracy on Test data: 0.7520847916603088, 0.5297890305519104\n",
      "Step 6 | Training Loss: 0.367518 | Validation Accuracy: 0.970313\n",
      "Accuracy on Test data: 0.7519960999488831, 0.5296202301979065\n",
      "Step 7 | Training Loss: 0.357482 | Validation Accuracy: 0.971107\n",
      "Accuracy on Test data: 0.7519960999488831, 0.5296202301979065\n",
      "Step 8 | Training Loss: 0.371922 | Validation Accuracy: 0.969757\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5295358896255493\n",
      "Step 9 | Training Loss: 0.365495 | Validation Accuracy: 0.969122\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5295358896255493\n",
      "Step 10 | Training Loss: 0.384963 | Validation Accuracy: 0.967931\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5295358896255493\n",
      "Step 11 | Training Loss: 0.364431 | Validation Accuracy: 0.970313\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5295358896255493\n",
      "Step 12 | Training Loss: 0.358448 | Validation Accuracy: 0.970472\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5295358896255493\n",
      "Step 13 | Training Loss: 0.369780 | Validation Accuracy: 0.969916\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5295358896255493\n",
      "Step 14 | Training Loss: 0.358077 | Validation Accuracy: 0.971821\n",
      "Accuracy on Test data: 0.7519960999488831, 0.5295358896255493\n",
      "Step 15 | Training Loss: 0.356928 | Validation Accuracy: 0.971583\n",
      "Accuracy on Test data: 0.7519960999488831, 0.5295358896255493\n",
      "Step 16 | Training Loss: 0.356421 | Validation Accuracy: 0.967455\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5294514894485474\n",
      "Step 17 | Training Loss: 0.368825 | Validation Accuracy: 0.971424\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5294514894485474\n",
      "Step 18 | Training Loss: 0.367984 | Validation Accuracy: 0.970392\n",
      "Accuracy on Test data: 0.7518186569213867, 0.5291982889175415\n",
      "Step 19 | Training Loss: 0.365355 | Validation Accuracy: 0.969678\n",
      "Accuracy on Test data: 0.7518186569213867, 0.5291982889175415\n",
      "Step 20 | Training Loss: 0.367933 | Validation Accuracy: 0.971503\n",
      "Accuracy on Test data: 0.7518186569213867, 0.5291982889175415\n",
      "Current Layer Attributes - epochs:20 hidden layers:3 features count:1\n",
      "Step 1 | Training Loss: 0.687869 | Validation Accuracy: 0.701937\n",
      "Accuracy on Test data: 0.6584457159042358, 0.40928271412849426\n",
      "Step 2 | Training Loss: 0.679256 | Validation Accuracy: 0.754405\n",
      "Accuracy on Test data: 0.6722853183746338, 0.42540085315704346\n",
      "Step 3 | Training Loss: 0.680833 | Validation Accuracy: 0.778060\n",
      "Accuracy on Test data: 0.6869233250617981, 0.4459071755409241\n",
      "Step 4 | Training Loss: 0.678580 | Validation Accuracy: 0.789729\n",
      "Accuracy on Test data: 0.6968151330947876, 0.4587341845035553\n",
      "Step 5 | Training Loss: 0.680860 | Validation Accuracy: 0.818146\n",
      "Accuracy on Test data: 0.7052430510520935, 0.4678480923175812\n",
      "Step 6 | Training Loss: 0.667171 | Validation Accuracy: 0.846722\n",
      "Accuracy on Test data: 0.7161994576454163, 0.4810970425605774\n",
      "Step 7 | Training Loss: 0.664276 | Validation Accuracy: 0.861089\n",
      "Accuracy on Test data: 0.7238289713859558, 0.4913924038410187\n",
      "Step 8 | Training Loss: 0.674164 | Validation Accuracy: 0.866487\n",
      "Accuracy on Test data: 0.7320795059204102, 0.5046413540840149\n",
      "Step 9 | Training Loss: 0.670643 | Validation Accuracy: 0.881648\n",
      "Accuracy on Test data: 0.7372249960899353, 0.5126582384109497\n",
      "Step 10 | Training Loss: 0.669143 | Validation Accuracy: 0.888474\n",
      "Accuracy on Test data: 0.740773618221283, 0.5178902745246887\n",
      "Step 11 | Training Loss: 0.667111 | Validation Accuracy: 0.893555\n",
      "Accuracy on Test data: 0.740773618221283, 0.5168776512145996\n",
      "Step 12 | Training Loss: 0.662427 | Validation Accuracy: 0.898476\n",
      "Accuracy on Test data: 0.7420599460601807, 0.5177215337753296\n",
      "Step 13 | Training Loss: 0.669322 | Validation Accuracy: 0.898635\n",
      "Accuracy on Test data: 0.7394428849220276, 0.5115611553192139\n",
      "Step 14 | Training Loss: 0.659787 | Validation Accuracy: 0.899190\n",
      "Accuracy on Test data: 0.7377572655677795, 0.5085232257843018\n",
      "Step 15 | Training Loss: 0.660118 | Validation Accuracy: 0.909430\n",
      "Accuracy on Test data: 0.7395759224891663, 0.5108016729354858\n",
      "Step 16 | Training Loss: 0.663073 | Validation Accuracy: 0.914590\n",
      "Accuracy on Test data: 0.739354133605957, 0.5098733901977539\n",
      "Step 17 | Training Loss: 0.658082 | Validation Accuracy: 0.911891\n",
      "Accuracy on Test data: 0.7414833307266235, 0.5135865211486816\n",
      "Step 18 | Training Loss: 0.657838 | Validation Accuracy: 0.918241\n",
      "Accuracy on Test data: 0.7460965514183044, 0.5209282636642456\n",
      "Step 19 | Training Loss: 0.658002 | Validation Accuracy: 0.920384\n",
      "Accuracy on Test data: 0.7484474778175354, 0.5242193937301636\n",
      "Step 20 | Training Loss: 0.649668 | Validation Accuracy: 0.924115\n",
      "Accuracy on Test data: 0.7523509860038757, 0.5316455960273743\n",
      "Step 1 | Training Loss: 0.649810 | Validation Accuracy: 0.927131\n",
      "Accuracy on Test data: 0.752572774887085, 0.5320675373077393\n",
      "Step 2 | Training Loss: 0.655633 | Validation Accuracy: 0.923718\n",
      "Accuracy on Test data: 0.7531050443649292, 0.5330801606178284\n",
      "Step 3 | Training Loss: 0.661251 | Validation Accuracy: 0.926496\n",
      "Accuracy on Test data: 0.7532381415367126, 0.5333333611488342\n",
      "Step 4 | Training Loss: 0.656393 | Validation Accuracy: 0.927766\n",
      "Accuracy on Test data: 0.7534599304199219, 0.5337553024291992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.657520 | Validation Accuracy: 0.924671\n",
      "Accuracy on Test data: 0.753726065158844, 0.5342615842819214\n",
      "Step 6 | Training Loss: 0.655300 | Validation Accuracy: 0.929433\n",
      "Accuracy on Test data: 0.754036545753479, 0.5348523259162903\n",
      "Step 7 | Training Loss: 0.646328 | Validation Accuracy: 0.929592\n",
      "Accuracy on Test data: 0.7543026804924011, 0.5353586673736572\n",
      "Step 8 | Training Loss: 0.658532 | Validation Accuracy: 0.927925\n",
      "Accuracy on Test data: 0.7546132206916809, 0.5359493494033813\n",
      "Step 9 | Training Loss: 0.654409 | Validation Accuracy: 0.924432\n",
      "Accuracy on Test data: 0.7547906041145325, 0.5362868905067444\n",
      "Step 10 | Training Loss: 0.658692 | Validation Accuracy: 0.928004\n",
      "Accuracy on Test data: 0.754879355430603, 0.5364556908607483\n",
      "Step 11 | Training Loss: 0.650710 | Validation Accuracy: 0.929513\n",
      "Accuracy on Test data: 0.7551011443138123, 0.5368776321411133\n",
      "Step 12 | Training Loss: 0.654212 | Validation Accuracy: 0.929354\n",
      "Accuracy on Test data: 0.7550567984580994, 0.5367932319641113\n",
      "Step 13 | Training Loss: 0.653194 | Validation Accuracy: 0.929910\n",
      "Accuracy on Test data: 0.7553229331970215, 0.5372995734214783\n",
      "Step 14 | Training Loss: 0.653647 | Validation Accuracy: 0.925941\n",
      "Accuracy on Test data: 0.7555447220802307, 0.5377215147018433\n",
      "Step 15 | Training Loss: 0.665143 | Validation Accuracy: 0.931735\n",
      "Accuracy on Test data: 0.7557665109634399, 0.5381434559822083\n",
      "Step 16 | Training Loss: 0.665256 | Validation Accuracy: 0.926893\n",
      "Accuracy on Test data: 0.7560326457023621, 0.5386497974395752\n",
      "Step 17 | Training Loss: 0.656805 | Validation Accuracy: 0.928639\n",
      "Accuracy on Test data: 0.7561213374137878, 0.5388185381889343\n",
      "Step 18 | Training Loss: 0.654120 | Validation Accuracy: 0.928243\n",
      "Accuracy on Test data: 0.7563431262969971, 0.5392404794692993\n",
      "Step 19 | Training Loss: 0.654944 | Validation Accuracy: 0.932926\n",
      "Accuracy on Test data: 0.7564318776130676, 0.5394092798233032\n",
      "Step 20 | Training Loss: 0.649447 | Validation Accuracy: 0.932529\n",
      "Accuracy on Test data: 0.7564762234687805, 0.5394936800003052\n",
      "Step 1 | Training Loss: 0.655788 | Validation Accuracy: 0.933164\n",
      "Accuracy on Test data: 0.7564762234687805, 0.5394936800003052\n",
      "Step 2 | Training Loss: 0.657124 | Validation Accuracy: 0.932291\n",
      "Accuracy on Test data: 0.7565205693244934, 0.5395780801773071\n",
      "Step 3 | Training Loss: 0.657653 | Validation Accuracy: 0.933878\n",
      "Accuracy on Test data: 0.7564762234687805, 0.5394936800003052\n",
      "Step 4 | Training Loss: 0.649953 | Validation Accuracy: 0.931100\n",
      "Accuracy on Test data: 0.7565649151802063, 0.5396624207496643\n",
      "Step 5 | Training Loss: 0.652399 | Validation Accuracy: 0.927449\n",
      "Accuracy on Test data: 0.7565649151802063, 0.5396624207496643\n",
      "Step 6 | Training Loss: 0.664407 | Validation Accuracy: 0.932608\n",
      "Accuracy on Test data: 0.756609320640564, 0.5397468209266663\n",
      "Step 7 | Training Loss: 0.655430 | Validation Accuracy: 0.931418\n",
      "Accuracy on Test data: 0.756609320640564, 0.5397468209266663\n",
      "Step 8 | Training Loss: 0.658851 | Validation Accuracy: 0.927449\n",
      "Accuracy on Test data: 0.7566536664962769, 0.5398312211036682\n",
      "Step 9 | Training Loss: 0.647309 | Validation Accuracy: 0.931021\n",
      "Accuracy on Test data: 0.7566980123519897, 0.5399156212806702\n",
      "Step 10 | Training Loss: 0.652971 | Validation Accuracy: 0.930941\n",
      "Accuracy on Test data: 0.7566980123519897, 0.5399156212806702\n",
      "Step 11 | Training Loss: 0.654602 | Validation Accuracy: 0.928719\n",
      "Accuracy on Test data: 0.7566980123519897, 0.5399156212806702\n",
      "Step 12 | Training Loss: 0.658070 | Validation Accuracy: 0.932767\n",
      "Accuracy on Test data: 0.7566980123519897, 0.5399156212806702\n",
      "Step 13 | Training Loss: 0.657604 | Validation Accuracy: 0.936974\n",
      "Accuracy on Test data: 0.7566980123519897, 0.5399156212806702\n",
      "Step 14 | Training Loss: 0.652099 | Validation Accuracy: 0.932926\n",
      "Accuracy on Test data: 0.7566980123519897, 0.5399156212806702\n",
      "Step 15 | Training Loss: 0.654409 | Validation Accuracy: 0.932132\n",
      "Accuracy on Test data: 0.7567423582077026, 0.5400000214576721\n",
      "Step 16 | Training Loss: 0.653692 | Validation Accuracy: 0.934355\n",
      "Accuracy on Test data: 0.7567423582077026, 0.5400000214576721\n",
      "Step 17 | Training Loss: 0.654075 | Validation Accuracy: 0.934513\n",
      "Accuracy on Test data: 0.7567423582077026, 0.5400000214576721\n",
      "Step 18 | Training Loss: 0.646422 | Validation Accuracy: 0.933005\n",
      "Accuracy on Test data: 0.7567867040634155, 0.5400843620300293\n",
      "Step 19 | Training Loss: 0.650721 | Validation Accuracy: 0.931497\n",
      "Accuracy on Test data: 0.7568311095237732, 0.5401687622070312\n",
      "Step 20 | Training Loss: 0.655260 | Validation Accuracy: 0.929830\n",
      "Accuracy on Test data: 0.7568754553794861, 0.5402531623840332\n",
      "Current Layer Attributes - epochs:20 hidden layers:3 features count:4\n",
      "Step 1 | Training Loss: 0.686800 | Validation Accuracy: 0.538339\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.676832 | Validation Accuracy: 0.529687\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.678068 | Validation Accuracy: 0.536752\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.676266 | Validation Accuracy: 0.719003\n",
      "Accuracy on Test data: 0.5244410634040833, 0.27932488918304443\n",
      "Step 5 | Training Loss: 0.661977 | Validation Accuracy: 0.755279\n",
      "Accuracy on Test data: 0.546353816986084, 0.31873416900634766\n",
      "Step 6 | Training Loss: 0.674460 | Validation Accuracy: 0.790125\n",
      "Accuracy on Test data: 0.571105420589447, 0.3628692030906677\n",
      "Step 7 | Training Loss: 0.662655 | Validation Accuracy: 0.818543\n",
      "Accuracy on Test data: 0.6244677305221558, 0.3913080096244812\n",
      "Step 8 | Training Loss: 0.636697 | Validation Accuracy: 0.834736\n",
      "Accuracy on Test data: 0.6640347838401794, 0.41071730852127075\n",
      "Step 9 | Training Loss: 0.645890 | Validation Accuracy: 0.854977\n",
      "Accuracy on Test data: 0.6910486221313477, 0.43324893712997437\n",
      "Step 10 | Training Loss: 0.630750 | Validation Accuracy: 0.880537\n",
      "Accuracy on Test data: 0.7017831802368164, 0.45181435346603394\n",
      "Step 11 | Training Loss: 0.626354 | Validation Accuracy: 0.894586\n",
      "Accuracy on Test data: 0.7100337147712708, 0.4655696153640747\n",
      "Step 12 | Training Loss: 0.637122 | Validation Accuracy: 0.906255\n",
      "Accuracy on Test data: 0.7156671285629272, 0.47594937682151794\n",
      "Step 13 | Training Loss: 0.639456 | Validation Accuracy: 0.918479\n",
      "Accuracy on Test data: 0.7219659090042114, 0.4871729910373688\n",
      "Step 14 | Training Loss: 0.618803 | Validation Accuracy: 0.924988\n",
      "Accuracy on Test data: 0.730970561504364, 0.504135012626648\n",
      "Step 15 | Training Loss: 0.627863 | Validation Accuracy: 0.933243\n",
      "Accuracy on Test data: 0.7366926670074463, 0.5135865211486816\n",
      "Step 16 | Training Loss: 0.623824 | Validation Accuracy: 0.940864\n",
      "Accuracy on Test data: 0.7405961751937866, 0.5192404985427856\n",
      "Step 17 | Training Loss: 0.623380 | Validation Accuracy: 0.947611\n",
      "Accuracy on Test data: 0.7439673542976379, 0.5247257351875305\n",
      "Step 18 | Training Loss: 0.624790 | Validation Accuracy: 0.951103\n",
      "Accuracy on Test data: 0.7468506097793579, 0.5295358896255493\n",
      "Step 19 | Training Loss: 0.625874 | Validation Accuracy: 0.951659\n",
      "Accuracy on Test data: 0.7522622346878052, 0.5389873385429382\n",
      "Step 20 | Training Loss: 0.624975 | Validation Accuracy: 0.954675\n",
      "Accuracy on Test data: 0.7568754553794861, 0.5448945164680481\n",
      "Step 1 | Training Loss: 0.612960 | Validation Accuracy: 0.956183\n",
      "Accuracy on Test data: 0.757230281829834, 0.545400857925415\n",
      "Step 2 | Training Loss: 0.616639 | Validation Accuracy: 0.953802\n",
      "Accuracy on Test data: 0.7575851678848267, 0.5459915399551392\n",
      "Step 3 | Training Loss: 0.611041 | Validation Accuracy: 0.953882\n",
      "Accuracy on Test data: 0.7578069567680359, 0.5464134812355042\n",
      "Step 4 | Training Loss: 0.615924 | Validation Accuracy: 0.955152\n",
      "Accuracy on Test data: 0.7580287456512451, 0.5468354225158691\n",
      "Step 5 | Training Loss: 0.622639 | Validation Accuracy: 0.955945\n",
      "Accuracy on Test data: 0.7581618428230286, 0.547088623046875\n",
      "Step 6 | Training Loss: 0.614694 | Validation Accuracy: 0.953882\n",
      "Accuracy on Test data: 0.7585166692733765, 0.5477637052536011\n",
      "Step 7 | Training Loss: 0.621504 | Validation Accuracy: 0.949436\n",
      "Accuracy on Test data: 0.7586497664451599, 0.5480169057846069\n",
      "Step 8 | Training Loss: 0.612258 | Validation Accuracy: 0.954358\n",
      "Accuracy on Test data: 0.7587384581565857, 0.5481012463569641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 | Training Loss: 0.621199 | Validation Accuracy: 0.954755\n",
      "Accuracy on Test data: 0.7590045928955078, 0.548607587814331\n",
      "Step 10 | Training Loss: 0.608296 | Validation Accuracy: 0.953088\n",
      "Accuracy on Test data: 0.7591820359230042, 0.548776388168335\n",
      "Step 11 | Training Loss: 0.609780 | Validation Accuracy: 0.957771\n",
      "Accuracy on Test data: 0.7594038248062134, 0.5491983294487\n",
      "Step 12 | Training Loss: 0.598368 | Validation Accuracy: 0.954596\n",
      "Accuracy on Test data: 0.7595369219779968, 0.5493670701980591\n",
      "Step 13 | Training Loss: 0.616361 | Validation Accuracy: 0.951818\n",
      "Accuracy on Test data: 0.7597143650054932, 0.549535870552063\n",
      "Step 14 | Training Loss: 0.618529 | Validation Accuracy: 0.957692\n",
      "Accuracy on Test data: 0.759758710861206, 0.5496202707290649\n",
      "Step 15 | Training Loss: 0.603730 | Validation Accuracy: 0.952453\n",
      "Accuracy on Test data: 0.759803056716919, 0.5497046709060669\n",
      "Step 16 | Training Loss: 0.622472 | Validation Accuracy: 0.955310\n",
      "Accuracy on Test data: 0.7600691914558411, 0.5501265525817871\n",
      "Step 17 | Training Loss: 0.609730 | Validation Accuracy: 0.952850\n",
      "Accuracy on Test data: 0.7604684233665466, 0.550717294216156\n",
      "Step 18 | Training Loss: 0.599785 | Validation Accuracy: 0.952612\n",
      "Accuracy on Test data: 0.7610450387001038, 0.5516455769538879\n",
      "Step 19 | Training Loss: 0.605454 | Validation Accuracy: 0.954596\n",
      "Accuracy on Test data: 0.7613555788993835, 0.5521519184112549\n",
      "Step 20 | Training Loss: 0.612562 | Validation Accuracy: 0.958882\n",
      "Accuracy on Test data: 0.7618435025215149, 0.552827000617981\n",
      "Step 1 | Training Loss: 0.615635 | Validation Accuracy: 0.956501\n",
      "Accuracy on Test data: 0.7618435025215149, 0.552827000617981\n",
      "Step 2 | Training Loss: 0.620265 | Validation Accuracy: 0.954278\n",
      "Accuracy on Test data: 0.7618435025215149, 0.552827000617981\n",
      "Step 3 | Training Loss: 0.611465 | Validation Accuracy: 0.954675\n",
      "Accuracy on Test data: 0.7618878483772278, 0.5529114007949829\n",
      "Step 4 | Training Loss: 0.598396 | Validation Accuracy: 0.955787\n",
      "Accuracy on Test data: 0.7619321942329407, 0.5529114007949829\n",
      "Step 5 | Training Loss: 0.613526 | Validation Accuracy: 0.959121\n",
      "Accuracy on Test data: 0.7619765996932983, 0.5529958009719849\n",
      "Step 6 | Training Loss: 0.595227 | Validation Accuracy: 0.956580\n",
      "Accuracy on Test data: 0.7619765996932983, 0.5529958009719849\n",
      "Step 7 | Training Loss: 0.612104 | Validation Accuracy: 0.956739\n",
      "Accuracy on Test data: 0.7620209455490112, 0.553080141544342\n",
      "Step 8 | Training Loss: 0.609601 | Validation Accuracy: 0.955469\n",
      "Accuracy on Test data: 0.7621539831161499, 0.5533333420753479\n",
      "Step 9 | Training Loss: 0.619813 | Validation Accuracy: 0.955707\n",
      "Accuracy on Test data: 0.7621983885765076, 0.5534177422523499\n",
      "Step 10 | Training Loss: 0.616804 | Validation Accuracy: 0.955549\n",
      "Accuracy on Test data: 0.7621983885765076, 0.5534177422523499\n",
      "Step 11 | Training Loss: 0.610422 | Validation Accuracy: 0.958327\n",
      "Accuracy on Test data: 0.7621983885765076, 0.5534177422523499\n",
      "Step 12 | Training Loss: 0.604882 | Validation Accuracy: 0.953088\n",
      "Accuracy on Test data: 0.7622427344322205, 0.553502082824707\n",
      "Step 13 | Training Loss: 0.599153 | Validation Accuracy: 0.956501\n",
      "Accuracy on Test data: 0.7622427344322205, 0.553502082824707\n",
      "Step 14 | Training Loss: 0.605786 | Validation Accuracy: 0.956263\n",
      "Accuracy on Test data: 0.7622427344322205, 0.553502082824707\n",
      "Step 15 | Training Loss: 0.612585 | Validation Accuracy: 0.956580\n",
      "Accuracy on Test data: 0.7622870802879333, 0.553586483001709\n",
      "Step 16 | Training Loss: 0.610347 | Validation Accuracy: 0.955628\n",
      "Accuracy on Test data: 0.7622870802879333, 0.553586483001709\n",
      "Step 17 | Training Loss: 0.605762 | Validation Accuracy: 0.957374\n",
      "Accuracy on Test data: 0.7622870802879333, 0.553586483001709\n",
      "Step 18 | Training Loss: 0.617582 | Validation Accuracy: 0.957612\n",
      "Accuracy on Test data: 0.7623757719993591, 0.5536708831787109\n",
      "Step 19 | Training Loss: 0.619119 | Validation Accuracy: 0.957136\n",
      "Accuracy on Test data: 0.7624201774597168, 0.5536708831787109\n",
      "Step 20 | Training Loss: 0.599645 | Validation Accuracy: 0.957533\n",
      "Accuracy on Test data: 0.7624201774597168, 0.5536708831787109\n",
      "Current Layer Attributes - epochs:20 hidden layers:3 features count:16\n",
      "Step 1 | Training Loss: 0.682983 | Validation Accuracy: 0.540562\n",
      "Accuracy on Test data: 0.5322924256324768, 0.6433755159378052\n",
      "Step 2 | Training Loss: 0.687802 | Validation Accuracy: 0.822908\n",
      "Accuracy on Test data: 0.8203956484794617, 0.6944303512573242\n",
      "Step 3 | Training Loss: 0.661451 | Validation Accuracy: 0.855691\n",
      "Accuracy on Test data: 0.8243878483772278, 0.6962869167327881\n",
      "Step 4 | Training Loss: 0.654872 | Validation Accuracy: 0.874742\n",
      "Accuracy on Test data: 0.834102213382721, 0.7075949311256409\n",
      "Step 5 | Training Loss: 0.614847 | Validation Accuracy: 0.885379\n",
      "Accuracy on Test data: 0.8400461077690125, 0.7173839807510376\n",
      "Step 6 | Training Loss: 0.621255 | Validation Accuracy: 0.883712\n",
      "Accuracy on Test data: 0.8431955575942993, 0.7210126519203186\n",
      "Step 7 | Training Loss: 0.612731 | Validation Accuracy: 0.894507\n",
      "Accuracy on Test data: 0.8505589365959167, 0.7312236428260803\n",
      "Step 8 | Training Loss: 0.606955 | Validation Accuracy: 0.907604\n",
      "Accuracy on Test data: 0.856813371181488, 0.7393248677253723\n",
      "Step 9 | Training Loss: 0.586146 | Validation Accuracy: 0.909589\n",
      "Accuracy on Test data: 0.860007107257843, 0.7436286807060242\n",
      "Step 10 | Training Loss: 0.573079 | Validation Accuracy: 0.913399\n",
      "Accuracy on Test data: 0.8613821864128113, 0.7457383871078491\n",
      "Step 11 | Training Loss: 0.554965 | Validation Accuracy: 0.923083\n",
      "Accuracy on Test data: 0.862358033657074, 0.7470042109489441\n",
      "Step 12 | Training Loss: 0.562461 | Validation Accuracy: 0.933323\n",
      "Accuracy on Test data: 0.8625354766845703, 0.74725741147995\n",
      "Step 13 | Training Loss: 0.538368 | Validation Accuracy: 0.940229\n",
      "Accuracy on Test data: 0.8621362447738647, 0.746329128742218\n",
      "Step 14 | Training Loss: 0.534164 | Validation Accuracy: 0.944356\n",
      "Accuracy on Test data: 0.8615596294403076, 0.74472576379776\n",
      "Step 15 | Training Loss: 0.522102 | Validation Accuracy: 0.946103\n",
      "Accuracy on Test data: 0.8422639966011047, 0.707426130771637\n",
      "Step 16 | Training Loss: 0.520168 | Validation Accuracy: 0.948801\n",
      "Accuracy on Test data: 0.8332594037055969, 0.6898733973503113\n",
      "Step 17 | Training Loss: 0.521147 | Validation Accuracy: 0.951421\n",
      "Accuracy on Test data: 0.8317955732345581, 0.6862447261810303\n",
      "Step 18 | Training Loss: 0.503646 | Validation Accuracy: 0.948722\n",
      "Accuracy on Test data: 0.831529438495636, 0.6848945021629333\n",
      "Step 19 | Training Loss: 0.488873 | Validation Accuracy: 0.954040\n",
      "Accuracy on Test data: 0.8299769163131714, 0.6814345717430115\n",
      "Step 20 | Training Loss: 0.488496 | Validation Accuracy: 0.952770\n",
      "Accuracy on Test data: 0.8259403705596924, 0.6735864877700806\n",
      "Step 1 | Training Loss: 0.479286 | Validation Accuracy: 0.956025\n",
      "Accuracy on Test data: 0.8254967927932739, 0.6726582050323486\n",
      "Step 2 | Training Loss: 0.502907 | Validation Accuracy: 0.951262\n",
      "Accuracy on Test data: 0.8250975608825684, 0.6718143224716187\n",
      "Step 3 | Training Loss: 0.474367 | Validation Accuracy: 0.954358\n",
      "Accuracy on Test data: 0.8250088691711426, 0.6716455817222595\n",
      "Step 4 | Training Loss: 0.486061 | Validation Accuracy: 0.954199\n",
      "Accuracy on Test data: 0.8247427344322205, 0.6711392402648926\n",
      "Step 5 | Training Loss: 0.483748 | Validation Accuracy: 0.956263\n",
      "Accuracy on Test data: 0.8241660594940186, 0.6700422167778015\n",
      "Step 6 | Training Loss: 0.487499 | Validation Accuracy: 0.955945\n",
      "Accuracy on Test data: 0.8236337900161743, 0.6689451336860657\n",
      "Step 7 | Training Loss: 0.479064 | Validation Accuracy: 0.954120\n",
      "Accuracy on Test data: 0.8234120011329651, 0.6684387922286987\n",
      "Step 8 | Training Loss: 0.500124 | Validation Accuracy: 0.954517\n",
      "Accuracy on Test data: 0.8230571150779724, 0.6676793098449707\n",
      "Step 9 | Training Loss: 0.477483 | Validation Accuracy: 0.953882\n",
      "Accuracy on Test data: 0.8227466344833374, 0.6670886278152466\n",
      "Step 10 | Training Loss: 0.467354 | Validation Accuracy: 0.954834\n",
      "Accuracy on Test data: 0.8224361538887024, 0.6664978861808777\n",
      "Step 11 | Training Loss: 0.480171 | Validation Accuracy: 0.954278\n",
      "Accuracy on Test data: 0.8220369219779968, 0.6657384037971497\n",
      "Step 12 | Training Loss: 0.487758 | Validation Accuracy: 0.954755\n",
      "Accuracy on Test data: 0.8217707872390747, 0.6652320623397827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13 | Training Loss: 0.480520 | Validation Accuracy: 0.955390\n",
      "Accuracy on Test data: 0.821415901184082, 0.6645569801330566\n",
      "Step 14 | Training Loss: 0.478253 | Validation Accuracy: 0.955152\n",
      "Accuracy on Test data: 0.8211941123008728, 0.6641350388526917\n",
      "Step 15 | Training Loss: 0.489915 | Validation Accuracy: 0.957771\n",
      "Accuracy on Test data: 0.8209723234176636, 0.6636286973953247\n",
      "Step 16 | Training Loss: 0.469420 | Validation Accuracy: 0.953802\n",
      "Accuracy on Test data: 0.8201295137405396, 0.6620253324508667\n",
      "Step 17 | Training Loss: 0.457451 | Validation Accuracy: 0.956660\n",
      "Accuracy on Test data: 0.8192423582077026, 0.6603375673294067\n",
      "Step 18 | Training Loss: 0.474396 | Validation Accuracy: 0.954358\n",
      "Accuracy on Test data: 0.8187544345855713, 0.6594092845916748\n",
      "Step 19 | Training Loss: 0.477576 | Validation Accuracy: 0.952056\n",
      "Accuracy on Test data: 0.8183108568191528, 0.6584810018539429\n",
      "Step 20 | Training Loss: 0.469578 | Validation Accuracy: 0.959914\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6578059196472168\n",
      "Step 1 | Training Loss: 0.481450 | Validation Accuracy: 0.955469\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6578059196472168\n",
      "Step 2 | Training Loss: 0.469775 | Validation Accuracy: 0.954913\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6578059196472168\n",
      "Step 3 | Training Loss: 0.487207 | Validation Accuracy: 0.954834\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6578059196472168\n",
      "Step 4 | Training Loss: 0.455901 | Validation Accuracy: 0.955945\n",
      "Accuracy on Test data: 0.8179116249084473, 0.6577215194702148\n",
      "Step 5 | Training Loss: 0.499648 | Validation Accuracy: 0.955390\n",
      "Accuracy on Test data: 0.8179116249084473, 0.6576371192932129\n",
      "Step 6 | Training Loss: 0.467213 | Validation Accuracy: 0.955310\n",
      "Accuracy on Test data: 0.8179116249084473, 0.6576371192932129\n",
      "Step 7 | Training Loss: 0.483610 | Validation Accuracy: 0.957057\n",
      "Accuracy on Test data: 0.8179116249084473, 0.6576371192932129\n",
      "Step 8 | Training Loss: 0.492087 | Validation Accuracy: 0.951818\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6577215194702148\n",
      "Step 9 | Training Loss: 0.467778 | Validation Accuracy: 0.955945\n",
      "Accuracy on Test data: 0.8179559707641602, 0.6577215194702148\n",
      "Step 10 | Training Loss: 0.461614 | Validation Accuracy: 0.956739\n",
      "Accuracy on Test data: 0.8178229331970215, 0.6574683785438538\n",
      "Step 11 | Training Loss: 0.488036 | Validation Accuracy: 0.957612\n",
      "Accuracy on Test data: 0.8177785873413086, 0.6573839783668518\n",
      "Step 12 | Training Loss: 0.488314 | Validation Accuracy: 0.954040\n",
      "Accuracy on Test data: 0.8176011443138123, 0.6570464372634888\n",
      "Step 13 | Training Loss: 0.488491 | Validation Accuracy: 0.956104\n",
      "Accuracy on Test data: 0.8175123929977417, 0.6568776369094849\n",
      "Step 14 | Training Loss: 0.465055 | Validation Accuracy: 0.953564\n",
      "Accuracy on Test data: 0.8175123929977417, 0.6568776369094849\n",
      "Step 15 | Training Loss: 0.472884 | Validation Accuracy: 0.956263\n",
      "Accuracy on Test data: 0.8174237012863159, 0.656708836555481\n",
      "Step 16 | Training Loss: 0.468413 | Validation Accuracy: 0.955866\n",
      "Accuracy on Test data: 0.8174237012863159, 0.656708836555481\n",
      "Step 17 | Training Loss: 0.473089 | Validation Accuracy: 0.954596\n",
      "Accuracy on Test data: 0.8174237012863159, 0.656708836555481\n",
      "Step 18 | Training Loss: 0.485045 | Validation Accuracy: 0.958406\n",
      "Accuracy on Test data: 0.8174237012863159, 0.656708836555481\n",
      "Step 19 | Training Loss: 0.477333 | Validation Accuracy: 0.957136\n",
      "Accuracy on Test data: 0.8172906041145325, 0.6564556956291199\n",
      "Step 20 | Training Loss: 0.461250 | Validation Accuracy: 0.956104\n",
      "Accuracy on Test data: 0.8171575665473938, 0.6562025547027588\n",
      "Current Layer Attributes - epochs:20 hidden layers:3 features count:32\n",
      "Step 1 | Training Loss: 0.670381 | Validation Accuracy: 0.794650\n",
      "Accuracy on Test data: 0.7804293632507324, 0.6891983151435852\n",
      "Step 2 | Training Loss: 0.657980 | Validation Accuracy: 0.858390\n",
      "Accuracy on Test data: 0.7940915822982788, 0.6610126495361328\n",
      "Step 3 | Training Loss: 0.659155 | Validation Accuracy: 0.897444\n",
      "Accuracy on Test data: 0.8066891431808472, 0.6646413207054138\n",
      "Step 4 | Training Loss: 0.643207 | Validation Accuracy: 0.926814\n",
      "Accuracy on Test data: 0.8164921998977661, 0.6691139340400696\n",
      "Step 5 | Training Loss: 0.604498 | Validation Accuracy: 0.931815\n",
      "Accuracy on Test data: 0.8196415901184082, 0.6729114055633545\n",
      "Step 6 | Training Loss: 0.579225 | Validation Accuracy: 0.934752\n",
      "Accuracy on Test data: 0.8030518293380737, 0.6408438682556152\n",
      "Step 7 | Training Loss: 0.582126 | Validation Accuracy: 0.938085\n",
      "Accuracy on Test data: 0.8023420572280884, 0.6351054906845093\n",
      "Step 8 | Training Loss: 0.587918 | Validation Accuracy: 0.944277\n",
      "Accuracy on Test data: 0.8005234003067017, 0.6307172775268555\n",
      "Step 9 | Training Loss: 0.556529 | Validation Accuracy: 0.943483\n",
      "Accuracy on Test data: 0.8014993071556091, 0.6316455602645874\n",
      "Step 10 | Training Loss: 0.543928 | Validation Accuracy: 0.946023\n",
      "Accuracy on Test data: 0.8048704862594604, 0.6372151970863342\n",
      "Step 11 | Training Loss: 0.535201 | Validation Accuracy: 0.947293\n",
      "Accuracy on Test data: 0.8114797472953796, 0.6486920118331909\n",
      "Step 12 | Training Loss: 0.522401 | Validation Accuracy: 0.951659\n",
      "Accuracy on Test data: 0.8163591027259827, 0.6579746603965759\n",
      "Step 13 | Training Loss: 0.518489 | Validation Accuracy: 0.949833\n",
      "Accuracy on Test data: 0.8083747625350952, 0.6425316333770752\n",
      "Step 14 | Training Loss: 0.496319 | Validation Accuracy: 0.955866\n",
      "Accuracy on Test data: 0.8021646738052368, 0.6304641366004944\n",
      "Step 15 | Training Loss: 0.479866 | Validation Accuracy: 0.950706\n",
      "Accuracy on Test data: 0.7972853183746338, 0.6208438873291016\n",
      "Step 16 | Training Loss: 0.460669 | Validation Accuracy: 0.958644\n",
      "Accuracy on Test data: 0.7946681976318359, 0.6153586506843567\n",
      "Step 17 | Training Loss: 0.459698 | Validation Accuracy: 0.955787\n",
      "Accuracy on Test data: 0.7924946546554565, 0.6108016967773438\n",
      "Step 18 | Training Loss: 0.454599 | Validation Accuracy: 0.958644\n",
      "Accuracy on Test data: 0.7890347838401794, 0.6038818359375\n",
      "Step 19 | Training Loss: 0.455301 | Validation Accuracy: 0.958803\n",
      "Accuracy on Test data: 0.7865951061248779, 0.5987342000007629\n",
      "Step 20 | Training Loss: 0.441884 | Validation Accuracy: 0.960867\n",
      "Accuracy on Test data: 0.7836675047874451, 0.5927426218986511\n",
      "Step 1 | Training Loss: 0.423254 | Validation Accuracy: 0.963328\n",
      "Accuracy on Test data: 0.7836231589317322, 0.5925738215446472\n",
      "Step 2 | Training Loss: 0.435564 | Validation Accuracy: 0.965629\n",
      "Accuracy on Test data: 0.7834457159042358, 0.5922362804412842\n",
      "Step 3 | Training Loss: 0.431682 | Validation Accuracy: 0.964201\n",
      "Accuracy on Test data: 0.783401370048523, 0.5921518802642822\n",
      "Step 4 | Training Loss: 0.430573 | Validation Accuracy: 0.963010\n",
      "Accuracy on Test data: 0.7831795811653137, 0.5917299389839172\n",
      "Step 5 | Training Loss: 0.434145 | Validation Accuracy: 0.961184\n",
      "Accuracy on Test data: 0.7830021381378174, 0.5913923978805542\n",
      "Step 6 | Training Loss: 0.427118 | Validation Accuracy: 0.960946\n",
      "Accuracy on Test data: 0.7827803492546082, 0.5909704566001892\n",
      "Step 7 | Training Loss: 0.435275 | Validation Accuracy: 0.964598\n",
      "Accuracy on Test data: 0.7827803492546082, 0.5908860564231873\n",
      "Step 8 | Training Loss: 0.440030 | Validation Accuracy: 0.961899\n",
      "Accuracy on Test data: 0.7823811173439026, 0.5901265740394592\n",
      "Step 9 | Training Loss: 0.440131 | Validation Accuracy: 0.961819\n",
      "Accuracy on Test data: 0.7822036743164062, 0.5897890329360962\n",
      "Step 10 | Training Loss: 0.421181 | Validation Accuracy: 0.961105\n",
      "Accuracy on Test data: 0.781981885433197, 0.5892826914787292\n",
      "Step 11 | Training Loss: 0.433215 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.7818044424057007, 0.5889451503753662\n",
      "Step 12 | Training Loss: 0.458029 | Validation Accuracy: 0.961422\n",
      "Accuracy on Test data: 0.7817600965499878, 0.5887763500213623\n",
      "Step 13 | Training Loss: 0.422335 | Validation Accuracy: 0.960549\n",
      "Accuracy on Test data: 0.7814939618110657, 0.5882700681686401\n",
      "Step 14 | Training Loss: 0.450498 | Validation Accuracy: 0.961264\n",
      "Accuracy on Test data: 0.7812278270721436, 0.5877637267112732\n",
      "Step 15 | Training Loss: 0.424915 | Validation Accuracy: 0.963328\n",
      "Accuracy on Test data: 0.7808729410171509, 0.5870885848999023\n",
      "Step 16 | Training Loss: 0.413430 | Validation Accuracy: 0.960391\n",
      "Accuracy on Test data: 0.7806955575942993, 0.5866666436195374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17 | Training Loss: 0.432100 | Validation Accuracy: 0.962772\n",
      "Accuracy on Test data: 0.7803406715393066, 0.5859915614128113\n",
      "Step 18 | Training Loss: 0.424720 | Validation Accuracy: 0.964836\n",
      "Accuracy on Test data: 0.7801188826560974, 0.5855696201324463\n",
      "Step 19 | Training Loss: 0.427350 | Validation Accuracy: 0.959835\n",
      "Accuracy on Test data: 0.779675304889679, 0.5847257375717163\n",
      "Step 20 | Training Loss: 0.426072 | Validation Accuracy: 0.964518\n",
      "Accuracy on Test data: 0.7794535160064697, 0.5842193961143494\n",
      "Step 1 | Training Loss: 0.421677 | Validation Accuracy: 0.960946\n",
      "Accuracy on Test data: 0.7794535160064697, 0.5842193961143494\n",
      "Step 2 | Training Loss: 0.422438 | Validation Accuracy: 0.960867\n",
      "Accuracy on Test data: 0.7792760729789734, 0.5838818550109863\n",
      "Step 3 | Training Loss: 0.428208 | Validation Accuracy: 0.961978\n",
      "Accuracy on Test data: 0.7791873812675476, 0.5837130546569824\n",
      "Step 4 | Training Loss: 0.426361 | Validation Accuracy: 0.964439\n",
      "Accuracy on Test data: 0.7791873812675476, 0.5836287140846252\n",
      "Step 5 | Training Loss: 0.418258 | Validation Accuracy: 0.961819\n",
      "Accuracy on Test data: 0.7791873812675476, 0.5836287140846252\n",
      "Step 6 | Training Loss: 0.428481 | Validation Accuracy: 0.963724\n",
      "Accuracy on Test data: 0.7791873812675476, 0.5836287140846252\n",
      "Step 7 | Training Loss: 0.416564 | Validation Accuracy: 0.962534\n",
      "Accuracy on Test data: 0.7791430354118347, 0.5835443139076233\n",
      "Step 8 | Training Loss: 0.422879 | Validation Accuracy: 0.960232\n",
      "Accuracy on Test data: 0.7791430354118347, 0.5835443139076233\n",
      "Step 9 | Training Loss: 0.428781 | Validation Accuracy: 0.963724\n",
      "Accuracy on Test data: 0.779098629951477, 0.5834599137306213\n",
      "Step 10 | Training Loss: 0.415011 | Validation Accuracy: 0.961343\n",
      "Accuracy on Test data: 0.779098629951477, 0.5834599137306213\n",
      "Step 11 | Training Loss: 0.421401 | Validation Accuracy: 0.960232\n",
      "Accuracy on Test data: 0.7790542840957642, 0.5833755135536194\n",
      "Step 12 | Training Loss: 0.412278 | Validation Accuracy: 0.964280\n",
      "Accuracy on Test data: 0.7790099382400513, 0.5832911133766174\n",
      "Step 13 | Training Loss: 0.434948 | Validation Accuracy: 0.964201\n",
      "Accuracy on Test data: 0.7790099382400513, 0.5832911133766174\n",
      "Step 14 | Training Loss: 0.426944 | Validation Accuracy: 0.964994\n",
      "Accuracy on Test data: 0.7789655923843384, 0.5832067728042603\n",
      "Step 15 | Training Loss: 0.409857 | Validation Accuracy: 0.962534\n",
      "Accuracy on Test data: 0.7789212465286255, 0.5831223726272583\n",
      "Step 16 | Training Loss: 0.406162 | Validation Accuracy: 0.964042\n",
      "Accuracy on Test data: 0.7789212465286255, 0.5831223726272583\n",
      "Step 17 | Training Loss: 0.427391 | Validation Accuracy: 0.960232\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5830379724502563\n",
      "Step 18 | Training Loss: 0.418130 | Validation Accuracy: 0.964598\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5830379724502563\n",
      "Step 19 | Training Loss: 0.405984 | Validation Accuracy: 0.963328\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5830379724502563\n",
      "Step 20 | Training Loss: 0.419287 | Validation Accuracy: 0.962692\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5830379724502563\n",
      "Current Layer Attributes - epochs:20 hidden layers:3 features count:64\n",
      "Step 1 | Training Loss: 0.705684 | Validation Accuracy: 0.756707\n",
      "Accuracy on Test data: 0.7181511521339417, 0.4825316369533539\n",
      "Step 2 | Training Loss: 0.674949 | Validation Accuracy: 0.830370\n",
      "Accuracy on Test data: 0.7417494654655457, 0.5205063223838806\n",
      "Step 3 | Training Loss: 0.639090 | Validation Accuracy: 0.891253\n",
      "Accuracy on Test data: 0.8068222403526306, 0.6417721509933472\n",
      "Step 4 | Training Loss: 0.586870 | Validation Accuracy: 0.917527\n",
      "Accuracy on Test data: 0.8294003009796143, 0.6832911372184753\n",
      "Step 5 | Training Loss: 0.569587 | Validation Accuracy: 0.920702\n",
      "Accuracy on Test data: 0.8399130702018738, 0.702869176864624\n",
      "Step 6 | Training Loss: 0.539325 | Validation Accuracy: 0.930783\n",
      "Accuracy on Test data: 0.8472321033477783, 0.7137552499771118\n",
      "Step 7 | Training Loss: 0.520396 | Validation Accuracy: 0.932132\n",
      "Accuracy on Test data: 0.850248396396637, 0.7192404866218567\n",
      "Step 8 | Training Loss: 0.496803 | Validation Accuracy: 0.937768\n",
      "Accuracy on Test data: 0.8506919741630554, 0.7197468280792236\n",
      "Step 9 | Training Loss: 0.487368 | Validation Accuracy: 0.938959\n",
      "Accuracy on Test data: 0.847054660320282, 0.7123206853866577\n",
      "Step 10 | Training Loss: 0.447966 | Validation Accuracy: 0.943562\n",
      "Accuracy on Test data: 0.8439052700996399, 0.7070885896682739\n",
      "Step 11 | Training Loss: 0.443464 | Validation Accuracy: 0.945071\n",
      "Accuracy on Test data: 0.8406671285629272, 0.7007594704627991\n",
      "Step 12 | Training Loss: 0.450248 | Validation Accuracy: 0.952612\n",
      "Accuracy on Test data: 0.8374290466308594, 0.694346010684967\n",
      "Step 13 | Training Loss: 0.444124 | Validation Accuracy: 0.952532\n",
      "Accuracy on Test data: 0.8311302065849304, 0.6823628544807434\n",
      "Step 14 | Training Loss: 0.416098 | Validation Accuracy: 0.953405\n",
      "Accuracy on Test data: 0.828335702419281, 0.6769620180130005\n",
      "Step 15 | Training Loss: 0.405827 | Validation Accuracy: 0.958803\n",
      "Accuracy on Test data: 0.8254081010818481, 0.6713080406188965\n",
      "Step 16 | Training Loss: 0.410730 | Validation Accuracy: 0.958168\n",
      "Accuracy on Test data: 0.8230571150779724, 0.6668354272842407\n",
      "Step 17 | Training Loss: 0.406549 | Validation Accuracy: 0.959359\n",
      "Accuracy on Test data: 0.8209279775619507, 0.6627848148345947\n",
      "Step 18 | Training Loss: 0.383228 | Validation Accuracy: 0.959359\n",
      "Accuracy on Test data: 0.8179116249084473, 0.6570464372634888\n",
      "Step 19 | Training Loss: 0.380306 | Validation Accuracy: 0.958089\n",
      "Accuracy on Test data: 0.81538325548172, 0.65223628282547\n",
      "Step 20 | Training Loss: 0.377477 | Validation Accuracy: 0.960152\n",
      "Accuracy on Test data: 0.8121451139450073, 0.6459071636199951\n",
      "Step 1 | Training Loss: 0.379528 | Validation Accuracy: 0.958009\n",
      "Accuracy on Test data: 0.8118789792060852, 0.6454008221626282\n",
      "Step 2 | Training Loss: 0.377845 | Validation Accuracy: 0.960073\n",
      "Accuracy on Test data: 0.8117015361785889, 0.6450632810592651\n",
      "Step 3 | Training Loss: 0.379821 | Validation Accuracy: 0.960391\n",
      "Accuracy on Test data: 0.8114354014396667, 0.6445569396018982\n",
      "Step 4 | Training Loss: 0.379914 | Validation Accuracy: 0.960867\n",
      "Accuracy on Test data: 0.8112136125564575, 0.6441349983215332\n",
      "Step 5 | Training Loss: 0.382986 | Validation Accuracy: 0.961581\n",
      "Accuracy on Test data: 0.8110805749893188, 0.6438818573951721\n",
      "Step 6 | Training Loss: 0.385961 | Validation Accuracy: 0.961422\n",
      "Accuracy on Test data: 0.8107700347900391, 0.643206775188446\n",
      "Step 7 | Training Loss: 0.387194 | Validation Accuracy: 0.962137\n",
      "Accuracy on Test data: 0.8105482459068298, 0.642784833908081\n",
      "Step 8 | Training Loss: 0.381545 | Validation Accuracy: 0.959597\n",
      "Accuracy on Test data: 0.8104152083396912, 0.6425316333770752\n",
      "Step 9 | Training Loss: 0.368052 | Validation Accuracy: 0.960867\n",
      "Accuracy on Test data: 0.8103708028793335, 0.6424472332000732\n",
      "Step 10 | Training Loss: 0.372673 | Validation Accuracy: 0.962057\n",
      "Accuracy on Test data: 0.8102377653121948, 0.6421940922737122\n",
      "Step 11 | Training Loss: 0.374529 | Validation Accuracy: 0.962534\n",
      "Accuracy on Test data: 0.8100603222846985, 0.6417721509933472\n",
      "Step 12 | Training Loss: 0.368542 | Validation Accuracy: 0.960391\n",
      "Accuracy on Test data: 0.8097498416900635, 0.6410970687866211\n",
      "Step 13 | Training Loss: 0.388862 | Validation Accuracy: 0.958882\n",
      "Accuracy on Test data: 0.8092175126075745, 0.6399999856948853\n",
      "Step 14 | Training Loss: 0.374206 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.8089957237243652, 0.6395780444145203\n",
      "Step 15 | Training Loss: 0.377953 | Validation Accuracy: 0.962772\n",
      "Accuracy on Test data: 0.8088626861572266, 0.6393249034881592\n",
      "Step 16 | Training Loss: 0.362846 | Validation Accuracy: 0.964042\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6392405033111572\n",
      "Step 17 | Training Loss: 0.392366 | Validation Accuracy: 0.961819\n",
      "Accuracy on Test data: 0.8086852431297302, 0.6389029622077942\n",
      "Step 18 | Training Loss: 0.384017 | Validation Accuracy: 0.963169\n",
      "Accuracy on Test data: 0.8086408972740173, 0.6388185620307922\n",
      "Step 19 | Training Loss: 0.376258 | Validation Accuracy: 0.960787\n",
      "Accuracy on Test data: 0.8085078001022339, 0.6385654211044312\n",
      "Step 20 | Training Loss: 0.363603 | Validation Accuracy: 0.961661\n",
      "Accuracy on Test data: 0.8084191083908081, 0.6383966207504272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.382039 | Validation Accuracy: 0.960391\n",
      "Accuracy on Test data: 0.808463454246521, 0.6383966207504272\n",
      "Step 2 | Training Loss: 0.378744 | Validation Accuracy: 0.962931\n",
      "Accuracy on Test data: 0.808463454246521, 0.6383966207504272\n",
      "Step 3 | Training Loss: 0.381959 | Validation Accuracy: 0.960629\n",
      "Accuracy on Test data: 0.808463454246521, 0.6383966207504272\n",
      "Step 4 | Training Loss: 0.379745 | Validation Accuracy: 0.959994\n",
      "Accuracy on Test data: 0.808463454246521, 0.6383966207504272\n",
      "Step 5 | Training Loss: 0.382799 | Validation Accuracy: 0.962931\n",
      "Accuracy on Test data: 0.808463454246521, 0.6383966207504272\n",
      "Step 6 | Training Loss: 0.369070 | Validation Accuracy: 0.962057\n",
      "Accuracy on Test data: 0.808463454246521, 0.6383966207504272\n",
      "Step 7 | Training Loss: 0.399125 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.8084191083908081, 0.6383122205734253\n",
      "Step 8 | Training Loss: 0.363575 | Validation Accuracy: 0.961740\n",
      "Accuracy on Test data: 0.8084191083908081, 0.6383122205734253\n",
      "Step 9 | Training Loss: 0.368666 | Validation Accuracy: 0.962296\n",
      "Accuracy on Test data: 0.8084191083908081, 0.6383122205734253\n",
      "Step 10 | Training Loss: 0.384117 | Validation Accuracy: 0.957533\n",
      "Accuracy on Test data: 0.8083747625350952, 0.6382278203964233\n",
      "Step 11 | Training Loss: 0.372889 | Validation Accuracy: 0.960311\n",
      "Accuracy on Test data: 0.8083747625350952, 0.6382278203964233\n",
      "Step 12 | Training Loss: 0.382922 | Validation Accuracy: 0.961899\n",
      "Accuracy on Test data: 0.8083747625350952, 0.6382278203964233\n",
      "Step 13 | Training Loss: 0.362947 | Validation Accuracy: 0.963169\n",
      "Accuracy on Test data: 0.8083747625350952, 0.6382278203964233\n",
      "Step 14 | Training Loss: 0.377336 | Validation Accuracy: 0.959041\n",
      "Accuracy on Test data: 0.8083747625350952, 0.6382278203964233\n",
      "Step 15 | Training Loss: 0.372615 | Validation Accuracy: 0.961026\n",
      "Accuracy on Test data: 0.8083303570747375, 0.6381434798240662\n",
      "Step 16 | Training Loss: 0.385224 | Validation Accuracy: 0.958803\n",
      "Accuracy on Test data: 0.8083303570747375, 0.6381434798240662\n",
      "Step 17 | Training Loss: 0.362795 | Validation Accuracy: 0.962375\n",
      "Accuracy on Test data: 0.8082860112190247, 0.6380590796470642\n",
      "Step 18 | Training Loss: 0.368616 | Validation Accuracy: 0.961740\n",
      "Accuracy on Test data: 0.8082860112190247, 0.6380590796470642\n",
      "Step 19 | Training Loss: 0.368572 | Validation Accuracy: 0.958565\n",
      "Accuracy on Test data: 0.8082860112190247, 0.6380590796470642\n",
      "Step 20 | Training Loss: 0.392864 | Validation Accuracy: 0.963486\n",
      "Accuracy on Test data: 0.8082860112190247, 0.6380590796470642\n",
      "Current Layer Attributes - epochs:20 hidden layers:3 features count:122\n",
      "Step 1 | Training Loss: 0.718218 | Validation Accuracy: 0.576282\n",
      "Accuracy on Test data: 0.5524308085441589, 0.4545147716999054\n",
      "Step 2 | Training Loss: 0.639011 | Validation Accuracy: 0.873154\n",
      "Accuracy on Test data: 0.8329045176506042, 0.7162025570869446\n",
      "Step 3 | Training Loss: 0.611702 | Validation Accuracy: 0.890062\n",
      "Accuracy on Test data: 0.8584989309310913, 0.7485232353210449\n",
      "Step 4 | Training Loss: 0.569566 | Validation Accuracy: 0.898238\n",
      "Accuracy on Test data: 0.8599183559417725, 0.7490295171737671\n",
      "Step 5 | Training Loss: 0.507397 | Validation Accuracy: 0.904588\n",
      "Accuracy on Test data: 0.8619144558906555, 0.750717282295227\n",
      "Step 6 | Training Loss: 0.492400 | Validation Accuracy: 0.915780\n",
      "Accuracy on Test data: 0.8665276765823364, 0.7585654258728027\n",
      "Step 7 | Training Loss: 0.460357 | Validation Accuracy: 0.927131\n",
      "Accuracy on Test data: 0.8360095620155334, 0.6999155879020691\n",
      "Step 8 | Training Loss: 0.445248 | Validation Accuracy: 0.937054\n",
      "Accuracy on Test data: 0.8525106310844421, 0.7272573709487915\n",
      "Step 9 | Training Loss: 0.440087 | Validation Accuracy: 0.941578\n",
      "Accuracy on Test data: 0.844481885433197, 0.7105485200881958\n",
      "Step 10 | Training Loss: 0.420041 | Validation Accuracy: 0.947769\n",
      "Accuracy on Test data: 0.8376508355140686, 0.6970463991165161\n",
      "Step 11 | Training Loss: 0.423408 | Validation Accuracy: 0.954278\n",
      "Accuracy on Test data: 0.8335698843002319, 0.6886919736862183\n",
      "Step 12 | Training Loss: 0.405269 | Validation Accuracy: 0.954278\n",
      "Accuracy on Test data: 0.8297107815742493, 0.6804219484329224\n",
      "Step 13 | Training Loss: 0.396429 | Validation Accuracy: 0.950627\n",
      "Accuracy on Test data: 0.8251863121986389, 0.6716455817222595\n",
      "Step 14 | Training Loss: 0.384430 | Validation Accuracy: 0.956104\n",
      "Accuracy on Test data: 0.8195972442626953, 0.6607595086097717\n",
      "Step 15 | Training Loss: 0.386638 | Validation Accuracy: 0.956263\n",
      "Accuracy on Test data: 0.8154719471931458, 0.6528270244598389\n",
      "Step 16 | Training Loss: 0.385365 | Validation Accuracy: 0.955945\n",
      "Accuracy on Test data: 0.813653290271759, 0.6491139531135559\n",
      "Step 17 | Training Loss: 0.375341 | Validation Accuracy: 0.955310\n",
      "Accuracy on Test data: 0.8136089444160461, 0.64886075258255\n",
      "Step 18 | Training Loss: 0.363837 | Validation Accuracy: 0.959279\n",
      "Accuracy on Test data: 0.8135202527046204, 0.648607611656189\n",
      "Step 19 | Training Loss: 0.367989 | Validation Accuracy: 0.957533\n",
      "Accuracy on Test data: 0.8109031319618225, 0.643628716468811\n",
      "Step 20 | Training Loss: 0.373554 | Validation Accuracy: 0.960708\n",
      "Accuracy on Test data: 0.8108587861061096, 0.6433755159378052\n",
      "Step 1 | Training Loss: 0.370941 | Validation Accuracy: 0.958882\n",
      "Accuracy on Test data: 0.8106369972229004, 0.6429535746574402\n",
      "Step 2 | Training Loss: 0.345104 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.810459554195404, 0.6426160335540771\n",
      "Step 3 | Training Loss: 0.374694 | Validation Accuracy: 0.958168\n",
      "Accuracy on Test data: 0.8104152083396912, 0.6425316333770752\n",
      "Step 4 | Training Loss: 0.372407 | Validation Accuracy: 0.959279\n",
      "Accuracy on Test data: 0.8105039000511169, 0.6427004337310791\n",
      "Step 5 | Training Loss: 0.368257 | Validation Accuracy: 0.959041\n",
      "Accuracy on Test data: 0.8102377653121948, 0.6421940922737122\n",
      "Step 6 | Training Loss: 0.374356 | Validation Accuracy: 0.960549\n",
      "Accuracy on Test data: 0.8105482459068298, 0.6427004337310791\n",
      "Step 7 | Training Loss: 0.373910 | Validation Accuracy: 0.958962\n",
      "Accuracy on Test data: 0.8101934194564819, 0.6419409513473511\n",
      "Step 8 | Training Loss: 0.364542 | Validation Accuracy: 0.961343\n",
      "Accuracy on Test data: 0.8100603222846985, 0.6416877508163452\n",
      "Step 9 | Training Loss: 0.360462 | Validation Accuracy: 0.961661\n",
      "Accuracy on Test data: 0.8100159764289856, 0.6416033506393433\n",
      "Step 10 | Training Loss: 0.375082 | Validation Accuracy: 0.960232\n",
      "Accuracy on Test data: 0.8100159764289856, 0.6416033506393433\n",
      "Step 11 | Training Loss: 0.363244 | Validation Accuracy: 0.959121\n",
      "Accuracy on Test data: 0.809927225112915, 0.6411814093589783\n",
      "Step 12 | Training Loss: 0.370133 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.8098385334014893, 0.6409282684326172\n",
      "Step 13 | Training Loss: 0.355099 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.8098828792572021, 0.6409282684326172\n",
      "Step 14 | Training Loss: 0.369772 | Validation Accuracy: 0.961264\n",
      "Accuracy on Test data: 0.8097941875457764, 0.6407594680786133\n",
      "Step 15 | Training Loss: 0.361567 | Validation Accuracy: 0.961661\n",
      "Accuracy on Test data: 0.8096610903739929, 0.6405063271522522\n",
      "Step 16 | Training Loss: 0.369855 | Validation Accuracy: 0.962057\n",
      "Accuracy on Test data: 0.8096610903739929, 0.6405063271522522\n",
      "Step 17 | Training Loss: 0.366731 | Validation Accuracy: 0.961978\n",
      "Accuracy on Test data: 0.8096610903739929, 0.6405063271522522\n",
      "Step 18 | Training Loss: 0.361514 | Validation Accuracy: 0.960946\n",
      "Accuracy on Test data: 0.8094393014907837, 0.6399999856948853\n",
      "Step 19 | Training Loss: 0.366805 | Validation Accuracy: 0.960311\n",
      "Accuracy on Test data: 0.8093949556350708, 0.6397468447685242\n",
      "Step 20 | Training Loss: 0.362623 | Validation Accuracy: 0.965391\n",
      "Accuracy on Test data: 0.8093949556350708, 0.6395780444145203\n",
      "Step 1 | Training Loss: 0.355789 | Validation Accuracy: 0.961661\n",
      "Accuracy on Test data: 0.8093949556350708, 0.6395780444145203\n",
      "Step 2 | Training Loss: 0.343349 | Validation Accuracy: 0.962534\n",
      "Accuracy on Test data: 0.8093949556350708, 0.6395780444145203\n",
      "Step 3 | Training Loss: 0.373909 | Validation Accuracy: 0.962851\n",
      "Accuracy on Test data: 0.8093949556350708, 0.6395780444145203\n",
      "Step 4 | Training Loss: 0.368176 | Validation Accuracy: 0.962216\n",
      "Accuracy on Test data: 0.8093506097793579, 0.6394936442375183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 | Training Loss: 0.364052 | Validation Accuracy: 0.961978\n",
      "Accuracy on Test data: 0.8093506097793579, 0.6394936442375183\n",
      "Step 6 | Training Loss: 0.359439 | Validation Accuracy: 0.961184\n",
      "Accuracy on Test data: 0.8092175126075745, 0.6392405033111572\n",
      "Step 7 | Training Loss: 0.385892 | Validation Accuracy: 0.963328\n",
      "Accuracy on Test data: 0.8091731667518616, 0.6391561031341553\n",
      "Step 8 | Training Loss: 0.345344 | Validation Accuracy: 0.964121\n",
      "Accuracy on Test data: 0.8092175126075745, 0.6392405033111572\n",
      "Step 9 | Training Loss: 0.360304 | Validation Accuracy: 0.963486\n",
      "Accuracy on Test data: 0.8091731667518616, 0.6391561031341553\n",
      "Step 10 | Training Loss: 0.371345 | Validation Accuracy: 0.962613\n",
      "Accuracy on Test data: 0.8091288208961487, 0.6390717029571533\n",
      "Step 11 | Training Loss: 0.384581 | Validation Accuracy: 0.964201\n",
      "Accuracy on Test data: 0.8091731667518616, 0.6391561031341553\n",
      "Step 12 | Training Loss: 0.365151 | Validation Accuracy: 0.962613\n",
      "Accuracy on Test data: 0.8091288208961487, 0.6390717029571533\n",
      "Step 13 | Training Loss: 0.349615 | Validation Accuracy: 0.962692\n",
      "Accuracy on Test data: 0.8091288208961487, 0.6390717029571533\n",
      "Step 14 | Training Loss: 0.361520 | Validation Accuracy: 0.960946\n",
      "Accuracy on Test data: 0.8090844750404358, 0.6389873623847961\n",
      "Step 15 | Training Loss: 0.355770 | Validation Accuracy: 0.961422\n",
      "Accuracy on Test data: 0.8090844750404358, 0.6389873623847961\n",
      "Step 16 | Training Loss: 0.350413 | Validation Accuracy: 0.963486\n",
      "Accuracy on Test data: 0.8090400695800781, 0.6389029622077942\n",
      "Step 17 | Training Loss: 0.358035 | Validation Accuracy: 0.963804\n",
      "Accuracy on Test data: 0.8090400695800781, 0.6389029622077942\n",
      "Step 18 | Training Loss: 0.368253 | Validation Accuracy: 0.966106\n",
      "Accuracy on Test data: 0.8090844750404358, 0.6389873623847961\n",
      "Step 19 | Training Loss: 0.365640 | Validation Accuracy: 0.962216\n",
      "Accuracy on Test data: 0.8090844750404358, 0.6389873623847961\n",
      "Step 20 | Training Loss: 0.365217 | Validation Accuracy: 0.963486\n",
      "Accuracy on Test data: 0.8091288208961487, 0.6390717029571533\n",
      "Current Layer Attributes - epochs:20 hidden layers:5 features count:1\n",
      "Step 1 | Training Loss: 0.693064 | Validation Accuracy: 0.537228\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.693120 | Validation Accuracy: 0.525798\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.693045 | Validation Accuracy: 0.541276\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692828 | Validation Accuracy: 0.534529\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.692823 | Validation Accuracy: 0.534847\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.693019 | Validation Accuracy: 0.530481\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692696 | Validation Accuracy: 0.537387\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692914 | Validation Accuracy: 0.534371\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.693097 | Validation Accuracy: 0.535402\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.693353 | Validation Accuracy: 0.535482\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.692840 | Validation Accuracy: 0.535244\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.692404 | Validation Accuracy: 0.529608\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.691716 | Validation Accuracy: 0.531275\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.691573 | Validation Accuracy: 0.534450\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.691600 | Validation Accuracy: 0.533497\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 16 | Training Loss: 0.692413 | Validation Accuracy: 0.532307\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 17 | Training Loss: 0.692182 | Validation Accuracy: 0.535164\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 18 | Training Loss: 0.692691 | Validation Accuracy: 0.536514\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 19 | Training Loss: 0.691498 | Validation Accuracy: 0.534212\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 20 | Training Loss: 0.691703 | Validation Accuracy: 0.532227\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.692034 | Validation Accuracy: 0.529767\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692482 | Validation Accuracy: 0.530799\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.692650 | Validation Accuracy: 0.534132\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692248 | Validation Accuracy: 0.539530\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.691612 | Validation Accuracy: 0.535879\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692184 | Validation Accuracy: 0.535958\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.692469 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 8 | Training Loss: 0.692000 | Validation Accuracy: 0.533418\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.693343 | Validation Accuracy: 0.535006\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692344 | Validation Accuracy: 0.536276\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.691632 | Validation Accuracy: 0.539213\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.692339 | Validation Accuracy: 0.537863\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.691560 | Validation Accuracy: 0.535323\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.690354 | Validation Accuracy: 0.538419\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.691668 | Validation Accuracy: 0.535164\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 16 | Training Loss: 0.693778 | Validation Accuracy: 0.533894\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 17 | Training Loss: 0.693173 | Validation Accuracy: 0.536672\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 18 | Training Loss: 0.691831 | Validation Accuracy: 0.534926\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 19 | Training Loss: 0.691642 | Validation Accuracy: 0.542070\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 20 | Training Loss: 0.691945 | Validation Accuracy: 0.532227\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 1 | Training Loss: 0.691513 | Validation Accuracy: 0.537149\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 2 | Training Loss: 0.692560 | Validation Accuracy: 0.522861\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 3 | Training Loss: 0.691942 | Validation Accuracy: 0.527068\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 4 | Training Loss: 0.692312 | Validation Accuracy: 0.535164\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 5 | Training Loss: 0.690892 | Validation Accuracy: 0.535323\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 6 | Training Loss: 0.692496 | Validation Accuracy: 0.534450\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 7 | Training Loss: 0.691323 | Validation Accuracy: 0.535323\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 | Training Loss: 0.691445 | Validation Accuracy: 0.535720\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 9 | Training Loss: 0.692743 | Validation Accuracy: 0.536196\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 10 | Training Loss: 0.692248 | Validation Accuracy: 0.534688\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 11 | Training Loss: 0.691939 | Validation Accuracy: 0.534371\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 12 | Training Loss: 0.693177 | Validation Accuracy: 0.535323\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 13 | Training Loss: 0.692124 | Validation Accuracy: 0.537466\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 14 | Training Loss: 0.692867 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 15 | Training Loss: 0.691255 | Validation Accuracy: 0.535402\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 16 | Training Loss: 0.691688 | Validation Accuracy: 0.531195\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 17 | Training Loss: 0.692866 | Validation Accuracy: 0.539213\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 18 | Training Loss: 0.693611 | Validation Accuracy: 0.533974\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 19 | Training Loss: 0.692991 | Validation Accuracy: 0.532862\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Step 20 | Training Loss: 0.691749 | Validation Accuracy: 0.531037\n",
      "Accuracy on Test data: 0.43075764179229736, 0.18160337209701538\n",
      "Current Layer Attributes - epochs:20 hidden layers:5 features count:4\n",
      "Step 1 | Training Loss: 0.688237 | Validation Accuracy: 0.825607\n",
      "Accuracy on Test data: 0.8521113991737366, 0.7768776416778564\n",
      "Step 2 | Training Loss: 0.685113 | Validation Accuracy: 0.853786\n",
      "Accuracy on Test data: 0.8620918989181519, 0.7718143463134766\n",
      "Step 3 | Training Loss: 0.678661 | Validation Accuracy: 0.874583\n",
      "Accuracy on Test data: 0.8698545098304749, 0.7746835350990295\n",
      "Step 4 | Training Loss: 0.676526 | Validation Accuracy: 0.878314\n",
      "Accuracy on Test data: 0.8714070320129395, 0.7756118178367615\n",
      "Step 5 | Training Loss: 0.674203 | Validation Accuracy: 0.879902\n",
      "Accuracy on Test data: 0.8747782111167908, 0.7770464420318604\n",
      "Step 6 | Training Loss: 0.670014 | Validation Accuracy: 0.889189\n",
      "Accuracy on Test data: 0.875842809677124, 0.7760337591171265\n",
      "Step 7 | Training Loss: 0.675918 | Validation Accuracy: 0.897127\n",
      "Accuracy on Test data: 0.8768630027770996, 0.7762869000434875\n",
      "Step 8 | Training Loss: 0.669542 | Validation Accuracy: 0.904032\n",
      "Accuracy on Test data: 0.8751774430274963, 0.7724894285202026\n",
      "Step 9 | Training Loss: 0.671072 | Validation Accuracy: 0.911970\n",
      "Accuracy on Test data: 0.8736249208450317, 0.7692826986312866\n",
      "Step 10 | Training Loss: 0.662928 | Validation Accuracy: 0.908398\n",
      "Accuracy on Test data: 0.8728264570236206, 0.7675105333328247\n",
      "Step 11 | Training Loss: 0.665545 | Validation Accuracy: 0.918876\n",
      "Accuracy on Test data: 0.8716731667518616, 0.7646413445472717\n",
      "Step 12 | Training Loss: 0.661227 | Validation Accuracy: 0.912446\n",
      "Accuracy on Test data: 0.8711852431297302, 0.7636287212371826\n",
      "Step 13 | Training Loss: 0.667224 | Validation Accuracy: 0.918320\n",
      "Accuracy on Test data: 0.8707416653633118, 0.7626160383224487\n",
      "Step 14 | Training Loss: 0.667725 | Validation Accuracy: 0.923956\n",
      "Accuracy on Test data: 0.8704311847686768, 0.7621096968650818\n",
      "Step 15 | Training Loss: 0.663033 | Validation Accuracy: 0.929195\n",
      "Accuracy on Test data: 0.8702537417411804, 0.7615190148353577\n",
      "Step 16 | Training Loss: 0.661582 | Validation Accuracy: 0.927528\n",
      "Accuracy on Test data: 0.8696327209472656, 0.7601687908172607\n",
      "Step 17 | Training Loss: 0.656298 | Validation Accuracy: 0.933482\n",
      "Accuracy on Test data: 0.8680358529090881, 0.7568776607513428\n",
      "Step 18 | Training Loss: 0.651921 | Validation Accuracy: 0.937212\n",
      "Accuracy on Test data: 0.8666163682937622, 0.754092812538147\n",
      "Step 19 | Training Loss: 0.650263 | Validation Accuracy: 0.936815\n",
      "Accuracy on Test data: 0.8662171959877014, 0.753248929977417\n",
      "Step 20 | Training Loss: 0.657961 | Validation Accuracy: 0.937292\n",
      "Accuracy on Test data: 0.8654630780220032, 0.7514767646789551\n",
      "Step 1 | Training Loss: 0.654756 | Validation Accuracy: 0.937292\n",
      "Accuracy on Test data: 0.8654630780220032, 0.7514767646789551\n",
      "Step 2 | Training Loss: 0.644655 | Validation Accuracy: 0.942292\n",
      "Accuracy on Test data: 0.8655074238777161, 0.751561164855957\n",
      "Step 3 | Training Loss: 0.654837 | Validation Accuracy: 0.940784\n",
      "Accuracy on Test data: 0.8654630780220032, 0.7514767646789551\n",
      "Step 4 | Training Loss: 0.646764 | Validation Accuracy: 0.944197\n",
      "Accuracy on Test data: 0.8654630780220032, 0.7514767646789551\n",
      "Step 5 | Training Loss: 0.654436 | Validation Accuracy: 0.944039\n",
      "Accuracy on Test data: 0.8654187321662903, 0.7513924241065979\n",
      "Step 6 | Training Loss: 0.650128 | Validation Accuracy: 0.943166\n",
      "Accuracy on Test data: 0.8653300404548645, 0.751223623752594\n",
      "Step 7 | Training Loss: 0.656793 | Validation Accuracy: 0.941419\n",
      "Accuracy on Test data: 0.8653300404548645, 0.751223623752594\n",
      "Step 8 | Training Loss: 0.650406 | Validation Accuracy: 0.945309\n",
      "Accuracy on Test data: 0.8653300404548645, 0.751223623752594\n",
      "Step 9 | Training Loss: 0.648520 | Validation Accuracy: 0.943562\n",
      "Accuracy on Test data: 0.8652856349945068, 0.751139223575592\n",
      "Step 10 | Training Loss: 0.657891 | Validation Accuracy: 0.943483\n",
      "Accuracy on Test data: 0.865196943283081, 0.7509704828262329\n",
      "Step 11 | Training Loss: 0.651540 | Validation Accuracy: 0.943245\n",
      "Accuracy on Test data: 0.8650638461112976, 0.750717282295227\n",
      "Step 12 | Training Loss: 0.645250 | Validation Accuracy: 0.941657\n",
      "Accuracy on Test data: 0.8650195002555847, 0.7505485415458679\n",
      "Step 13 | Training Loss: 0.640012 | Validation Accuracy: 0.944991\n",
      "Accuracy on Test data: 0.8649751543998718, 0.7502953410148621\n",
      "Step 14 | Training Loss: 0.646152 | Validation Accuracy: 0.944515\n",
      "Accuracy on Test data: 0.8649308085441589, 0.7502109408378601\n",
      "Step 15 | Training Loss: 0.654167 | Validation Accuracy: 0.950627\n",
      "Accuracy on Test data: 0.8647533655166626, 0.7497890591621399\n",
      "Step 16 | Training Loss: 0.647083 | Validation Accuracy: 0.943721\n",
      "Accuracy on Test data: 0.8644428849220276, 0.749198317527771\n",
      "Step 17 | Training Loss: 0.642101 | Validation Accuracy: 0.939514\n",
      "Accuracy on Test data: 0.8642210960388184, 0.748776376247406\n",
      "Step 18 | Training Loss: 0.653518 | Validation Accuracy: 0.947214\n",
      "Accuracy on Test data: 0.864043653011322, 0.748438835144043\n",
      "Step 19 | Training Loss: 0.656491 | Validation Accuracy: 0.944197\n",
      "Accuracy on Test data: 0.864043653011322, 0.748354434967041\n",
      "Step 20 | Training Loss: 0.649095 | Validation Accuracy: 0.944039\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 1 | Training Loss: 0.649194 | Validation Accuracy: 0.942134\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 2 | Training Loss: 0.644502 | Validation Accuracy: 0.946817\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 3 | Training Loss: 0.654865 | Validation Accuracy: 0.941261\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 4 | Training Loss: 0.637645 | Validation Accuracy: 0.941896\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 5 | Training Loss: 0.648788 | Validation Accuracy: 0.947055\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 6 | Training Loss: 0.654804 | Validation Accuracy: 0.944118\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 7 | Training Loss: 0.661607 | Validation Accuracy: 0.944197\n",
      "Accuracy on Test data: 0.8638662099838257, 0.748016893863678\n",
      "Step 8 | Training Loss: 0.661941 | Validation Accuracy: 0.945547\n",
      "Accuracy on Test data: 0.8638218641281128, 0.747932493686676\n",
      "Step 9 | Training Loss: 0.642678 | Validation Accuracy: 0.945785\n",
      "Accuracy on Test data: 0.8638218641281128, 0.747932493686676\n",
      "Step 10 | Training Loss: 0.656878 | Validation Accuracy: 0.946658\n",
      "Accuracy on Test data: 0.8638218641281128, 0.747932493686676\n",
      "Step 11 | Training Loss: 0.645933 | Validation Accuracy: 0.946261\n",
      "Accuracy on Test data: 0.8638218641281128, 0.7478480935096741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12 | Training Loss: 0.644326 | Validation Accuracy: 0.941181\n",
      "Accuracy on Test data: 0.8637775182723999, 0.7477636933326721\n",
      "Step 13 | Training Loss: 0.652975 | Validation Accuracy: 0.942531\n",
      "Accuracy on Test data: 0.863733172416687, 0.7476793527603149\n",
      "Step 14 | Training Loss: 0.647514 | Validation Accuracy: 0.946738\n",
      "Accuracy on Test data: 0.863733172416687, 0.7476793527603149\n",
      "Step 15 | Training Loss: 0.654443 | Validation Accuracy: 0.945864\n",
      "Accuracy on Test data: 0.863733172416687, 0.7476793527603149\n",
      "Step 16 | Training Loss: 0.642780 | Validation Accuracy: 0.948166\n",
      "Accuracy on Test data: 0.8636887669563293, 0.747594952583313\n",
      "Step 17 | Training Loss: 0.649232 | Validation Accuracy: 0.948166\n",
      "Accuracy on Test data: 0.8636887669563293, 0.747594952583313\n",
      "Step 18 | Training Loss: 0.652166 | Validation Accuracy: 0.947134\n",
      "Accuracy on Test data: 0.8637775182723999, 0.7476793527603149\n",
      "Step 19 | Training Loss: 0.652762 | Validation Accuracy: 0.948722\n",
      "Accuracy on Test data: 0.8637775182723999, 0.7476793527603149\n",
      "Step 20 | Training Loss: 0.649702 | Validation Accuracy: 0.946499\n",
      "Accuracy on Test data: 0.8637775182723999, 0.7476793527603149\n",
      "Current Layer Attributes - epochs:20 hidden layers:5 features count:16\n",
      "Step 1 | Training Loss: 0.717928 | Validation Accuracy: 0.385379\n",
      "Accuracy on Test data: 0.37921398878097534, 0.3914767801761627\n",
      "Step 2 | Training Loss: 0.725961 | Validation Accuracy: 0.533339\n",
      "Accuracy on Test data: 0.5183640718460083, 0.39924049377441406\n",
      "Step 3 | Training Loss: 0.736640 | Validation Accuracy: 0.579536\n",
      "Accuracy on Test data: 0.5509669780731201, 0.41274261474609375\n",
      "Step 4 | Training Loss: 0.714248 | Validation Accuracy: 0.614383\n",
      "Accuracy on Test data: 0.5724804997444153, 0.41552743315696716\n",
      "Step 5 | Training Loss: 0.710880 | Validation Accuracy: 0.661930\n",
      "Accuracy on Test data: 0.5863201022148132, 0.4166244864463806\n",
      "Step 6 | Training Loss: 0.707020 | Validation Accuracy: 0.688046\n",
      "Accuracy on Test data: 0.5886710286140442, 0.4187341630458832\n",
      "Step 7 | Training Loss: 0.709598 | Validation Accuracy: 0.701461\n",
      "Accuracy on Test data: 0.595901370048523, 0.41915610432624817\n",
      "Step 8 | Training Loss: 0.697786 | Validation Accuracy: 0.703048\n",
      "Accuracy on Test data: 0.5974982380867004, 0.42075949907302856\n",
      "Step 9 | Training Loss: 0.714030 | Validation Accuracy: 0.707096\n",
      "Accuracy on Test data: 0.598518431186676, 0.421856552362442\n",
      "Step 10 | Training Loss: 0.684962 | Validation Accuracy: 0.710668\n",
      "Accuracy on Test data: 0.5958569645881653, 0.4158649742603302\n",
      "Step 11 | Training Loss: 0.696205 | Validation Accuracy: 0.724639\n",
      "Accuracy on Test data: 0.5926188826560974, 0.40953585505485535\n",
      "Step 12 | Training Loss: 0.685322 | Validation Accuracy: 0.734164\n",
      "Accuracy on Test data: 0.592175304889679, 0.4081856608390808\n",
      "Step 13 | Training Loss: 0.688944 | Validation Accuracy: 0.749246\n",
      "Accuracy on Test data: 0.5869410634040833, 0.3965400755405426\n",
      "Step 14 | Training Loss: 0.667187 | Validation Accuracy: 0.754405\n",
      "Accuracy on Test data: 0.5876951813697815, 0.3956961929798126\n",
      "Step 15 | Training Loss: 0.669762 | Validation Accuracy: 0.761867\n",
      "Accuracy on Test data: 0.5907558798789978, 0.40042194724082947\n",
      "Step 16 | Training Loss: 0.672281 | Validation Accuracy: 0.767820\n",
      "Accuracy on Test data: 0.5851224064826965, 0.38734176754951477\n",
      "Step 17 | Training Loss: 0.668377 | Validation Accuracy: 0.781950\n",
      "Accuracy on Test data: 0.5795777440071106, 0.37493669986724854\n",
      "Step 18 | Training Loss: 0.661609 | Validation Accuracy: 0.804810\n",
      "Accuracy on Test data: 0.5914212465286255, 0.3956961929798126\n",
      "Step 19 | Training Loss: 0.684138 | Validation Accuracy: 0.834974\n",
      "Accuracy on Test data: 0.6959279775619507, 0.43468353152275085\n",
      "Step 20 | Training Loss: 0.679040 | Validation Accuracy: 0.866963\n",
      "Accuracy on Test data: 0.7329222559928894, 0.503713071346283\n",
      "Step 1 | Training Loss: 0.673209 | Validation Accuracy: 0.866010\n",
      "Accuracy on Test data: 0.7356724739074707, 0.5089451670646667\n",
      "Step 2 | Training Loss: 0.653368 | Validation Accuracy: 0.877838\n",
      "Accuracy on Test data: 0.7372693419456482, 0.5119830965995789\n",
      "Step 3 | Training Loss: 0.653497 | Validation Accuracy: 0.872837\n",
      "Accuracy on Test data: 0.7381564974784851, 0.5135865211486816\n",
      "Step 4 | Training Loss: 0.656490 | Validation Accuracy: 0.876885\n",
      "Accuracy on Test data: 0.7399308085441589, 0.5169620513916016\n",
      "Step 5 | Training Loss: 0.650994 | Validation Accuracy: 0.880933\n",
      "Accuracy on Test data: 0.741394579410553, 0.5196624398231506\n",
      "Step 6 | Training Loss: 0.648257 | Validation Accuracy: 0.883394\n",
      "Accuracy on Test data: 0.741926908493042, 0.5206751227378845\n",
      "Step 7 | Training Loss: 0.661743 | Validation Accuracy: 0.890062\n",
      "Accuracy on Test data: 0.7426366209983826, 0.5220252871513367\n",
      "Step 8 | Training Loss: 0.669659 | Validation Accuracy: 0.889665\n",
      "Accuracy on Test data: 0.7429027557373047, 0.5225316286087036\n",
      "Step 9 | Training Loss: 0.644664 | Validation Accuracy: 0.893555\n",
      "Accuracy on Test data: 0.7440117001533508, 0.5245569348335266\n",
      "Step 10 | Training Loss: 0.649040 | Validation Accuracy: 0.896888\n",
      "Accuracy on Test data: 0.7446770668029785, 0.5257384181022644\n",
      "Step 11 | Training Loss: 0.632964 | Validation Accuracy: 0.899111\n",
      "Accuracy on Test data: 0.7456973195075989, 0.5272573828697205\n",
      "Step 12 | Training Loss: 0.652505 | Validation Accuracy: 0.898397\n",
      "Accuracy on Test data: 0.7469836473464966, 0.5295358896255493\n",
      "Step 13 | Training Loss: 0.653677 | Validation Accuracy: 0.900064\n",
      "Accuracy on Test data: 0.7475603222846985, 0.5303797721862793\n",
      "Step 14 | Training Loss: 0.645163 | Validation Accuracy: 0.908557\n",
      "Accuracy on Test data: 0.7486249208450317, 0.5322362780570984\n",
      "Step 15 | Training Loss: 0.653290 | Validation Accuracy: 0.902604\n",
      "Accuracy on Test data: 0.7491128444671631, 0.5331645607948303\n",
      "Step 16 | Training Loss: 0.668207 | Validation Accuracy: 0.906255\n",
      "Accuracy on Test data: 0.7492902874946594, 0.5335021018981934\n",
      "Step 17 | Training Loss: 0.652027 | Validation Accuracy: 0.916812\n",
      "Accuracy on Test data: 0.7497782111167908, 0.5344303846359253\n",
      "Step 18 | Training Loss: 0.647565 | Validation Accuracy: 0.911018\n",
      "Accuracy on Test data: 0.7506210207939148, 0.5359493494033813\n",
      "Step 19 | Training Loss: 0.639784 | Validation Accuracy: 0.907525\n",
      "Accuracy on Test data: 0.7511976361274719, 0.5367088317871094\n",
      "Step 20 | Training Loss: 0.646359 | Validation Accuracy: 0.913716\n",
      "Accuracy on Test data: 0.751685619354248, 0.5376371145248413\n",
      "Step 1 | Training Loss: 0.639890 | Validation Accuracy: 0.911335\n",
      "Accuracy on Test data: 0.7517299652099609, 0.5377215147018433\n",
      "Step 2 | Training Loss: 0.633000 | Validation Accuracy: 0.908874\n",
      "Accuracy on Test data: 0.7518630027770996, 0.5379746556282043\n",
      "Step 3 | Training Loss: 0.646788 | Validation Accuracy: 0.914193\n",
      "Accuracy on Test data: 0.7519074082374573, 0.5380590558052063\n",
      "Step 4 | Training Loss: 0.659935 | Validation Accuracy: 0.909668\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5381434559822083\n",
      "Step 5 | Training Loss: 0.636705 | Validation Accuracy: 0.910541\n",
      "Accuracy on Test data: 0.7519517540931702, 0.5381434559822083\n",
      "Step 6 | Training Loss: 0.650009 | Validation Accuracy: 0.908001\n",
      "Accuracy on Test data: 0.752040445804596, 0.5383122563362122\n",
      "Step 7 | Training Loss: 0.638107 | Validation Accuracy: 0.910065\n",
      "Accuracy on Test data: 0.7521291971206665, 0.5383965969085693\n",
      "Step 8 | Training Loss: 0.665931 | Validation Accuracy: 0.911970\n",
      "Accuracy on Test data: 0.7522178888320923, 0.5385653972625732\n",
      "Step 9 | Training Loss: 0.633874 | Validation Accuracy: 0.909192\n",
      "Accuracy on Test data: 0.7521735429763794, 0.5384809970855713\n",
      "Step 10 | Training Loss: 0.666023 | Validation Accuracy: 0.913161\n",
      "Accuracy on Test data: 0.7522178888320923, 0.5385653972625732\n",
      "Step 11 | Training Loss: 0.650310 | Validation Accuracy: 0.906176\n",
      "Accuracy on Test data: 0.7522622346878052, 0.5386497974395752\n",
      "Step 12 | Training Loss: 0.646818 | Validation Accuracy: 0.912605\n",
      "Accuracy on Test data: 0.7523065805435181, 0.5387341976165771\n",
      "Step 13 | Training Loss: 0.653062 | Validation Accuracy: 0.909906\n",
      "Accuracy on Test data: 0.7523065805435181, 0.5387341976165771\n",
      "Step 14 | Training Loss: 0.642678 | Validation Accuracy: 0.915145\n",
      "Accuracy on Test data: 0.7523065805435181, 0.5387341976165771\n",
      "Step 15 | Training Loss: 0.638611 | Validation Accuracy: 0.911018\n",
      "Accuracy on Test data: 0.7523509860038757, 0.5388185381889343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16 | Training Loss: 0.655655 | Validation Accuracy: 0.912526\n",
      "Accuracy on Test data: 0.7523509860038757, 0.5388185381889343\n",
      "Step 17 | Training Loss: 0.654967 | Validation Accuracy: 0.913637\n",
      "Accuracy on Test data: 0.7523509860038757, 0.5388185381889343\n",
      "Step 18 | Training Loss: 0.652377 | Validation Accuracy: 0.911018\n",
      "Accuracy on Test data: 0.7523065805435181, 0.5387341976165771\n",
      "Step 19 | Training Loss: 0.622419 | Validation Accuracy: 0.910700\n",
      "Accuracy on Test data: 0.7523509860038757, 0.5388185381889343\n",
      "Step 20 | Training Loss: 0.641479 | Validation Accuracy: 0.912923\n",
      "Accuracy on Test data: 0.7523953318595886, 0.5389029383659363\n",
      "Current Layer Attributes - epochs:20 hidden layers:5 features count:32\n",
      "Step 1 | Training Loss: 0.740122 | Validation Accuracy: 0.542467\n",
      "Accuracy on Test data: 0.4309350550174713, 0.1817721575498581\n",
      "Step 2 | Training Loss: 0.746009 | Validation Accuracy: 0.538816\n",
      "Accuracy on Test data: 0.43204399943351746, 0.1833755224943161\n",
      "Step 3 | Training Loss: 0.743248 | Validation Accuracy: 0.536514\n",
      "Accuracy on Test data: 0.4322657883167267, 0.1837974637746811\n",
      "Step 4 | Training Loss: 0.721960 | Validation Accuracy: 0.549611\n",
      "Accuracy on Test data: 0.43479418754577637, 0.18784810602664948\n",
      "Step 5 | Training Loss: 0.702883 | Validation Accuracy: 0.558025\n",
      "Accuracy on Test data: 0.43732255697250366, 0.19147679209709167\n",
      "Step 6 | Training Loss: 0.724796 | Validation Accuracy: 0.555009\n",
      "Accuracy on Test data: 0.4418470561504364, 0.19797468185424805\n",
      "Step 7 | Training Loss: 0.690333 | Validation Accuracy: 0.588189\n",
      "Accuracy on Test data: 0.5342885255813599, 0.21248945593833923\n",
      "Step 8 | Training Loss: 0.677144 | Validation Accuracy: 0.712653\n",
      "Accuracy on Test data: 0.5874290466308594, 0.238734170794487\n",
      "Step 9 | Training Loss: 0.658322 | Validation Accuracy: 0.775917\n",
      "Accuracy on Test data: 0.6261976361274719, 0.3091983199119568\n",
      "Step 10 | Training Loss: 0.652778 | Validation Accuracy: 0.827115\n",
      "Accuracy on Test data: 0.6721965670585632, 0.3908860683441162\n",
      "Step 11 | Training Loss: 0.658183 | Validation Accuracy: 0.864026\n",
      "Accuracy on Test data: 0.6910929679870605, 0.42506328225135803\n",
      "Step 12 | Training Loss: 0.658796 | Validation Accuracy: 0.882839\n",
      "Accuracy on Test data: 0.7120298147201538, 0.4597468376159668\n",
      "Step 13 | Training Loss: 0.639323 | Validation Accuracy: 0.901889\n",
      "Accuracy on Test data: 0.7353619337081909, 0.5033755302429199\n",
      "Step 14 | Training Loss: 0.631274 | Validation Accuracy: 0.921337\n",
      "Accuracy on Test data: 0.7517743110656738, 0.5340084433555603\n",
      "Step 15 | Training Loss: 0.622287 | Validation Accuracy: 0.930386\n",
      "Accuracy on Test data: 0.7737757563591003, 0.5733333230018616\n",
      "Step 16 | Training Loss: 0.601932 | Validation Accuracy: 0.935387\n",
      "Accuracy on Test data: 0.7857966423034668, 0.5956118106842041\n",
      "Step 17 | Training Loss: 0.603294 | Validation Accuracy: 0.936101\n",
      "Accuracy on Test data: 0.7869943380355835, 0.5989029407501221\n",
      "Step 18 | Training Loss: 0.576331 | Validation Accuracy: 0.935307\n",
      "Accuracy on Test data: 0.7881476283073425, 0.601097047328949\n",
      "Step 19 | Training Loss: 0.585469 | Validation Accuracy: 0.938562\n",
      "Accuracy on Test data: 0.7953335642814636, 0.6145147681236267\n",
      "Step 20 | Training Loss: 0.552845 | Validation Accuracy: 0.941499\n",
      "Accuracy on Test data: 0.8023420572280884, 0.6277636885643005\n",
      "Step 1 | Training Loss: 0.550575 | Validation Accuracy: 0.941578\n",
      "Accuracy on Test data: 0.8026525974273682, 0.6283544301986694\n",
      "Step 2 | Training Loss: 0.561362 | Validation Accuracy: 0.940705\n",
      "Accuracy on Test data: 0.8035397529602051, 0.6300421953201294\n",
      "Step 3 | Training Loss: 0.540667 | Validation Accuracy: 0.942769\n",
      "Accuracy on Test data: 0.8046043515205383, 0.6320675015449524\n",
      "Step 4 | Training Loss: 0.553878 | Validation Accuracy: 0.937133\n",
      "Accuracy on Test data: 0.8054027557373047, 0.6335865259170532\n",
      "Step 5 | Training Loss: 0.550040 | Validation Accuracy: 0.941261\n",
      "Accuracy on Test data: 0.8059350848197937, 0.6345991492271423\n",
      "Step 6 | Training Loss: 0.542859 | Validation Accuracy: 0.938165\n",
      "Accuracy on Test data: 0.806423008441925, 0.6355274319648743\n",
      "Step 7 | Training Loss: 0.553095 | Validation Accuracy: 0.943324\n",
      "Accuracy on Test data: 0.8066891431808472, 0.6360337734222412\n",
      "Step 8 | Training Loss: 0.541485 | Validation Accuracy: 0.936815\n",
      "Accuracy on Test data: 0.8070883750915527, 0.6367932558059692\n",
      "Step 9 | Training Loss: 0.556820 | Validation Accuracy: 0.940229\n",
      "Accuracy on Test data: 0.8074432015419006, 0.6374683380126953\n",
      "Step 10 | Training Loss: 0.550724 | Validation Accuracy: 0.941896\n",
      "Accuracy on Test data: 0.807620644569397, 0.6378058791160583\n",
      "Step 11 | Training Loss: 0.554044 | Validation Accuracy: 0.940625\n",
      "Accuracy on Test data: 0.8078867793083191, 0.6383122205734253\n",
      "Step 12 | Training Loss: 0.544203 | Validation Accuracy: 0.941975\n",
      "Accuracy on Test data: 0.8080198764801025, 0.6385654211044312\n",
      "Step 13 | Training Loss: 0.531278 | Validation Accuracy: 0.940705\n",
      "Accuracy on Test data: 0.8082860112190247, 0.6390717029571533\n",
      "Step 14 | Training Loss: 0.516991 | Validation Accuracy: 0.948087\n",
      "Accuracy on Test data: 0.8085965514183044, 0.6396624445915222\n",
      "Step 15 | Training Loss: 0.518841 | Validation Accuracy: 0.940070\n",
      "Accuracy on Test data: 0.8086408972740173, 0.6397468447685242\n",
      "Step 16 | Training Loss: 0.526310 | Validation Accuracy: 0.943166\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 17 | Training Loss: 0.532672 | Validation Accuracy: 0.940308\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 18 | Training Loss: 0.548486 | Validation Accuracy: 0.943245\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 19 | Training Loss: 0.527513 | Validation Accuracy: 0.942134\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 20 | Training Loss: 0.542257 | Validation Accuracy: 0.940705\n",
      "Accuracy on Test data: 0.8089070320129395, 0.6402531862258911\n",
      "Step 1 | Training Loss: 0.527994 | Validation Accuracy: 0.941022\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 2 | Training Loss: 0.536680 | Validation Accuracy: 0.943324\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 3 | Training Loss: 0.527268 | Validation Accuracy: 0.942292\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 4 | Training Loss: 0.525355 | Validation Accuracy: 0.942769\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 5 | Training Loss: 0.538879 | Validation Accuracy: 0.943642\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 6 | Training Loss: 0.548039 | Validation Accuracy: 0.945468\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 7 | Training Loss: 0.535582 | Validation Accuracy: 0.946103\n",
      "Accuracy on Test data: 0.8088183403015137, 0.6400843858718872\n",
      "Step 8 | Training Loss: 0.529117 | Validation Accuracy: 0.943404\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 9 | Training Loss: 0.553238 | Validation Accuracy: 0.941816\n",
      "Accuracy on Test data: 0.8087295889854431, 0.6399155855178833\n",
      "Step 10 | Training Loss: 0.520283 | Validation Accuracy: 0.944039\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 11 | Training Loss: 0.554617 | Validation Accuracy: 0.945468\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 12 | Training Loss: 0.539825 | Validation Accuracy: 0.943959\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 13 | Training Loss: 0.537236 | Validation Accuracy: 0.943880\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 14 | Training Loss: 0.538950 | Validation Accuracy: 0.945309\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399999856948853\n",
      "Step 15 | Training Loss: 0.541068 | Validation Accuracy: 0.945864\n",
      "Accuracy on Test data: 0.8087295889854431, 0.6399155855178833\n",
      "Step 16 | Training Loss: 0.536238 | Validation Accuracy: 0.945785\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399155855178833\n",
      "Step 17 | Training Loss: 0.516449 | Validation Accuracy: 0.946023\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399155855178833\n",
      "Step 18 | Training Loss: 0.521591 | Validation Accuracy: 0.944197\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399155855178833\n",
      "Step 19 | Training Loss: 0.529132 | Validation Accuracy: 0.944912\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399155855178833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20 | Training Loss: 0.524212 | Validation Accuracy: 0.944515\n",
      "Accuracy on Test data: 0.808773934841156, 0.6399155855178833\n",
      "Current Layer Attributes - epochs:20 hidden layers:5 features count:64\n",
      "Step 1 | Training Loss: 0.712067 | Validation Accuracy: 0.560010\n",
      "Accuracy on Test data: 0.43674591183662415, 0.19248944520950317\n",
      "Step 2 | Training Loss: 0.722781 | Validation Accuracy: 0.623035\n",
      "Accuracy on Test data: 0.5243080258369446, 0.3532489538192749\n",
      "Step 3 | Training Loss: 0.710759 | Validation Accuracy: 0.810367\n",
      "Accuracy on Test data: 0.7234740853309631, 0.4935865104198456\n",
      "Step 4 | Training Loss: 0.698079 | Validation Accuracy: 0.856565\n",
      "Accuracy on Test data: 0.7674325704574585, 0.5732489228248596\n",
      "Step 5 | Training Loss: 0.664673 | Validation Accuracy: 0.901334\n",
      "Accuracy on Test data: 0.788591206073761, 0.6124050617218018\n",
      "Step 6 | Training Loss: 0.659139 | Validation Accuracy: 0.908716\n",
      "Accuracy on Test data: 0.7970635294914246, 0.6264978647232056\n",
      "Step 7 | Training Loss: 0.641198 | Validation Accuracy: 0.923401\n",
      "Accuracy on Test data: 0.7916074991226196, 0.6140084266662598\n",
      "Step 8 | Training Loss: 0.619980 | Validation Accuracy: 0.932767\n",
      "Accuracy on Test data: 0.7920510768890381, 0.6127426028251648\n",
      "Step 9 | Training Loss: 0.625476 | Validation Accuracy: 0.939038\n",
      "Accuracy on Test data: 0.7901880741119385, 0.6069198250770569\n",
      "Step 10 | Training Loss: 0.576613 | Validation Accuracy: 0.946023\n",
      "Accuracy on Test data: 0.7879258394241333, 0.6016877889633179\n",
      "Step 11 | Training Loss: 0.564245 | Validation Accuracy: 0.946023\n",
      "Accuracy on Test data: 0.7898775935173035, 0.6040506362915039\n",
      "Step 12 | Training Loss: 0.522804 | Validation Accuracy: 0.947531\n",
      "Accuracy on Test data: 0.7911195755004883, 0.6058228015899658\n",
      "Step 13 | Training Loss: 0.508467 | Validation Accuracy: 0.953008\n",
      "Accuracy on Test data: 0.7909865379333496, 0.6054008603096008\n",
      "Step 14 | Training Loss: 0.487849 | Validation Accuracy: 0.954278\n",
      "Accuracy on Test data: 0.790587306022644, 0.6043881773948669\n",
      "Step 15 | Training Loss: 0.483139 | Validation Accuracy: 0.952453\n",
      "Accuracy on Test data: 0.7902324199676514, 0.6033755540847778\n",
      "Step 16 | Training Loss: 0.482337 | Validation Accuracy: 0.954517\n",
      "Accuracy on Test data: 0.7900993824005127, 0.6029536128044128\n",
      "Step 17 | Training Loss: 0.439553 | Validation Accuracy: 0.957612\n",
      "Accuracy on Test data: 0.787748396396637, 0.5984809994697571\n",
      "Step 18 | Training Loss: 0.438859 | Validation Accuracy: 0.959438\n",
      "Accuracy on Test data: 0.7857522964477539, 0.5945991277694702\n",
      "Step 19 | Training Loss: 0.428129 | Validation Accuracy: 0.958962\n",
      "Accuracy on Test data: 0.7843328714370728, 0.5918143391609192\n",
      "Step 20 | Training Loss: 0.421388 | Validation Accuracy: 0.959597\n",
      "Accuracy on Test data: 0.7825585603713989, 0.5884388089179993\n",
      "Step 1 | Training Loss: 0.399108 | Validation Accuracy: 0.961502\n",
      "Accuracy on Test data: 0.7824254631996155, 0.5881856679916382\n",
      "Step 2 | Training Loss: 0.426352 | Validation Accuracy: 0.960708\n",
      "Accuracy on Test data: 0.7822480201721191, 0.5878481268882751\n",
      "Step 3 | Training Loss: 0.420135 | Validation Accuracy: 0.962772\n",
      "Accuracy on Test data: 0.7820706367492676, 0.5875105261802673\n",
      "Step 4 | Training Loss: 0.441123 | Validation Accuracy: 0.962772\n",
      "Accuracy on Test data: 0.7818488478660583, 0.5870885848999023\n",
      "Step 5 | Training Loss: 0.409880 | Validation Accuracy: 0.957692\n",
      "Accuracy on Test data: 0.7816270589828491, 0.5866666436195374\n",
      "Step 6 | Training Loss: 0.426141 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.7815826535224915, 0.5865823030471802\n",
      "Step 7 | Training Loss: 0.418120 | Validation Accuracy: 0.961819\n",
      "Accuracy on Test data: 0.7814496159553528, 0.5863291025161743\n",
      "Step 8 | Training Loss: 0.417343 | Validation Accuracy: 0.960073\n",
      "Accuracy on Test data: 0.7813165187835693, 0.5860759615898132\n",
      "Step 9 | Training Loss: 0.398573 | Validation Accuracy: 0.959914\n",
      "Accuracy on Test data: 0.7812721729278564, 0.5859915614128113\n",
      "Step 10 | Training Loss: 0.397813 | Validation Accuracy: 0.964042\n",
      "Accuracy on Test data: 0.7811834812164307, 0.5858227610588074\n",
      "Step 11 | Training Loss: 0.415414 | Validation Accuracy: 0.963407\n",
      "Accuracy on Test data: 0.7810947299003601, 0.5856540203094482\n",
      "Step 12 | Training Loss: 0.418852 | Validation Accuracy: 0.961899\n",
      "Accuracy on Test data: 0.7806955575942993, 0.5848945379257202\n",
      "Step 13 | Training Loss: 0.401949 | Validation Accuracy: 0.962931\n",
      "Accuracy on Test data: 0.7802963256835938, 0.5841349959373474\n",
      "Step 14 | Training Loss: 0.414146 | Validation Accuracy: 0.966423\n",
      "Accuracy on Test data: 0.7798970937728882, 0.5833755135536194\n",
      "Step 15 | Training Loss: 0.398732 | Validation Accuracy: 0.962057\n",
      "Accuracy on Test data: 0.7798084020614624, 0.5832067728042603\n",
      "Step 16 | Training Loss: 0.400699 | Validation Accuracy: 0.961502\n",
      "Accuracy on Test data: 0.7794091701507568, 0.5824472308158875\n",
      "Step 17 | Training Loss: 0.431035 | Validation Accuracy: 0.962375\n",
      "Accuracy on Test data: 0.7793204188346863, 0.5822784900665283\n",
      "Step 18 | Training Loss: 0.418829 | Validation Accuracy: 0.963407\n",
      "Accuracy on Test data: 0.7791430354118347, 0.5819409489631653\n",
      "Step 19 | Training Loss: 0.409698 | Validation Accuracy: 0.962692\n",
      "Accuracy on Test data: 0.7790542840957642, 0.5817721486091614\n",
      "Step 20 | Training Loss: 0.398217 | Validation Accuracy: 0.962454\n",
      "Accuracy on Test data: 0.7789212465286255, 0.5815190076828003\n",
      "Step 1 | Training Loss: 0.410053 | Validation Accuracy: 0.961581\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5814346075057983\n",
      "Step 2 | Training Loss: 0.404996 | Validation Accuracy: 0.961343\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5814346075057983\n",
      "Step 3 | Training Loss: 0.402684 | Validation Accuracy: 0.963963\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5814346075057983\n",
      "Step 4 | Training Loss: 0.407026 | Validation Accuracy: 0.965233\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5814346075057983\n",
      "Step 5 | Training Loss: 0.422238 | Validation Accuracy: 0.961661\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5814346075057983\n",
      "Step 6 | Training Loss: 0.409479 | Validation Accuracy: 0.961978\n",
      "Accuracy on Test data: 0.7788768410682678, 0.5814346075057983\n",
      "Step 7 | Training Loss: 0.417169 | Validation Accuracy: 0.961422\n",
      "Accuracy on Test data: 0.778788149356842, 0.5812658071517944\n",
      "Step 8 | Training Loss: 0.403019 | Validation Accuracy: 0.964280\n",
      "Accuracy on Test data: 0.7786994576454163, 0.5810970664024353\n",
      "Step 9 | Training Loss: 0.405302 | Validation Accuracy: 0.960391\n",
      "Accuracy on Test data: 0.7786107063293457, 0.5809282660484314\n",
      "Step 10 | Training Loss: 0.399392 | Validation Accuracy: 0.964994\n",
      "Accuracy on Test data: 0.7786550521850586, 0.5810126662254333\n",
      "Step 11 | Training Loss: 0.398746 | Validation Accuracy: 0.959438\n",
      "Accuracy on Test data: 0.7786550521850586, 0.5810126662254333\n",
      "Step 12 | Training Loss: 0.401971 | Validation Accuracy: 0.963328\n",
      "Accuracy on Test data: 0.7786107063293457, 0.5809282660484314\n",
      "Step 13 | Training Loss: 0.402328 | Validation Accuracy: 0.963169\n",
      "Accuracy on Test data: 0.7786107063293457, 0.5809282660484314\n",
      "Step 14 | Training Loss: 0.404290 | Validation Accuracy: 0.963169\n",
      "Accuracy on Test data: 0.7786107063293457, 0.5809282660484314\n",
      "Step 15 | Training Loss: 0.402266 | Validation Accuracy: 0.961343\n",
      "Accuracy on Test data: 0.7785220146179199, 0.5807594656944275\n",
      "Step 16 | Training Loss: 0.409862 | Validation Accuracy: 0.965709\n",
      "Accuracy on Test data: 0.778477668762207, 0.5806751251220703\n",
      "Step 17 | Training Loss: 0.413875 | Validation Accuracy: 0.964439\n",
      "Accuracy on Test data: 0.778477668762207, 0.5806751251220703\n",
      "Step 18 | Training Loss: 0.410128 | Validation Accuracy: 0.964042\n",
      "Accuracy on Test data: 0.778477668762207, 0.5806751251220703\n",
      "Step 19 | Training Loss: 0.411010 | Validation Accuracy: 0.962692\n",
      "Accuracy on Test data: 0.7784332633018494, 0.5805907249450684\n",
      "Step 20 | Training Loss: 0.412040 | Validation Accuracy: 0.959597\n",
      "Accuracy on Test data: 0.7783889174461365, 0.5805063247680664\n",
      "Current Layer Attributes - epochs:20 hidden layers:5 features count:122\n",
      "Step 1 | Training Loss: 0.722673 | Validation Accuracy: 0.742102\n",
      "Accuracy on Test data: 0.7268896102905273, 0.5813502073287964\n",
      "Step 2 | Training Loss: 0.691213 | Validation Accuracy: 0.758533\n",
      "Accuracy on Test data: 0.834945023059845, 0.7430379986763\n",
      "Step 3 | Training Loss: 0.673753 | Validation Accuracy: 0.814891\n",
      "Accuracy on Test data: 0.8596522212028503, 0.7713924050331116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: 0.652509 | Validation Accuracy: 0.896968\n",
      "Accuracy on Test data: 0.845635175704956, 0.7240506410598755\n",
      "Step 5 | Training Loss: 0.625383 | Validation Accuracy: 0.917685\n",
      "Accuracy on Test data: 0.8328601717948914, 0.6937552690505981\n",
      "Step 6 | Training Loss: 0.596021 | Validation Accuracy: 0.936895\n",
      "Accuracy on Test data: 0.8200408220291138, 0.6662447452545166\n",
      "Step 7 | Training Loss: 0.556349 | Validation Accuracy: 0.944912\n",
      "Accuracy on Test data: 0.8211497664451599, 0.6654852032661438\n",
      "Step 8 | Training Loss: 0.512167 | Validation Accuracy: 0.952453\n",
      "Accuracy on Test data: 0.8270936608314514, 0.6751898527145386\n",
      "Step 9 | Training Loss: 0.470324 | Validation Accuracy: 0.955469\n",
      "Accuracy on Test data: 0.8249201774597168, 0.6708016991615295\n",
      "Step 10 | Training Loss: 0.466229 | Validation Accuracy: 0.955866\n",
      "Accuracy on Test data: 0.8233232498168945, 0.6676793098449707\n",
      "Step 11 | Training Loss: 0.432817 | Validation Accuracy: 0.958327\n",
      "Accuracy on Test data: 0.8231902122497559, 0.6672573685646057\n",
      "Step 12 | Training Loss: 0.450814 | Validation Accuracy: 0.956104\n",
      "Accuracy on Test data: 0.8195528984069824, 0.6603375673294067\n",
      "Step 13 | Training Loss: 0.410981 | Validation Accuracy: 0.958485\n",
      "Accuracy on Test data: 0.8183108568191528, 0.6578903198242188\n",
      "Step 14 | Training Loss: 0.404480 | Validation Accuracy: 0.958803\n",
      "Accuracy on Test data: 0.817689836025238, 0.6566244959831238\n",
      "Step 15 | Training Loss: 0.389303 | Validation Accuracy: 0.957057\n",
      "Accuracy on Test data: 0.8174237012863159, 0.6561181545257568\n",
      "Step 16 | Training Loss: 0.400399 | Validation Accuracy: 0.958803\n",
      "Accuracy on Test data: 0.8119677305221558, 0.6455696225166321\n",
      "Step 17 | Training Loss: 0.379569 | Validation Accuracy: 0.959914\n",
      "Accuracy on Test data: 0.7890791296958923, 0.6020253300666809\n",
      "Step 18 | Training Loss: 0.383184 | Validation Accuracy: 0.960867\n",
      "Accuracy on Test data: 0.7766146063804626, 0.5783122181892395\n",
      "Step 19 | Training Loss: 0.383161 | Validation Accuracy: 0.961343\n",
      "Accuracy on Test data: 0.7735539674758911, 0.5724894404411316\n",
      "Step 20 | Training Loss: 0.368273 | Validation Accuracy: 0.964518\n",
      "Accuracy on Test data: 0.7720457911491394, 0.5696202516555786\n",
      "Step 1 | Training Loss: 0.381698 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.7719570398330688, 0.5694514513015747\n",
      "Step 2 | Training Loss: 0.376535 | Validation Accuracy: 0.957771\n",
      "Accuracy on Test data: 0.7718683481216431, 0.5692827105522156\n",
      "Step 3 | Training Loss: 0.367862 | Validation Accuracy: 0.961343\n",
      "Accuracy on Test data: 0.7715578675270081, 0.5686919689178467\n",
      "Step 4 | Training Loss: 0.370972 | Validation Accuracy: 0.960629\n",
      "Accuracy on Test data: 0.7711586356163025, 0.5679324865341187\n",
      "Step 5 | Training Loss: 0.374655 | Validation Accuracy: 0.962296\n",
      "Accuracy on Test data: 0.7707594037055969, 0.5671730041503906\n",
      "Step 6 | Training Loss: 0.373057 | Validation Accuracy: 0.961184\n",
      "Accuracy on Test data: 0.770715057849884, 0.5670886039733887\n",
      "Step 7 | Training Loss: 0.373198 | Validation Accuracy: 0.960787\n",
      "Accuracy on Test data: 0.7702271342277527, 0.5661603212356567\n",
      "Step 8 | Training Loss: 0.372589 | Validation Accuracy: 0.963328\n",
      "Accuracy on Test data: 0.7698279023170471, 0.5654008388519287\n",
      "Step 9 | Training Loss: 0.366770 | Validation Accuracy: 0.963486\n",
      "Accuracy on Test data: 0.7696061134338379, 0.5649788975715637\n",
      "Step 10 | Training Loss: 0.380346 | Validation Accuracy: 0.962137\n",
      "Accuracy on Test data: 0.7695173621177673, 0.5648100972175598\n",
      "Step 11 | Training Loss: 0.363478 | Validation Accuracy: 0.961581\n",
      "Accuracy on Test data: 0.7694286704063416, 0.5646413564682007\n",
      "Step 12 | Training Loss: 0.367813 | Validation Accuracy: 0.964439\n",
      "Accuracy on Test data: 0.7693399786949158, 0.5644725561141968\n",
      "Step 13 | Training Loss: 0.375790 | Validation Accuracy: 0.963248\n",
      "Accuracy on Test data: 0.7692512273788452, 0.5643038153648376\n",
      "Step 14 | Training Loss: 0.376507 | Validation Accuracy: 0.961026\n",
      "Accuracy on Test data: 0.7691625356674194, 0.5641350150108337\n",
      "Step 15 | Training Loss: 0.403614 | Validation Accuracy: 0.964121\n",
      "Accuracy on Test data: 0.7689407467842102, 0.5637130737304688\n",
      "Step 16 | Training Loss: 0.360388 | Validation Accuracy: 0.963963\n",
      "Accuracy on Test data: 0.7687633037567139, 0.5633755326271057\n",
      "Step 17 | Training Loss: 0.374410 | Validation Accuracy: 0.963089\n",
      "Accuracy on Test data: 0.7685858607292175, 0.5630379915237427\n",
      "Step 18 | Training Loss: 0.374908 | Validation Accuracy: 0.961740\n",
      "Accuracy on Test data: 0.7684528231620789, 0.5627847909927368\n",
      "Step 19 | Training Loss: 0.355271 | Validation Accuracy: 0.961740\n",
      "Accuracy on Test data: 0.7683197259902954, 0.5625316500663757\n",
      "Step 20 | Training Loss: 0.365408 | Validation Accuracy: 0.963248\n",
      "Accuracy on Test data: 0.7680979371070862, 0.5621097087860107\n",
      "Step 1 | Training Loss: 0.353407 | Validation Accuracy: 0.963407\n",
      "Accuracy on Test data: 0.7680979371070862, 0.5621097087860107\n",
      "Step 2 | Training Loss: 0.383333 | Validation Accuracy: 0.959200\n",
      "Accuracy on Test data: 0.7680979371070862, 0.5621097087860107\n",
      "Step 3 | Training Loss: 0.382493 | Validation Accuracy: 0.964201\n",
      "Accuracy on Test data: 0.7680979371070862, 0.5621097087860107\n",
      "Step 4 | Training Loss: 0.379419 | Validation Accuracy: 0.960073\n",
      "Accuracy on Test data: 0.7680979371070862, 0.5621097087860107\n",
      "Step 5 | Training Loss: 0.374768 | Validation Accuracy: 0.961264\n",
      "Accuracy on Test data: 0.7680535912513733, 0.5620253086090088\n",
      "Step 6 | Training Loss: 0.372014 | Validation Accuracy: 0.962137\n",
      "Accuracy on Test data: 0.7680535912513733, 0.5620253086090088\n",
      "Step 7 | Training Loss: 0.365495 | Validation Accuracy: 0.962296\n",
      "Accuracy on Test data: 0.7680535912513733, 0.5620253086090088\n",
      "Step 8 | Training Loss: 0.378487 | Validation Accuracy: 0.963328\n",
      "Accuracy on Test data: 0.7680535912513733, 0.5620253086090088\n",
      "Step 9 | Training Loss: 0.361782 | Validation Accuracy: 0.965629\n",
      "Accuracy on Test data: 0.7680535912513733, 0.5620253086090088\n",
      "Step 10 | Training Loss: 0.371645 | Validation Accuracy: 0.964121\n",
      "Accuracy on Test data: 0.7680092453956604, 0.5619409084320068\n",
      "Step 11 | Training Loss: 0.365337 | Validation Accuracy: 0.964518\n",
      "Accuracy on Test data: 0.7680092453956604, 0.5619409084320068\n",
      "Step 12 | Training Loss: 0.383768 | Validation Accuracy: 0.961661\n",
      "Accuracy on Test data: 0.7679648399353027, 0.5618565678596497\n",
      "Step 13 | Training Loss: 0.359438 | Validation Accuracy: 0.961264\n",
      "Accuracy on Test data: 0.7679648399353027, 0.5618565678596497\n",
      "Step 14 | Training Loss: 0.358169 | Validation Accuracy: 0.960549\n",
      "Accuracy on Test data: 0.7679204940795898, 0.5617721676826477\n",
      "Step 15 | Training Loss: 0.361838 | Validation Accuracy: 0.960787\n",
      "Accuracy on Test data: 0.7679204940795898, 0.5617721676826477\n",
      "Step 16 | Training Loss: 0.363372 | Validation Accuracy: 0.962613\n",
      "Accuracy on Test data: 0.767876148223877, 0.5616877675056458\n",
      "Step 17 | Training Loss: 0.366841 | Validation Accuracy: 0.962296\n",
      "Accuracy on Test data: 0.767876148223877, 0.5616877675056458\n",
      "Step 18 | Training Loss: 0.355324 | Validation Accuracy: 0.962931\n",
      "Accuracy on Test data: 0.767876148223877, 0.5616877675056458\n",
      "Step 19 | Training Loss: 0.373475 | Validation Accuracy: 0.961740\n",
      "Accuracy on Test data: 0.7679204940795898, 0.5616877675056458\n",
      "Step 20 | Training Loss: 0.351176 | Validation Accuracy: 0.964042\n",
      "Accuracy on Test data: 0.7679204940795898, 0.5616877675056458\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [1, 4, 16, 32, 64, 122]\n",
    "    hidden_layers_arr = [1, 3, 5]\n",
    "    \n",
    "    epochs = [20]\n",
    "    lrs = [1e-5, 1e-6, 1e-7]\n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f, lrs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:36.613678Z",
     "start_time": "2017-06-16T19:08:36.608867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "dict2 = []\n",
    "for k, (v1, v2) in Train.predictions.items():\n",
    "    dict1.update({k: v1})\n",
    "    dict2.append(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:36.691285Z",
     "start_time": "2017-06-16T19:08:36.614942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train.predictions = dict1\n",
    "Train.results = dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:36.761494Z",
     "start_time": "2017-06-16T19:08:36.693322Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:36.845967Z",
     "start_time": "2017-06-16T19:08:36.763605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.897127</td>\n",
       "      <td>0.876863</td>\n",
       "      <td>0.776287</td>\n",
       "      <td>12.011190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.915780</td>\n",
       "      <td>0.866528</td>\n",
       "      <td>0.758565</td>\n",
       "      <td>14.312281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933323</td>\n",
       "      <td>0.862535</td>\n",
       "      <td>0.747257</td>\n",
       "      <td>18.839120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915780</td>\n",
       "      <td>0.851623</td>\n",
       "      <td>0.728945</td>\n",
       "      <td>5.653030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.937768</td>\n",
       "      <td>0.850692</td>\n",
       "      <td>0.719747</td>\n",
       "      <td>15.541927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.929830</td>\n",
       "      <td>0.756875</td>\n",
       "      <td>0.540253</td>\n",
       "      <td>75.135141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "167      8               4              5     0.897127    0.876863   \n",
       "159      7             122              3     0.915780    0.866528   \n",
       "140     13              16              3     0.933323    0.862535   \n",
       "32       6              32              1     0.915780    0.851623   \n",
       "153      9              64              3     0.937768    0.850692   \n",
       "93      63               1              3     0.929830    0.756875   \n",
       "\n",
       "     test_score_20  time_taken  \n",
       "167       0.776287   12.011190  \n",
       "159       0.758565   14.312281  \n",
       "140       0.747257   18.839120  \n",
       "32        0.728945    5.653030  \n",
       "153       0.719747   15.541927  \n",
       "93        0.540253   75.135141  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:36.894155Z",
     "start_time": "2017-06-16T19:08:36.847506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.874778</td>\n",
       "      <td>0.777046</td>\n",
       "      <td>8.578906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.814891</td>\n",
       "      <td>0.859652</td>\n",
       "      <td>0.771392</td>\n",
       "      <td>10.434872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933323</td>\n",
       "      <td>0.862535</td>\n",
       "      <td>0.747257</td>\n",
       "      <td>18.839120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915780</td>\n",
       "      <td>0.851623</td>\n",
       "      <td>0.728945</td>\n",
       "      <td>5.653030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.937768</td>\n",
       "      <td>0.850692</td>\n",
       "      <td>0.719747</td>\n",
       "      <td>15.541927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.929830</td>\n",
       "      <td>0.756875</td>\n",
       "      <td>0.540253</td>\n",
       "      <td>75.135141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "165      6               4              5     0.879902    0.874778   \n",
       "236      4             122              5     0.814891    0.859652   \n",
       "140     13              16              3     0.933323    0.862535   \n",
       "32       6              32              1     0.915780    0.851623   \n",
       "153      9              64              3     0.937768    0.850692   \n",
       "93      63               1              3     0.929830    0.756875   \n",
       "\n",
       "     test_score_20  time_taken  \n",
       "165       0.777046    8.578906  \n",
       "236       0.771392   10.434872  \n",
       "140       0.747257   18.839120  \n",
       "32        0.728945    5.653030  \n",
       "153       0.719747   15.541927  \n",
       "93        0.540253   75.135141  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score_20'].transform(max) == df_results['test_score_20']\n",
    "df_results[idx].sort_values(by = 'test_score_20', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:09:49.758996Z",
     "start_time": "2017-06-16T19:09:49.720620Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.897127</td>\n",
       "      <td>0.876863</td>\n",
       "      <td>0.776287</td>\n",
       "      <td>12.011190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.889189</td>\n",
       "      <td>0.875843</td>\n",
       "      <td>0.776034</td>\n",
       "      <td>10.298957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.879902</td>\n",
       "      <td>0.874778</td>\n",
       "      <td>0.777046</td>\n",
       "      <td>8.578906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.878314</td>\n",
       "      <td>0.871407</td>\n",
       "      <td>0.775612</td>\n",
       "      <td>6.874862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946738</td>\n",
       "      <td>0.870121</td>\n",
       "      <td>0.760253</td>\n",
       "      <td>9.030899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.874583</td>\n",
       "      <td>0.869855</td>\n",
       "      <td>0.774684</td>\n",
       "      <td>5.171532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944118</td>\n",
       "      <td>0.869145</td>\n",
       "      <td>0.758650</td>\n",
       "      <td>7.906469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.915780</td>\n",
       "      <td>0.866528</td>\n",
       "      <td>0.758565</td>\n",
       "      <td>14.312281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898873</td>\n",
       "      <td>0.866173</td>\n",
       "      <td>0.755021</td>\n",
       "      <td>5.743569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885299</td>\n",
       "      <td>0.865729</td>\n",
       "      <td>0.756371</td>\n",
       "      <td>4.627712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.933323</td>\n",
       "      <td>0.862535</td>\n",
       "      <td>0.747257</td>\n",
       "      <td>18.839120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.923083</td>\n",
       "      <td>0.862358</td>\n",
       "      <td>0.747004</td>\n",
       "      <td>17.332388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.853786</td>\n",
       "      <td>0.862092</td>\n",
       "      <td>0.771814</td>\n",
       "      <td>3.479382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.904588</td>\n",
       "      <td>0.861914</td>\n",
       "      <td>0.750717</td>\n",
       "      <td>11.920144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.913399</td>\n",
       "      <td>0.861382</td>\n",
       "      <td>0.745738</td>\n",
       "      <td>15.741060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.858628</td>\n",
       "      <td>0.860850</td>\n",
       "      <td>0.749451</td>\n",
       "      <td>3.424663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.909589</td>\n",
       "      <td>0.860007</td>\n",
       "      <td>0.743629</td>\n",
       "      <td>14.072127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.898238</td>\n",
       "      <td>0.859918</td>\n",
       "      <td>0.749030</td>\n",
       "      <td>9.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.814891</td>\n",
       "      <td>0.859652</td>\n",
       "      <td>0.771392</td>\n",
       "      <td>10.434872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.890062</td>\n",
       "      <td>0.858499</td>\n",
       "      <td>0.748523</td>\n",
       "      <td>7.120271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.907604</td>\n",
       "      <td>0.856813</td>\n",
       "      <td>0.739325</td>\n",
       "      <td>12.527823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.825607</td>\n",
       "      <td>0.852111</td>\n",
       "      <td>0.776878</td>\n",
       "      <td>1.757602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915780</td>\n",
       "      <td>0.851623</td>\n",
       "      <td>0.728945</td>\n",
       "      <td>5.653030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.937768</td>\n",
       "      <td>0.850692</td>\n",
       "      <td>0.719747</td>\n",
       "      <td>15.541927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.894507</td>\n",
       "      <td>0.850559</td>\n",
       "      <td>0.731224</td>\n",
       "      <td>10.959881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.932132</td>\n",
       "      <td>0.850248</td>\n",
       "      <td>0.719240</td>\n",
       "      <td>13.584446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.834498</td>\n",
       "      <td>0.849228</td>\n",
       "      <td>0.732405</td>\n",
       "      <td>2.324099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.906652</td>\n",
       "      <td>0.848208</td>\n",
       "      <td>0.724641</td>\n",
       "      <td>4.505958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.930783</td>\n",
       "      <td>0.847232</td>\n",
       "      <td>0.713755</td>\n",
       "      <td>11.710555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.883712</td>\n",
       "      <td>0.843196</td>\n",
       "      <td>0.721013</td>\n",
       "      <td>9.442121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.885379</td>\n",
       "      <td>0.840046</td>\n",
       "      <td>0.717384</td>\n",
       "      <td>7.854544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.920702</td>\n",
       "      <td>0.839913</td>\n",
       "      <td>0.702869</td>\n",
       "      <td>9.723371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885934</td>\n",
       "      <td>0.837385</td>\n",
       "      <td>0.709873</td>\n",
       "      <td>3.410862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.758533</td>\n",
       "      <td>0.834945</td>\n",
       "      <td>0.743038</td>\n",
       "      <td>6.967592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.874742</td>\n",
       "      <td>0.834102</td>\n",
       "      <td>0.707595</td>\n",
       "      <td>6.266993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.873154</td>\n",
       "      <td>0.832905</td>\n",
       "      <td>0.716203</td>\n",
       "      <td>4.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.917527</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.683291</td>\n",
       "      <td>7.785719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.855691</td>\n",
       "      <td>0.824388</td>\n",
       "      <td>0.696287</td>\n",
       "      <td>4.673048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.822908</td>\n",
       "      <td>0.820396</td>\n",
       "      <td>0.694430</td>\n",
       "      <td>3.162081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.931815</td>\n",
       "      <td>0.819642</td>\n",
       "      <td>0.672911</td>\n",
       "      <td>8.062992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.926814</td>\n",
       "      <td>0.816492</td>\n",
       "      <td>0.669114</td>\n",
       "      <td>6.445546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857835</td>\n",
       "      <td>0.808951</td>\n",
       "      <td>0.661688</td>\n",
       "      <td>2.236590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.940705</td>\n",
       "      <td>0.808907</td>\n",
       "      <td>0.640253</td>\n",
       "      <td>84.667317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.942134</td>\n",
       "      <td>0.808818</td>\n",
       "      <td>0.640084</td>\n",
       "      <td>82.500738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.943166</td>\n",
       "      <td>0.808774</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>77.230107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.940070</td>\n",
       "      <td>0.808641</td>\n",
       "      <td>0.639747</td>\n",
       "      <td>75.071118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.948087</td>\n",
       "      <td>0.808597</td>\n",
       "      <td>0.639662</td>\n",
       "      <td>72.893134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.940705</td>\n",
       "      <td>0.808286</td>\n",
       "      <td>0.639072</td>\n",
       "      <td>70.636689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.941975</td>\n",
       "      <td>0.808020</td>\n",
       "      <td>0.638565</td>\n",
       "      <td>68.595748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.940625</td>\n",
       "      <td>0.807887</td>\n",
       "      <td>0.638312</td>\n",
       "      <td>66.448224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.919590</td>\n",
       "      <td>0.729817</td>\n",
       "      <td>0.496203</td>\n",
       "      <td>9.549525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>5</td>\n",
       "      <td>0.742102</td>\n",
       "      <td>0.726890</td>\n",
       "      <td>0.581350</td>\n",
       "      <td>3.512109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.810367</td>\n",
       "      <td>0.723474</td>\n",
       "      <td>0.493587</td>\n",
       "      <td>7.745776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693761</td>\n",
       "      <td>0.721123</td>\n",
       "      <td>0.707932</td>\n",
       "      <td>1.132346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.756707</td>\n",
       "      <td>0.718151</td>\n",
       "      <td>0.482532</td>\n",
       "      <td>2.008710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.901651</td>\n",
       "      <td>0.717929</td>\n",
       "      <td>0.472068</td>\n",
       "      <td>5.861550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.846722</td>\n",
       "      <td>0.716199</td>\n",
       "      <td>0.481097</td>\n",
       "      <td>8.249208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.906255</td>\n",
       "      <td>0.715667</td>\n",
       "      <td>0.475949</td>\n",
       "      <td>16.567069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882204</td>\n",
       "      <td>0.714381</td>\n",
       "      <td>0.464051</td>\n",
       "      <td>5.909654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903794</td>\n",
       "      <td>0.714381</td>\n",
       "      <td>0.468017</td>\n",
       "      <td>8.304408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.882839</td>\n",
       "      <td>0.712030</td>\n",
       "      <td>0.459747</td>\n",
       "      <td>25.548470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.880537</td>\n",
       "      <td>0.701783</td>\n",
       "      <td>0.451814</td>\n",
       "      <td>13.673539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.789729</td>\n",
       "      <td>0.696815</td>\n",
       "      <td>0.458734</td>\n",
       "      <td>5.485560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857358</td>\n",
       "      <td>0.694464</td>\n",
       "      <td>0.430127</td>\n",
       "      <td>4.746597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876568</td>\n",
       "      <td>0.692690</td>\n",
       "      <td>0.427426</td>\n",
       "      <td>7.126486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854183</td>\n",
       "      <td>0.677874</td>\n",
       "      <td>0.398650</td>\n",
       "      <td>4.729785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.754405</td>\n",
       "      <td>0.672285</td>\n",
       "      <td>0.425401</td>\n",
       "      <td>2.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.827115</td>\n",
       "      <td>0.672197</td>\n",
       "      <td>0.390886</td>\n",
       "      <td>21.275339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824178</td>\n",
       "      <td>0.667672</td>\n",
       "      <td>0.384726</td>\n",
       "      <td>5.994195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.834736</td>\n",
       "      <td>0.664035</td>\n",
       "      <td>0.410717</td>\n",
       "      <td>10.769241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808700</td>\n",
       "      <td>0.663724</td>\n",
       "      <td>0.385063</td>\n",
       "      <td>3.534013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.701937</td>\n",
       "      <td>0.658446</td>\n",
       "      <td>0.409283</td>\n",
       "      <td>1.434095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857200</td>\n",
       "      <td>0.649796</td>\n",
       "      <td>0.476371</td>\n",
       "      <td>2.417153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812907</td>\n",
       "      <td>0.648820</td>\n",
       "      <td>0.346076</td>\n",
       "      <td>3.602308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.773774</td>\n",
       "      <td>0.601668</td>\n",
       "      <td>0.260591</td>\n",
       "      <td>2.374648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704477</td>\n",
       "      <td>0.601579</td>\n",
       "      <td>0.328776</td>\n",
       "      <td>4.786133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.792189</td>\n",
       "      <td>0.597188</td>\n",
       "      <td>0.432574</td>\n",
       "      <td>1.264498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.688046</td>\n",
       "      <td>0.588671</td>\n",
       "      <td>0.418734</td>\n",
       "      <td>11.248049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.712653</td>\n",
       "      <td>0.587429</td>\n",
       "      <td>0.238734</td>\n",
       "      <td>17.019646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.614383</td>\n",
       "      <td>0.572480</td>\n",
       "      <td>0.415527</td>\n",
       "      <td>7.560297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.790125</td>\n",
       "      <td>0.571105</td>\n",
       "      <td>0.362869</td>\n",
       "      <td>7.903150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634386</td>\n",
       "      <td>0.560371</td>\n",
       "      <td>0.265316</td>\n",
       "      <td>3.638320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>0.576282</td>\n",
       "      <td>0.552431</td>\n",
       "      <td>0.454515</td>\n",
       "      <td>2.410234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.540562</td>\n",
       "      <td>0.532292</td>\n",
       "      <td>0.643376</td>\n",
       "      <td>1.574081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.719003</td>\n",
       "      <td>0.524441</td>\n",
       "      <td>0.279325</td>\n",
       "      <td>4.896370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.623035</td>\n",
       "      <td>0.524308</td>\n",
       "      <td>0.353249</td>\n",
       "      <td>5.194973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.631608</td>\n",
       "      <td>0.520848</td>\n",
       "      <td>0.373249</td>\n",
       "      <td>2.370456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.533339</td>\n",
       "      <td>0.518364</td>\n",
       "      <td>0.399240</td>\n",
       "      <td>3.781599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.698762</td>\n",
       "      <td>0.499113</td>\n",
       "      <td>0.224979</td>\n",
       "      <td>1.222138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593269</td>\n",
       "      <td>0.479019</td>\n",
       "      <td>0.243376</td>\n",
       "      <td>2.409583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.555009</td>\n",
       "      <td>0.441847</td>\n",
       "      <td>0.197975</td>\n",
       "      <td>12.678383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563582</td>\n",
       "      <td>0.441448</td>\n",
       "      <td>0.220928</td>\n",
       "      <td>1.276297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.560010</td>\n",
       "      <td>0.436746</td>\n",
       "      <td>0.192489</td>\n",
       "      <td>2.626912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.549611</td>\n",
       "      <td>0.434794</td>\n",
       "      <td>0.187848</td>\n",
       "      <td>8.442401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.538816</td>\n",
       "      <td>0.432044</td>\n",
       "      <td>0.183376</td>\n",
       "      <td>4.205601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>0.542467</td>\n",
       "      <td>0.430935</td>\n",
       "      <td>0.181772</td>\n",
       "      <td>2.077440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.537228</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.679860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.538339</td>\n",
       "      <td>0.430758</td>\n",
       "      <td>0.181603</td>\n",
       "      <td>1.414496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0.385379</td>\n",
       "      <td>0.379214</td>\n",
       "      <td>0.391477</td>\n",
       "      <td>1.924420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.369106</td>\n",
       "      <td>0.237048</td>\n",
       "      <td>0.316878</td>\n",
       "      <td>1.159605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "167      8               4              5     0.897127    0.876863   \n",
       "166      7               4              5     0.889189    0.875843   \n",
       "165      6               4              5     0.879902    0.874778   \n",
       "164      5               4              5     0.878314    0.871407   \n",
       "17       9               4              1     0.946738    0.870121   \n",
       "163      4               4              5     0.874583    0.869855   \n",
       "16       8               4              1     0.944118    0.869145   \n",
       "159      7             122              3     0.915780    0.866528   \n",
       "15       6               4              1     0.898873    0.866173   \n",
       "14       5               4              1     0.885299    0.865729   \n",
       "140     13              16              3     0.933323    0.862535   \n",
       "139     12              16              3     0.923083    0.862358   \n",
       "162      3               4              5     0.853786    0.862092   \n",
       "158      6             122              3     0.904588    0.861914   \n",
       "138     11              16              3     0.913399    0.861382   \n",
       "13       4               4              1     0.858628    0.860850   \n",
       "137     10              16              3     0.909589    0.860007   \n",
       "157      5             122              3     0.898238    0.859918   \n",
       "236      4             122              5     0.814891    0.859652   \n",
       "156      4             122              3     0.890062    0.858499   \n",
       "136      9              16              3     0.907604    0.856813   \n",
       "161      2               4              5     0.825607    0.852111   \n",
       "32       6              32              1     0.915780    0.851623   \n",
       "153      9              64              3     0.937768    0.850692   \n",
       "135      8              16              3     0.894507    0.850559   \n",
       "152      8              64              3     0.932132    0.850248   \n",
       "12       3               4              1     0.834498    0.849228   \n",
       "31       5              32              1     0.906652    0.848208   \n",
       "151      7              64              3     0.930783    0.847232   \n",
       "134      7              16              3     0.883712    0.843196   \n",
       "133      6              16              3     0.885379    0.840046   \n",
       "150      6              64              3     0.920702    0.839913   \n",
       "30       4              32              1     0.885934    0.837385   \n",
       "235      3             122              5     0.758533    0.834945   \n",
       "132      5              16              3     0.874742    0.834102   \n",
       "155      3             122              3     0.873154    0.832905   \n",
       "149      5              64              3     0.917527    0.829400   \n",
       "131      4              16              3     0.855691    0.824388   \n",
       "130      3              16              3     0.822908    0.820396   \n",
       "145      6              32              3     0.931815    0.819642   \n",
       "144      5              32              3     0.926814    0.816492   \n",
       "29       3              32              1     0.857835    0.808951   \n",
       "227     42              32              5     0.940705    0.808907   \n",
       "226     40              32              5     0.942134    0.808818   \n",
       "225     34              32              5     0.943166    0.808774   \n",
       "224     32              32              5     0.940070    0.808641   \n",
       "223     30              32              5     0.948087    0.808597   \n",
       "222     28              32              5     0.940705    0.808286   \n",
       "221     26              32              5     0.941975    0.808020   \n",
       "220     24              32              5     0.940625    0.807887   \n",
       "..     ...             ...            ...          ...         ...   \n",
       "7        9               1              1     0.919590    0.729817   \n",
       "234      2             122              5     0.742102    0.726890   \n",
       "230      4              64              5     0.810367    0.723474   \n",
       "11       2               4              1     0.693761    0.721123   \n",
       "146      2              64              3     0.756707    0.718151   \n",
       "46       6             122              1     0.901651    0.717929   \n",
       "65       7               1              3     0.846722    0.716199   \n",
       "103     13               4              3     0.906255    0.715667   \n",
       "22       6              16              1     0.882204    0.714381   \n",
       "6        8               1              1     0.903794    0.714381   \n",
       "210     13              32              5     0.882839    0.712030   \n",
       "101     11               4              3     0.880537    0.701783   \n",
       "63       5               1              3     0.789729    0.696815   \n",
       "45       5             122              1     0.857358    0.694464   \n",
       "5        7               1              1     0.876568    0.692690   \n",
       "21       5              16              1     0.854183    0.677874   \n",
       "61       3               1              3     0.754405    0.672285   \n",
       "208     11              32              5     0.827115    0.672197   \n",
       "4        6               1              1     0.824178    0.667672   \n",
       "99       9               4              3     0.834736    0.664035   \n",
       "44       4             122              1     0.808700    0.663724   \n",
       "60       2               1              3     0.701937    0.658446   \n",
       "34       3              64              1     0.857200    0.649796   \n",
       "20       4              16              1     0.812907    0.648820   \n",
       "19       3              16              1     0.773774    0.601668   \n",
       "3        5               1              1     0.704477    0.601579   \n",
       "33       2              64              1     0.792189    0.597188   \n",
       "173      7              16              5     0.688046    0.588671   \n",
       "206      9              32              5     0.712653    0.587429   \n",
       "171      5              16              5     0.614383    0.572480   \n",
       "97       7               4              3     0.790125    0.571105   \n",
       "2        4               1              1     0.634386    0.560371   \n",
       "154      2             122              3     0.576282    0.552431   \n",
       "129      2              16              3     0.540562    0.532292   \n",
       "95       5               4              3     0.719003    0.524441   \n",
       "229      3              64              5     0.623035    0.524308   \n",
       "43       3             122              1     0.631608    0.520848   \n",
       "169      3              16              5     0.533339    0.518364   \n",
       "18       2              16              1     0.698762    0.499113   \n",
       "1        3               1              1     0.593269    0.479019   \n",
       "204      7              32              5     0.555009    0.441847   \n",
       "0        2               1              1     0.563582    0.441448   \n",
       "228      2              64              5     0.560010    0.436746   \n",
       "202      5              32              5     0.549611    0.434794   \n",
       "200      3              32              5     0.538816    0.432044   \n",
       "199      2              32              5     0.542467    0.430935   \n",
       "160      2               1              5     0.537228    0.430758   \n",
       "94       2               4              3     0.538339    0.430758   \n",
       "168      2              16              5     0.385379    0.379214   \n",
       "42       2             122              1     0.369106    0.237048   \n",
       "\n",
       "     test_score_20  time_taken  \n",
       "167       0.776287   12.011190  \n",
       "166       0.776034   10.298957  \n",
       "165       0.777046    8.578906  \n",
       "164       0.775612    6.874862  \n",
       "17        0.760253    9.030899  \n",
       "163       0.774684    5.171532  \n",
       "16        0.758650    7.906469  \n",
       "159       0.758565   14.312281  \n",
       "15        0.755021    5.743569  \n",
       "14        0.756371    4.627712  \n",
       "140       0.747257   18.839120  \n",
       "139       0.747004   17.332388  \n",
       "162       0.771814    3.479382  \n",
       "158       0.750717   11.920144  \n",
       "138       0.745738   15.741060  \n",
       "13        0.749451    3.424663  \n",
       "137       0.743629   14.072127  \n",
       "157       0.749030    9.526100  \n",
       "236       0.771392   10.434872  \n",
       "156       0.748523    7.120271  \n",
       "136       0.739325   12.527823  \n",
       "161       0.776878    1.757602  \n",
       "32        0.728945    5.653030  \n",
       "153       0.719747   15.541927  \n",
       "135       0.731224   10.959881  \n",
       "152       0.719240   13.584446  \n",
       "12        0.732405    2.324099  \n",
       "31        0.724641    4.505958  \n",
       "151       0.713755   11.710555  \n",
       "134       0.721013    9.442121  \n",
       "133       0.717384    7.854544  \n",
       "150       0.702869    9.723371  \n",
       "30        0.709873    3.410862  \n",
       "235       0.743038    6.967592  \n",
       "132       0.707595    6.266993  \n",
       "155       0.716203    4.709700  \n",
       "149       0.683291    7.785719  \n",
       "131       0.696287    4.673048  \n",
       "130       0.694430    3.162081  \n",
       "145       0.672911    8.062992  \n",
       "144       0.669114    6.445546  \n",
       "29        0.661688    2.236590  \n",
       "227       0.640253   84.667317  \n",
       "226       0.640084   82.500738  \n",
       "225       0.640000   77.230107  \n",
       "224       0.639747   75.071118  \n",
       "223       0.639662   72.893134  \n",
       "222       0.639072   70.636689  \n",
       "221       0.638565   68.595748  \n",
       "220       0.638312   66.448224  \n",
       "..             ...         ...  \n",
       "7         0.496203    9.549525  \n",
       "234       0.581350    3.512109  \n",
       "230       0.493587    7.745776  \n",
       "11        0.707932    1.132346  \n",
       "146       0.482532    2.008710  \n",
       "46        0.472068    5.861550  \n",
       "65        0.481097    8.249208  \n",
       "103       0.475949   16.567069  \n",
       "22        0.464051    5.909654  \n",
       "6         0.468017    8.304408  \n",
       "210       0.459747   25.548470  \n",
       "101       0.451814   13.673539  \n",
       "63        0.458734    5.485560  \n",
       "45        0.430127    4.746597  \n",
       "5         0.427426    7.126486  \n",
       "21        0.398650    4.729785  \n",
       "61        0.425401    2.826087  \n",
       "208       0.390886   21.275339  \n",
       "4         0.384726    5.994195  \n",
       "99        0.410717   10.769241  \n",
       "44        0.385063    3.534013  \n",
       "60        0.409283    1.434095  \n",
       "34        0.476371    2.417153  \n",
       "20        0.346076    3.602308  \n",
       "19        0.260591    2.374648  \n",
       "3         0.328776    4.786133  \n",
       "33        0.432574    1.264498  \n",
       "173       0.418734   11.248049  \n",
       "206       0.238734   17.019646  \n",
       "171       0.415527    7.560297  \n",
       "97        0.362869    7.903150  \n",
       "2         0.265316    3.638320  \n",
       "154       0.454515    2.410234  \n",
       "129       0.643376    1.574081  \n",
       "95        0.279325    4.896370  \n",
       "229       0.353249    5.194973  \n",
       "43        0.373249    2.370456  \n",
       "169       0.399240    3.781599  \n",
       "18        0.224979    1.222138  \n",
       "1         0.243376    2.409583  \n",
       "204       0.197975   12.678383  \n",
       "0         0.220928    1.276297  \n",
       "228       0.192489    2.626912  \n",
       "202       0.187848    8.442401  \n",
       "200       0.183376    4.205601  \n",
       "199       0.181772    2.077440  \n",
       "160       0.181603    1.679860  \n",
       "94        0.181603    1.414496  \n",
       "168       0.391477    1.924420  \n",
       "42        0.316878    1.159605  \n",
       "\n",
       "[237 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:37.994525Z",
     "start_time": "2017-06-16T19:08:36.958676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_dense_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_dense_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:38.063171Z",
     "start_time": "2017-06-16T19:08:37.996387Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:38.656512Z",
     "start_time": "2017-06-16T19:08:38.064746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.9067  0.0933]\n",
      " [ 0.1457  0.8543]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM8sdBzczM8sdBzczM8ud/wc4W2juwjY6PQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65f601df28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T19:08:38.930109Z",
     "start_time": "2017-06-16T19:08:38.658104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.6366  0.3634]\n",
      " [ 0.1927  0.8073]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMqGg5qZmZUNBzUzMysb/x9FDwCq0zBnMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65e57974a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
