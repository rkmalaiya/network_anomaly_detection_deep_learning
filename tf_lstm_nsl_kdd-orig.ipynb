{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:23.692153Z",
     "start_time": "2017-07-17T20:04:23.264565Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:23.773816Z",
     "start_time": "2017-07-17T20:04:23.693536Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    kdd_test__2labels = pd.read_pickle(\"dataset/kdd_test__2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:23.780388Z",
     "start_time": "2017-07-17T20:04:23.775336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:23.790105Z",
     "start_time": "2017-07-17T20:04:23.781669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:24.610711Z",
     "start_time": "2017-07-17T20:04:23.791493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99589320646770185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Normal','is_Attack']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "    \n",
    "    x_test__input = dataset.kdd_test__2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test_ = dataset.kdd_test__2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "    x_test_ = ss.transform(x_test__input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "    y_test_ = y_test_.values\n",
    "\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:26.450780Z",
     "start_time": "2017-07-17T20:04:24.612150Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.legacy_seq2seq.python.ops.seq2seq import basic_rnn_seq2seq\n",
    "from tensorflow.contrib.rnn import RNNCell, LSTMCell, MultiRNNCell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:26.719076Z",
     "start_time": "2017-07-17T20:04:26.452410Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x_input = tf.placeholder(\"float\", shape=[None, 1, input_dim])\n",
    "            self.y_input_ = tf.placeholder(\"float\", shape=[None, 1, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "            self.x_list = tf.unstack(self.x_input, axis= 1)\n",
    "            self.y_list_ = tf.unstack(self.y_input_, axis = 1)\n",
    "            self.y_ = self.y_list_[0]\n",
    "            \n",
    "            #GO = tf.fill((tf.shape(self.x)[0], 1), 0.5)\n",
    "            \n",
    "            #y_with_GO = tf.stack([self.y_, GO])\n",
    "            \n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            multi_cell = MultiRNNCell([LSTMCell(input_dim) for i in range(hidden_layers)] )\n",
    "            \n",
    "            self.y, states = basic_rnn_seq2seq(self.x_list, self.y_list_, multi_cell)\n",
    "            #self.y = tf.slice(self.y, [0, 0], [-1,2])\n",
    "            \n",
    "            #self.out = tf.squeeze(self.y)\n",
    "            \n",
    "            #self.y = tf.layers.dense(self.y[0], classes, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.y[0], [0, 0], [-1,2])\n",
    "            \n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            self.regularized_loss = tf.losses.mean_squared_error(self.y_, self.y)\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-01T00:59:00.684124Z",
     "start_time": "2017-06-01T00:58:59.843181Z"
    }
   },
   "source": [
    "batch_iterations = 200\n",
    "\n",
    "x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "for i in batch_indices:\n",
    "    print(x_train[i,np.newaxis,:])\n",
    "    print(y_train[i,np.newaxis,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:27.057881Z",
     "start_time": "2017-07-17T20:04:26.720764Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import sklearn.metrics as me\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'test_score_20', 'f1_score_20', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "    predictions_ = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 200\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_lstm_nsl_kdd-orig/hidden layers_{}_features count_{}\".format(h,f),\n",
    "                    exist_ok = True)\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x_input: preprocess.x_test[:,np.newaxis,:], \n",
    "                                                                             net.y_input_: preprocess.y_test[:,np.newaxis,:], \n",
    "                                                                             net.keep_prob:1})\n",
    "            \n",
    "            print(\"Initial Accuracy, before training: {}\".format(accuracy))\n",
    "            \n",
    "            for c, lr in enumerate(lrs):\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                              preprocess.y_train, \n",
    "                                                                              test_size=0.1)\n",
    "                    batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                               batch_iterations)\n",
    "\n",
    "                    for i in batch_indices:\n",
    "\n",
    "                        _, train_loss = sess.run([net.train_op, net.regularized_loss], #net.summary_op\n",
    "                                                              feed_dict={net.x_input: x_train[i,np.newaxis,:], \n",
    "                                                                         net.y_input_: y_train[i,np.newaxis,:], \n",
    "                                                                         net.keep_prob:1, net.lr:lr})\n",
    "                        #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                        if(train_loss > 1e9):\n",
    "                            print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "\n",
    "\n",
    "                    valid_accuracy,valid_loss = sess.run([net.tf_accuracy, net.regularized_loss], #net.summary_op \n",
    "                                                          feed_dict={net.x_input: x_valid[:,np.newaxis,:], \n",
    "                                                                     net.y_input_: y_valid[:,np.newaxis,:], \n",
    "                                                                     net.keep_prob:1, net.lr:lr})\n",
    "                    #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "\n",
    "\n",
    "                    accuracy, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x_input: preprocess.x_test[:,np.newaxis,:], \n",
    "                                                                             net.y_input_: preprocess.y_test[:,np.newaxis,:], \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score = me.f1_score(actual_value, pred_value)\n",
    "                    accuracy_, pred_value_, actual_value_, y_pred_ = sess.run([net.tf_accuracy, \n",
    "                                                                   net.pred, \n",
    "                                                                   net.actual, net.y], \n",
    "                                                                  feed_dict={net.x_input: preprocess.x_test_[:,np.newaxis,:], \n",
    "                                                                             net.y_input_: preprocess.y_test_[:,np.newaxis,:], \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                    f1_score_ = me.f1_score(actual_value_, pred_value_)\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Train Accuracy: {:.6f} | Test Accuracy: {:.6f}, {:.6f}\".format(epoch, train_loss, valid_accuracy, accuracy, accuracy_))\n",
    "\n",
    "                    if accuracy > Train.best_acc_global:\n",
    "                                Train.best_acc_global = accuracy\n",
    "                                Train.pred_value = pred_value\n",
    "                                Train.actual_value = actual_value\n",
    "                                Train.pred_value_ = pred_value_\n",
    "                                Train.actual_value_ = actual_value_\n",
    "                                Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                    if accuracy > Train.best_acc:\n",
    "\n",
    "                        #net.saver.save(sess, \"dataset/tf_vae_only_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                        #curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                        #Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "\n",
    "                        Train.best_acc = accuracy\n",
    "                        if not (np.isnan(train_loss)):\n",
    "                            net.saver.save(sess, \n",
    "                                       \"dataset/tf_lstm_nsl_kdd-orig/hidden layers_{}_features count_{}/model\"\n",
    "                                       .format(h,f), \n",
    "                                       global_step = epoch, \n",
    "                                       write_meta_graph=False)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value})\n",
    "                        curr_pred_ = pd.DataFrame({\"Attack_prob\":y_pred_[:,-2], \"Normal_prob\":y_pred_[:, -1], \"Prediction\":pred_value_})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format((epochs+1)* (c+1),f,h):\n",
    "                                                  (curr_pred,\n",
    "                                                   Train.result((epochs+1)*(c+1), f, h,valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "                        Train.predictions_.update({\"{}_{}_{}\".format((epochs+1)* (c+1),f,h):\n",
    "                                                  (curr_pred_,\n",
    "                                                   Train.result((epochs+1)*(c+1), f, h,valid_accuracy, accuracy, f1_score, accuracy_, f1_score_, time.perf_counter() - start_time))})\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:04:27.139195Z",
     "start_time": "2017-07-17T20:04:27.059360Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "        \n",
    "        features_arr = [1] #[4, 8, 16, 32]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        epochs = [10]\n",
    "        lrs = [1e-2, 1e-3]\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "            \n",
    "        dict1 = {}\n",
    "        dict1_ = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "\n",
    "        for k, (v1_, v2) in Train.predictions.items():\n",
    "            dict1_.update({k: v1_})\n",
    "\n",
    "            \n",
    "        Train.predictions = dict1\n",
    "        Train.predictions_ = dict1_\n",
    "\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "        temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl'):\n",
    "            past_scores = temp\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl\")\n",
    "\n",
    "        past_scores.append(temp).to_pickle(\"dataset/scores/tf_lstm_nsl_kdd-orig_all.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:40.114834Z",
     "start_time": "2017-07-17T20:04:27.140782Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.3958037495613098\n",
      "Step 1 | Training Loss: 0.008687 | Train Accuracy: 0.996746 | Test Accuracy: 0.804693, 0.628439\n",
      "Step 2 | Training Loss: 0.004563 | Train Accuracy: 0.998492 | Test Accuracy: 0.835566, 0.687173\n",
      "Step 3 | Training Loss: 0.002449 | Train Accuracy: 0.999524 | Test Accuracy: 0.866040, 0.745148\n",
      "Step 4 | Training Loss: 0.001877 | Train Accuracy: 0.999762 | Test Accuracy: 0.890082, 0.790886\n",
      "Step 5 | Training Loss: 0.001510 | Train Accuracy: 0.999921 | Test Accuracy: 0.882452, 0.776371\n",
      "Step 6 | Training Loss: 0.003397 | Train Accuracy: 1.000000 | Test Accuracy: 0.896513, 0.803122\n",
      "Step 7 | Training Loss: 0.001449 | Train Accuracy: 0.999841 | Test Accuracy: 0.900772, 0.811224\n",
      "Step 8 | Training Loss: 0.001533 | Train Accuracy: 0.999762 | Test Accuracy: 0.905784, 0.820759\n",
      "Step 9 | Training Loss: 0.002469 | Train Accuracy: 0.999682 | Test Accuracy: 0.926189, 0.859578\n",
      "Step 10 | Training Loss: 0.001514 | Train Accuracy: 0.999841 | Test Accuracy: 0.925435, 0.858143\n",
      "Step 1 | Training Loss: 0.001405 | Train Accuracy: 0.999841 | Test Accuracy: 0.924414, 0.856203\n",
      "Step 2 | Training Loss: 0.001397 | Train Accuracy: 0.999762 | Test Accuracy: 0.922596, 0.852743\n",
      "Step 3 | Training Loss: 0.001373 | Train Accuracy: 0.999841 | Test Accuracy: 0.923660, 0.854768\n",
      "Step 4 | Training Loss: 0.002079 | Train Accuracy: 0.999921 | Test Accuracy: 0.920467, 0.848692\n",
      "Step 5 | Training Loss: 0.001701 | Train Accuracy: 0.999841 | Test Accuracy: 0.921443, 0.850549\n",
      "Step 6 | Training Loss: 0.002334 | Train Accuracy: 0.999682 | Test Accuracy: 0.922729, 0.852996\n",
      "Step 7 | Training Loss: 0.001428 | Train Accuracy: 0.999762 | Test Accuracy: 0.924060, 0.855527\n",
      "Step 8 | Training Loss: 0.001675 | Train Accuracy: 0.999603 | Test Accuracy: 0.923306, 0.854093\n",
      "Step 9 | Training Loss: 0.001214 | Train Accuracy: 0.999524 | Test Accuracy: 0.919668, 0.847173\n",
      "Step 10 | Training Loss: 0.002037 | Train Accuracy: 0.999682 | Test Accuracy: 0.926322, 0.859831\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.48864442110061646\n",
      "Step 1 | Training Loss: 0.010705 | Train Accuracy: 0.976345 | Test Accuracy: 0.759493, 0.543629\n",
      "Step 2 | Training Loss: 0.000655 | Train Accuracy: 0.999921 | Test Accuracy: 0.988866, 0.978819\n",
      "Step 3 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.999601, 0.999241\n",
      "Step 4 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 5 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999956, 0.999916\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999956, 0.999916\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999956, 0.999916\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 10 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 1 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 3 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 6 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 10 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.6961941123008728\n",
      "Step 1 | Training Loss: 0.005634 | Train Accuracy: 0.997142 | Test Accuracy: 0.794801, 0.609620\n",
      "Step 2 | Training Loss: 0.002932 | Train Accuracy: 0.999047 | Test Accuracy: 0.833969, 0.684135\n",
      "Step 3 | Training Loss: 0.001595 | Train Accuracy: 0.999603 | Test Accuracy: 0.873048, 0.758481\n",
      "Step 4 | Training Loss: 0.002196 | Train Accuracy: 0.999682 | Test Accuracy: 0.896824, 0.803713\n",
      "Step 5 | Training Loss: 0.001456 | Train Accuracy: 1.000000 | Test Accuracy: 0.907559, 0.824135\n",
      "Step 6 | Training Loss: 0.002629 | Train Accuracy: 0.999841 | Test Accuracy: 0.916607, 0.841350\n",
      "Step 7 | Training Loss: 0.002352 | Train Accuracy: 0.999762 | Test Accuracy: 0.920067, 0.847933\n",
      "Step 8 | Training Loss: 0.001282 | Train Accuracy: 1.000000 | Test Accuracy: 0.922818, 0.853165\n",
      "Step 9 | Training Loss: 0.001460 | Train Accuracy: 0.999762 | Test Accuracy: 0.922685, 0.852911\n",
      "Step 10 | Training Loss: 0.001059 | Train Accuracy: 0.999921 | Test Accuracy: 0.924326, 0.856034\n",
      "Step 1 | Training Loss: 0.001394 | Train Accuracy: 0.999921 | Test Accuracy: 0.922064, 0.851730\n",
      "Step 2 | Training Loss: 0.001243 | Train Accuracy: 0.999841 | Test Accuracy: 0.922906, 0.853333\n",
      "Step 3 | Training Loss: 0.001268 | Train Accuracy: 0.999762 | Test Accuracy: 0.922463, 0.852489\n",
      "Step 4 | Training Loss: 0.001438 | Train Accuracy: 0.999762 | Test Accuracy: 0.922995, 0.853502\n",
      "Step 5 | Training Loss: 0.001252 | Train Accuracy: 0.999921 | Test Accuracy: 0.923128, 0.853755\n",
      "Step 6 | Training Loss: 0.002333 | Train Accuracy: 0.999921 | Test Accuracy: 0.923306, 0.854093\n",
      "Step 7 | Training Loss: 0.001302 | Train Accuracy: 0.999762 | Test Accuracy: 0.924281, 0.855949\n",
      "Step 8 | Training Loss: 0.001116 | Train Accuracy: 1.000000 | Test Accuracy: 0.925169, 0.857637\n",
      "Step 9 | Training Loss: 0.001111 | Train Accuracy: 1.000000 | Test Accuracy: 0.925568, 0.858397\n",
      "Step 10 | Training Loss: 0.001618 | Train Accuracy: 1.000000 | Test Accuracy: 0.925701, 0.858650\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.7069286704063416\n",
      "Step 1 | Training Loss: 0.034706 | Train Accuracy: 0.972694 | Test Accuracy: 0.768098, 0.560338\n",
      "Step 2 | Training Loss: 0.000698 | Train Accuracy: 0.999762 | Test Accuracy: 0.981769, 0.965316\n",
      "Step 3 | Training Loss: 0.000663 | Train Accuracy: 1.000000 | Test Accuracy: 0.982479, 0.966667\n",
      "Step 4 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.982922, 0.967511\n",
      "Step 5 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.984209, 0.969958\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.984076, 0.969705\n",
      "Step 7 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.984031, 0.969620\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.983632, 0.968861\n",
      "Step 9 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.983632, 0.968861\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.983144, 0.967932\n",
      "Step 1 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.983144, 0.967932\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.983144, 0.967932\n",
      "Step 3 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.983144, 0.967932\n",
      "Step 4 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.983144, 0.967932\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.983100, 0.967848\n",
      "Step 6 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.983100, 0.967848\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.983100, 0.967848\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.983100, 0.967848\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.983100, 0.967848\n",
      "Step 10 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.983100, 0.967848\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy, before training: 0.3455464839935303\n",
      "Step 1 | Training Loss: 0.007515 | Train Accuracy: 0.994761 | Test Accuracy: 0.782381, 0.585992\n",
      "Step 2 | Training Loss: 0.002489 | Train Accuracy: 0.998651 | Test Accuracy: 0.819109, 0.655865\n",
      "Step 3 | Training Loss: 0.002127 | Train Accuracy: 0.999286 | Test Accuracy: 0.833925, 0.684051\n",
      "Step 4 | Training Loss: 0.001871 | Train Accuracy: 0.999682 | Test Accuracy: 0.855837, 0.725738\n",
      "Step 5 | Training Loss: 0.002092 | Train Accuracy: 0.999444 | Test Accuracy: 0.861249, 0.736034\n",
      "Step 6 | Training Loss: 0.002284 | Train Accuracy: 0.999206 | Test Accuracy: 0.902546, 0.814599\n",
      "Step 7 | Training Loss: 0.001378 | Train Accuracy: 0.999682 | Test Accuracy: 0.903877, 0.817131\n",
      "Step 8 | Training Loss: 0.001617 | Train Accuracy: 0.999603 | Test Accuracy: 0.901127, 0.811899\n",
      "Step 9 | Training Loss: 0.001372 | Train Accuracy: 0.999603 | Test Accuracy: 0.905119, 0.819494\n",
      "Step 10 | Training Loss: 0.001580 | Train Accuracy: 0.999921 | Test Accuracy: 0.907115, 0.823291\n",
      "Step 1 | Training Loss: 0.001746 | Train Accuracy: 0.999365 | Test Accuracy: 0.906627, 0.822363\n",
      "Step 2 | Training Loss: 0.001529 | Train Accuracy: 0.999603 | Test Accuracy: 0.905429, 0.820084\n",
      "Step 3 | Training Loss: 0.002373 | Train Accuracy: 0.999286 | Test Accuracy: 0.907248, 0.823544\n",
      "Step 4 | Training Loss: 0.001617 | Train Accuracy: 0.999682 | Test Accuracy: 0.906450, 0.822025\n",
      "Step 5 | Training Loss: 0.001496 | Train Accuracy: 0.999682 | Test Accuracy: 0.905651, 0.820506\n",
      "Step 6 | Training Loss: 0.002378 | Train Accuracy: 0.999444 | Test Accuracy: 0.906494, 0.822110\n",
      "Step 7 | Training Loss: 0.001091 | Train Accuracy: 0.999365 | Test Accuracy: 0.905341, 0.819916\n",
      "Step 8 | Training Loss: 0.001539 | Train Accuracy: 0.999682 | Test Accuracy: 0.904897, 0.819072\n",
      "Step 9 | Training Loss: 0.001596 | Train Accuracy: 0.999365 | Test Accuracy: 0.906228, 0.821603\n",
      "Step 10 | Training Loss: 0.002422 | Train Accuracy: 0.999524 | Test Accuracy: 0.905208, 0.819662\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.4933907091617584\n",
      "Step 1 | Training Loss: 0.021556 | Train Accuracy: 0.974520 | Test Accuracy: 0.801144, 0.622869\n",
      "Step 2 | Training Loss: 0.000663 | Train Accuracy: 0.999841 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 3 | Training Loss: 0.000652 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 4 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 5 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 6 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 7 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 9 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 1 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 3 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 6 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 1.000000, 1.000000\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.45355749130249023\n",
      "Step 1 | Training Loss: 0.011825 | Train Accuracy: 0.988252 | Test Accuracy: 0.775506, 0.572911\n",
      "Step 2 | Training Loss: 0.002752 | Train Accuracy: 0.998651 | Test Accuracy: 0.823944, 0.665063\n",
      "Step 3 | Training Loss: 0.003040 | Train Accuracy: 0.999682 | Test Accuracy: 0.847897, 0.710633\n",
      "Step 4 | Training Loss: 0.002354 | Train Accuracy: 1.000000 | Test Accuracy: 0.889993, 0.790717\n",
      "Step 5 | Training Loss: 0.002695 | Train Accuracy: 0.999921 | Test Accuracy: 0.901881, 0.813333\n",
      "Step 6 | Training Loss: 0.003028 | Train Accuracy: 1.000000 | Test Accuracy: 0.905917, 0.821013\n",
      "Step 7 | Training Loss: 0.002380 | Train Accuracy: 0.999841 | Test Accuracy: 0.910131, 0.829030\n",
      "Step 8 | Training Loss: 0.002256 | Train Accuracy: 0.999762 | Test Accuracy: 0.914878, 0.838059\n",
      "Step 9 | Training Loss: 0.002405 | Train Accuracy: 0.999921 | Test Accuracy: 0.911639, 0.831899\n",
      "Step 10 | Training Loss: 0.002668 | Train Accuracy: 0.999921 | Test Accuracy: 0.912527, 0.833587\n",
      "Step 1 | Training Loss: 0.002008 | Train Accuracy: 0.999921 | Test Accuracy: 0.913724, 0.835865\n",
      "Step 2 | Training Loss: 0.001886 | Train Accuracy: 0.999921 | Test Accuracy: 0.913813, 0.836034\n",
      "Step 3 | Training Loss: 0.001775 | Train Accuracy: 1.000000 | Test Accuracy: 0.913414, 0.835274\n",
      "Step 4 | Training Loss: 0.002228 | Train Accuracy: 0.999921 | Test Accuracy: 0.913769, 0.835949\n",
      "Step 5 | Training Loss: 0.002442 | Train Accuracy: 1.000000 | Test Accuracy: 0.913857, 0.836118\n",
      "Step 6 | Training Loss: 0.001890 | Train Accuracy: 0.999921 | Test Accuracy: 0.914168, 0.836709\n",
      "Step 7 | Training Loss: 0.002261 | Train Accuracy: 0.999921 | Test Accuracy: 0.913724, 0.835865\n",
      "Step 8 | Training Loss: 0.002261 | Train Accuracy: 1.000000 | Test Accuracy: 0.913680, 0.835781\n",
      "Step 9 | Training Loss: 0.001981 | Train Accuracy: 1.000000 | Test Accuracy: 0.913680, 0.835781\n",
      "Step 10 | Training Loss: 0.001828 | Train Accuracy: 0.999921 | Test Accuracy: 0.914789, 0.837890\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.24915720522403717\n",
      "Step 1 | Training Loss: 0.020480 | Train Accuracy: 0.978409 | Test Accuracy: 0.798616, 0.618059\n",
      "Step 2 | Training Loss: 0.002363 | Train Accuracy: 0.993967 | Test Accuracy: 0.888795, 0.788439\n",
      "Step 3 | Training Loss: 0.000655 | Train Accuracy: 0.999841 | Test Accuracy: 0.999423, 0.998903\n",
      "Step 4 | Training Loss: 0.000662 | Train Accuracy: 1.000000 | Test Accuracy: 0.999956, 0.999916\n",
      "Step 5 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 7 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 9 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 10 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 1 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 3 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 6 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.3086852431297302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Training Loss: 0.006128 | Train Accuracy: 0.997698 | Test Accuracy: 0.799193, 0.617975\n",
      "Step 2 | Training Loss: 0.005114 | Train Accuracy: 0.998968 | Test Accuracy: 0.825719, 0.668439\n",
      "Step 3 | Training Loss: 0.001793 | Train Accuracy: 0.999524 | Test Accuracy: 0.873980, 0.760253\n",
      "Step 4 | Training Loss: 0.001935 | Train Accuracy: 0.999524 | Test Accuracy: 0.895360, 0.800928\n",
      "Step 5 | Training Loss: 0.001902 | Train Accuracy: 0.999365 | Test Accuracy: 0.905740, 0.820675\n",
      "Step 6 | Training Loss: 0.001507 | Train Accuracy: 0.999762 | Test Accuracy: 0.924414, 0.856203\n",
      "Step 7 | Training Loss: 0.001475 | Train Accuracy: 1.000000 | Test Accuracy: 0.929604, 0.866076\n",
      "Step 8 | Training Loss: 0.001388 | Train Accuracy: 0.999682 | Test Accuracy: 0.931423, 0.869536\n",
      "Step 9 | Training Loss: 0.001710 | Train Accuracy: 0.999762 | Test Accuracy: 0.929427, 0.865738\n",
      "Step 10 | Training Loss: 0.001522 | Train Accuracy: 0.999841 | Test Accuracy: 0.929915, 0.866667\n",
      "Step 1 | Training Loss: 0.002121 | Train Accuracy: 0.999841 | Test Accuracy: 0.929028, 0.864979\n",
      "Step 2 | Training Loss: 0.001479 | Train Accuracy: 0.999682 | Test Accuracy: 0.929205, 0.865316\n",
      "Step 3 | Training Loss: 0.001257 | Train Accuracy: 0.999921 | Test Accuracy: 0.929249, 0.865401\n",
      "Step 4 | Training Loss: 0.001126 | Train Accuracy: 0.999841 | Test Accuracy: 0.929383, 0.865654\n",
      "Step 5 | Training Loss: 0.001290 | Train Accuracy: 0.999762 | Test Accuracy: 0.929383, 0.865654\n",
      "Step 6 | Training Loss: 0.001477 | Train Accuracy: 0.999682 | Test Accuracy: 0.929205, 0.865316\n",
      "Step 7 | Training Loss: 0.001343 | Train Accuracy: 0.999921 | Test Accuracy: 0.929427, 0.865738\n",
      "Step 8 | Training Loss: 0.001064 | Train Accuracy: 0.999921 | Test Accuracy: 0.928939, 0.864810\n",
      "Step 9 | Training Loss: 0.001345 | Train Accuracy: 0.999921 | Test Accuracy: 0.929338, 0.865570\n",
      "Step 10 | Training Loss: 0.001502 | Train Accuracy: 1.000000 | Test Accuracy: 0.929471, 0.865823\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.7524840235710144\n",
      "Step 1 | Training Loss: 0.023068 | Train Accuracy: 0.976742 | Test Accuracy: 0.772977, 0.569283\n",
      "Step 2 | Training Loss: 0.001144 | Train Accuracy: 0.999603 | Test Accuracy: 0.976313, 0.954937\n",
      "Step 3 | Training Loss: 0.000904 | Train Accuracy: 0.999762 | Test Accuracy: 0.998181, 0.996540\n",
      "Step 4 | Training Loss: 0.001045 | Train Accuracy: 1.000000 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 5 | Training Loss: 0.001141 | Train Accuracy: 0.999841 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 6 | Training Loss: 0.000944 | Train Accuracy: 1.000000 | Test Accuracy: 0.998314, 0.996793\n",
      "Step 7 | Training Loss: 0.001287 | Train Accuracy: 1.000000 | Test Accuracy: 0.998314, 0.996793\n",
      "Step 8 | Training Loss: 0.001139 | Train Accuracy: 1.000000 | Test Accuracy: 0.998314, 0.996793\n",
      "Step 9 | Training Loss: 0.000942 | Train Accuracy: 1.000000 | Test Accuracy: 0.998314, 0.996793\n",
      "Step 10 | Training Loss: 0.001090 | Train Accuracy: 1.000000 | Test Accuracy: 0.998359, 0.996878\n",
      "Step 1 | Training Loss: 0.000844 | Train Accuracy: 1.000000 | Test Accuracy: 0.998403, 0.996962\n",
      "Step 2 | Training Loss: 0.000893 | Train Accuracy: 1.000000 | Test Accuracy: 0.998403, 0.996962\n",
      "Step 3 | Training Loss: 0.000893 | Train Accuracy: 1.000000 | Test Accuracy: 0.998447, 0.997046\n",
      "Step 4 | Training Loss: 0.000991 | Train Accuracy: 1.000000 | Test Accuracy: 0.998447, 0.997046\n",
      "Step 5 | Training Loss: 0.000991 | Train Accuracy: 1.000000 | Test Accuracy: 0.998447, 0.997046\n",
      "Step 6 | Training Loss: 0.001040 | Train Accuracy: 1.000000 | Test Accuracy: 0.998536, 0.997215\n",
      "Step 7 | Training Loss: 0.000945 | Train Accuracy: 1.000000 | Test Accuracy: 0.998536, 0.997215\n",
      "Step 8 | Training Loss: 0.000942 | Train Accuracy: 1.000000 | Test Accuracy: 0.998536, 0.997215\n",
      "Step 9 | Training Loss: 0.000893 | Train Accuracy: 1.000000 | Test Accuracy: 0.998581, 0.997300\n",
      "Step 10 | Training Loss: 0.000844 | Train Accuracy: 1.000000 | Test Accuracy: 0.998669, 0.997468\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.577492892742157\n",
      "Step 1 | Training Loss: 0.004851 | Train Accuracy: 0.997142 | Test Accuracy: 0.814186, 0.646498\n",
      "Step 2 | Training Loss: 0.002201 | Train Accuracy: 0.999682 | Test Accuracy: 0.850559, 0.716371\n",
      "Step 3 | Training Loss: 0.003345 | Train Accuracy: 0.999841 | Test Accuracy: 0.882940, 0.777300\n",
      "Step 4 | Training Loss: 0.002548 | Train Accuracy: 0.999762 | Test Accuracy: 0.892654, 0.795781\n",
      "Step 5 | Training Loss: 0.001836 | Train Accuracy: 0.999682 | Test Accuracy: 0.899752, 0.809283\n",
      "Step 6 | Training Loss: 0.002795 | Train Accuracy: 0.999524 | Test Accuracy: 0.907381, 0.823797\n",
      "Step 7 | Training Loss: 0.001493 | Train Accuracy: 0.999921 | Test Accuracy: 0.919801, 0.847426\n",
      "Step 8 | Training Loss: 0.001583 | Train Accuracy: 0.999762 | Test Accuracy: 0.917716, 0.843460\n",
      "Step 9 | Training Loss: 0.001593 | Train Accuracy: 0.999762 | Test Accuracy: 0.922729, 0.852996\n",
      "Step 10 | Training Loss: 0.001596 | Train Accuracy: 0.999841 | Test Accuracy: 0.921576, 0.850802\n",
      "Step 1 | Training Loss: 0.002221 | Train Accuracy: 1.000000 | Test Accuracy: 0.923084, 0.853671\n",
      "Step 2 | Training Loss: 0.001678 | Train Accuracy: 0.999841 | Test Accuracy: 0.923306, 0.854093\n",
      "Step 3 | Training Loss: 0.001593 | Train Accuracy: 0.999762 | Test Accuracy: 0.923572, 0.854599\n",
      "Step 4 | Training Loss: 0.001626 | Train Accuracy: 0.999762 | Test Accuracy: 0.924148, 0.855696\n",
      "Step 5 | Training Loss: 0.001580 | Train Accuracy: 0.999762 | Test Accuracy: 0.924281, 0.855949\n",
      "Step 6 | Training Loss: 0.001265 | Train Accuracy: 0.999841 | Test Accuracy: 0.924370, 0.856118\n",
      "Step 7 | Training Loss: 0.001682 | Train Accuracy: 0.999841 | Test Accuracy: 0.925834, 0.858903\n",
      "Step 8 | Training Loss: 0.001739 | Train Accuracy: 0.999841 | Test Accuracy: 0.927475, 0.862025\n",
      "Step 9 | Training Loss: 0.001389 | Train Accuracy: 0.999921 | Test Accuracy: 0.927165, 0.861435\n",
      "Step 10 | Training Loss: 0.002466 | Train Accuracy: 0.999921 | Test Accuracy: 0.927165, 0.861435\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.409554660320282\n",
      "Step 1 | Training Loss: 0.007483 | Train Accuracy: 0.992856 | Test Accuracy: 0.955199, 0.915949\n",
      "Step 2 | Training Loss: 0.000659 | Train Accuracy: 0.999841 | Test Accuracy: 0.970813, 0.944473\n",
      "Step 3 | Training Loss: 0.000652 | Train Accuracy: 0.999841 | Test Accuracy: 0.977999, 0.958143\n",
      "Step 4 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.978708, 0.959494\n",
      "Step 5 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.978531, 0.959156\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.978442, 0.958987\n",
      "Step 7 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.978353, 0.958819\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.978309, 0.958734\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 0.999921 | Test Accuracy: 0.978309, 0.958734\n",
      "Step 10 | Training Loss: 0.001530 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 1 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 2 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 3 | Training Loss: 0.002412 | Train Accuracy: 0.999921 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 4 | Training Loss: 0.003294 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 0.999921 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 6 | Training Loss: 0.001530 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 7 | Training Loss: 0.001530 | Train Accuracy: 0.999762 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 9 | Training Loss: 0.001530 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Step 10 | Training Loss: 0.001530 | Train Accuracy: 1.000000 | Test Accuracy: 0.978265, 0.958650\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.5704400539398193\n",
      "Step 1 | Training Loss: 0.004706 | Train Accuracy: 0.996428 | Test Accuracy: 0.788724, 0.598059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.003823 | Train Accuracy: 0.998809 | Test Accuracy: 0.825541, 0.668101\n",
      "Step 3 | Training Loss: 0.002416 | Train Accuracy: 0.999047 | Test Accuracy: 0.850248, 0.715105\n",
      "Step 4 | Training Loss: 0.001346 | Train Accuracy: 0.999127 | Test Accuracy: 0.862225, 0.737890\n",
      "Step 5 | Training Loss: 0.002379 | Train Accuracy: 0.999444 | Test Accuracy: 0.871141, 0.754852\n",
      "Step 6 | Training Loss: 0.002042 | Train Accuracy: 0.999206 | Test Accuracy: 0.894695, 0.799662\n",
      "Step 7 | Training Loss: 0.002073 | Train Accuracy: 1.000000 | Test Accuracy: 0.896336, 0.802785\n",
      "Step 8 | Training Loss: 0.001044 | Train Accuracy: 0.999841 | Test Accuracy: 0.899574, 0.808945\n",
      "Step 9 | Training Loss: 0.001104 | Train Accuracy: 0.999603 | Test Accuracy: 0.907736, 0.824473\n",
      "Step 10 | Training Loss: 0.001965 | Train Accuracy: 0.999841 | Test Accuracy: 0.913591, 0.835612\n",
      "Step 1 | Training Loss: 0.001038 | Train Accuracy: 0.999841 | Test Accuracy: 0.914390, 0.837131\n",
      "Step 2 | Training Loss: 0.001200 | Train Accuracy: 0.999841 | Test Accuracy: 0.916253, 0.840675\n",
      "Step 3 | Training Loss: 0.001586 | Train Accuracy: 1.000000 | Test Accuracy: 0.917628, 0.843291\n",
      "Step 4 | Training Loss: 0.001045 | Train Accuracy: 1.000000 | Test Accuracy: 0.917716, 0.843460\n",
      "Step 5 | Training Loss: 0.001311 | Train Accuracy: 0.999921 | Test Accuracy: 0.917362, 0.842785\n",
      "Step 6 | Training Loss: 0.001450 | Train Accuracy: 0.999762 | Test Accuracy: 0.916918, 0.841941\n",
      "Step 7 | Training Loss: 0.001140 | Train Accuracy: 0.999762 | Test Accuracy: 0.916962, 0.842025\n",
      "Step 8 | Training Loss: 0.001274 | Train Accuracy: 0.999921 | Test Accuracy: 0.921975, 0.851561\n",
      "Step 9 | Training Loss: 0.001232 | Train Accuracy: 0.999921 | Test Accuracy: 0.922640, 0.852827\n",
      "Step 10 | Training Loss: 0.002056 | Train Accuracy: 0.999841 | Test Accuracy: 0.922951, 0.853418\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.2347853034734726\n",
      "Step 1 | Training Loss: 0.000667 | Train Accuracy: 0.999286 | Test Accuracy: 0.995520, 0.991477\n",
      "Step 2 | Training Loss: 0.000651 | Train Accuracy: 0.999762 | Test Accuracy: 0.997250, 0.994768\n",
      "Step 3 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.998980, 0.998059\n",
      "Step 4 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 5 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 7 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 0.999921 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 1 | Training Loss: 0.000647 | Train Accuracy: 0.999921 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 3 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 0.999921 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 9 | Training Loss: 0.000648 | Train Accuracy: 0.999921 | Test Accuracy: 0.998802, 0.997721\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998802, 0.997721\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.5534510016441345\n",
      "Step 1 | Training Loss: 0.012099 | Train Accuracy: 0.989840 | Test Accuracy: 0.790587, 0.601603\n",
      "Step 2 | Training Loss: 0.002154 | Train Accuracy: 0.999206 | Test Accuracy: 0.837385, 0.690633\n",
      "Step 3 | Training Loss: 0.001844 | Train Accuracy: 0.999762 | Test Accuracy: 0.874423, 0.761097\n",
      "Step 4 | Training Loss: 0.001417 | Train Accuracy: 1.000000 | Test Accuracy: 0.893630, 0.797637\n",
      "Step 5 | Training Loss: 0.000894 | Train Accuracy: 0.999921 | Test Accuracy: 0.896336, 0.802785\n",
      "Step 6 | Training Loss: 0.001823 | Train Accuracy: 1.000000 | Test Accuracy: 0.907825, 0.824641\n",
      "Step 7 | Training Loss: 0.001592 | Train Accuracy: 1.000000 | Test Accuracy: 0.927697, 0.862447\n",
      "Step 8 | Training Loss: 0.001404 | Train Accuracy: 1.000000 | Test Accuracy: 0.919313, 0.846498\n",
      "Step 9 | Training Loss: 0.001053 | Train Accuracy: 1.000000 | Test Accuracy: 0.927386, 0.861857\n",
      "Step 10 | Training Loss: 0.001165 | Train Accuracy: 1.000000 | Test Accuracy: 0.928407, 0.863797\n",
      "Step 1 | Training Loss: 0.001004 | Train Accuracy: 0.999921 | Test Accuracy: 0.928673, 0.864304\n",
      "Step 2 | Training Loss: 0.000935 | Train Accuracy: 1.000000 | Test Accuracy: 0.928052, 0.863122\n",
      "Step 3 | Training Loss: 0.001192 | Train Accuracy: 0.999921 | Test Accuracy: 0.927741, 0.862532\n",
      "Step 4 | Training Loss: 0.001194 | Train Accuracy: 1.000000 | Test Accuracy: 0.929205, 0.865316\n",
      "Step 5 | Training Loss: 0.001093 | Train Accuracy: 1.000000 | Test Accuracy: 0.928939, 0.864810\n",
      "Step 6 | Training Loss: 0.000844 | Train Accuracy: 1.000000 | Test Accuracy: 0.927963, 0.862954\n",
      "Step 7 | Training Loss: 0.000788 | Train Accuracy: 1.000000 | Test Accuracy: 0.929205, 0.865316\n",
      "Step 8 | Training Loss: 0.000935 | Train Accuracy: 1.000000 | Test Accuracy: 0.929560, 0.865992\n",
      "Step 9 | Training Loss: 0.001034 | Train Accuracy: 1.000000 | Test Accuracy: 0.930314, 0.867426\n",
      "Step 10 | Training Loss: 0.001344 | Train Accuracy: 0.999921 | Test Accuracy: 0.928717, 0.864388\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.5179204940795898\n",
      "Step 1 | Training Loss: 0.024915 | Train Accuracy: 0.973012 | Test Accuracy: 0.777768, 0.578819\n",
      "Step 2 | Training Loss: 0.000680 | Train Accuracy: 1.000000 | Test Accuracy: 0.998891, 0.997890\n",
      "Step 3 | Training Loss: 0.000658 | Train Accuracy: 1.000000 | Test Accuracy: 0.999867, 0.999747\n",
      "Step 4 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.999867, 0.999747\n",
      "Step 5 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.999867, 0.999747\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999867, 0.999747\n",
      "Step 7 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.999867, 0.999747\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999867, 0.999747\n",
      "Step 9 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999867, 0.999747\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 1 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 3 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 6 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999911, 0.999831\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.8003460168838501\n",
      "Step 1 | Training Loss: 0.007397 | Train Accuracy: 0.996984 | Test Accuracy: 0.791785, 0.603882\n",
      "Step 2 | Training Loss: 0.002781 | Train Accuracy: 0.999206 | Test Accuracy: 0.827848, 0.672489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Training Loss: 0.001414 | Train Accuracy: 0.999206 | Test Accuracy: 0.857168, 0.728270\n",
      "Step 4 | Training Loss: 0.001365 | Train Accuracy: 0.999603 | Test Accuracy: 0.880456, 0.772574\n",
      "Step 5 | Training Loss: 0.001315 | Train Accuracy: 0.999762 | Test Accuracy: 0.890481, 0.791646\n",
      "Step 6 | Training Loss: 0.001319 | Train Accuracy: 0.999921 | Test Accuracy: 0.901969, 0.813502\n",
      "Step 7 | Training Loss: 0.001120 | Train Accuracy: 0.999841 | Test Accuracy: 0.893054, 0.796540\n",
      "Step 8 | Training Loss: 0.001166 | Train Accuracy: 0.999921 | Test Accuracy: 0.894029, 0.798397\n",
      "Step 9 | Training Loss: 0.000994 | Train Accuracy: 0.999921 | Test Accuracy: 0.894562, 0.799409\n",
      "Step 10 | Training Loss: 0.001185 | Train Accuracy: 1.000000 | Test Accuracy: 0.897534, 0.805063\n",
      "Step 1 | Training Loss: 0.001200 | Train Accuracy: 1.000000 | Test Accuracy: 0.899086, 0.808017\n",
      "Step 2 | Training Loss: 0.001036 | Train Accuracy: 1.000000 | Test Accuracy: 0.900151, 0.810042\n",
      "Step 3 | Training Loss: 0.000951 | Train Accuracy: 1.000000 | Test Accuracy: 0.898554, 0.807004\n",
      "Step 4 | Training Loss: 0.001183 | Train Accuracy: 1.000000 | Test Accuracy: 0.897977, 0.805907\n",
      "Step 5 | Training Loss: 0.000878 | Train Accuracy: 1.000000 | Test Accuracy: 0.899264, 0.808354\n",
      "Step 6 | Training Loss: 0.001133 | Train Accuracy: 1.000000 | Test Accuracy: 0.900373, 0.810464\n",
      "Step 7 | Training Loss: 0.001196 | Train Accuracy: 0.999921 | Test Accuracy: 0.899131, 0.808101\n",
      "Step 8 | Training Loss: 0.001013 | Train Accuracy: 1.000000 | Test Accuracy: 0.900240, 0.810211\n",
      "Step 9 | Training Loss: 0.001150 | Train Accuracy: 1.000000 | Test Accuracy: 0.899796, 0.809367\n",
      "Step 10 | Training Loss: 0.001150 | Train Accuracy: 1.000000 | Test Accuracy: 0.900905, 0.811477\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.24543115496635437\n",
      "Step 1 | Training Loss: 0.019672 | Train Accuracy: 0.979124 | Test Accuracy: 0.738201, 0.502279\n",
      "Step 2 | Training Loss: 0.002357 | Train Accuracy: 0.999047 | Test Accuracy: 0.982434, 0.966920\n",
      "Step 3 | Training Loss: 0.000655 | Train Accuracy: 1.000000 | Test Accuracy: 0.980438, 0.962785\n",
      "Step 4 | Training Loss: 0.002487 | Train Accuracy: 1.000000 | Test Accuracy: 0.983011, 0.967679\n",
      "Step 5 | Training Loss: 0.000649 | Train Accuracy: 0.999921 | Test Accuracy: 0.999423, 0.998903\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 0.999921 | Test Accuracy: 0.999423, 0.998903\n",
      "Step 7 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999468, 0.998987\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999468, 0.998987\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999468, 0.998987\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999468, 0.998987\n",
      "Step 1 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999468, 0.998987\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999468, 0.998987\n",
      "Step 3 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Step 5 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Step 6 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.999512, 0.999072\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.40596166253089905\n",
      "Step 1 | Training Loss: 0.004931 | Train Accuracy: 0.996984 | Test Accuracy: 0.805225, 0.629536\n",
      "Step 2 | Training Loss: 0.002470 | Train Accuracy: 0.999206 | Test Accuracy: 0.840623, 0.696793\n",
      "Step 3 | Training Loss: 0.002708 | Train Accuracy: 0.999603 | Test Accuracy: 0.869810, 0.752321\n",
      "Step 4 | Training Loss: 0.002005 | Train Accuracy: 0.999762 | Test Accuracy: 0.897223, 0.804473\n",
      "Step 5 | Training Loss: 0.001990 | Train Accuracy: 0.999921 | Test Accuracy: 0.892388, 0.795274\n",
      "Step 6 | Training Loss: 0.002056 | Train Accuracy: 0.999921 | Test Accuracy: 0.905429, 0.820084\n",
      "Step 7 | Training Loss: 0.001742 | Train Accuracy: 0.999762 | Test Accuracy: 0.902857, 0.815190\n",
      "Step 8 | Training Loss: 0.002844 | Train Accuracy: 1.000000 | Test Accuracy: 0.906405, 0.821941\n",
      "Step 9 | Training Loss: 0.001601 | Train Accuracy: 1.000000 | Test Accuracy: 0.906006, 0.821181\n",
      "Step 10 | Training Loss: 0.001618 | Train Accuracy: 0.999921 | Test Accuracy: 0.912349, 0.833249\n",
      "Step 1 | Training Loss: 0.001731 | Train Accuracy: 0.999921 | Test Accuracy: 0.911462, 0.831561\n",
      "Step 2 | Training Loss: 0.001426 | Train Accuracy: 0.999921 | Test Accuracy: 0.910619, 0.829958\n",
      "Step 3 | Training Loss: 0.001570 | Train Accuracy: 0.999921 | Test Accuracy: 0.910353, 0.829452\n",
      "Step 4 | Training Loss: 0.001523 | Train Accuracy: 0.999921 | Test Accuracy: 0.909998, 0.828776\n",
      "Step 5 | Training Loss: 0.001815 | Train Accuracy: 0.999841 | Test Accuracy: 0.910264, 0.829283\n",
      "Step 6 | Training Loss: 0.001872 | Train Accuracy: 0.999841 | Test Accuracy: 0.909909, 0.828608\n",
      "Step 7 | Training Loss: 0.002083 | Train Accuracy: 1.000000 | Test Accuracy: 0.909954, 0.828692\n",
      "Step 8 | Training Loss: 0.001242 | Train Accuracy: 0.999921 | Test Accuracy: 0.910442, 0.829620\n",
      "Step 9 | Training Loss: 0.001403 | Train Accuracy: 0.999921 | Test Accuracy: 0.911462, 0.831561\n",
      "Step 10 | Training Loss: 0.001296 | Train Accuracy: 0.999841 | Test Accuracy: 0.913902, 0.836203\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.42831796407699585\n",
      "Step 1 | Training Loss: 0.000658 | Train Accuracy: 0.999762 | Test Accuracy: 0.997250, 0.994768\n",
      "Step 2 | Training Loss: 0.002416 | Train Accuracy: 1.000000 | Test Accuracy: 0.999024, 0.998143\n",
      "Step 3 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 4 | Training Loss: 0.000648 | Train Accuracy: 0.999762 | Test Accuracy: 0.998536, 0.997215\n",
      "Step 5 | Training Loss: 0.000648 | Train Accuracy: 0.999921 | Test Accuracy: 0.998669, 0.997468\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998625, 0.997384\n",
      "Step 7 | Training Loss: 0.001530 | Train Accuracy: 0.999921 | Test Accuracy: 0.998581, 0.997300\n",
      "Step 8 | Training Loss: 0.001530 | Train Accuracy: 0.999762 | Test Accuracy: 0.998447, 0.997046\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 0.999841 | Test Accuracy: 0.998314, 0.996793\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 1 | Training Loss: 0.000647 | Train Accuracy: 0.999762 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 2 | Training Loss: 0.001530 | Train Accuracy: 0.999921 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 3 | Training Loss: 0.002412 | Train Accuracy: 1.000000 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 0.999841 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 6 | Training Loss: 0.000647 | Train Accuracy: 0.999841 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 8 | Training Loss: 0.001530 | Train Accuracy: 0.999682 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 9 | Training Loss: 0.001530 | Train Accuracy: 0.999921 | Test Accuracy: 0.998270, 0.996709\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 0.999841 | Test Accuracy: 0.998270, 0.996709\n",
      "Current Layer Attributes - epochs:10 hidden layers:1 features count:1\n",
      "Initial Accuracy, before training: 0.804426908493042\n",
      "Step 1 | Training Loss: 0.003145 | Train Accuracy: 0.997222 | Test Accuracy: 0.815161, 0.648354\n",
      "Step 2 | Training Loss: 0.001808 | Train Accuracy: 0.999127 | Test Accuracy: 0.827892, 0.672574\n",
      "Step 3 | Training Loss: 0.002506 | Train Accuracy: 0.999921 | Test Accuracy: 0.879125, 0.770042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 | Training Loss: 0.001773 | Train Accuracy: 0.999921 | Test Accuracy: 0.906095, 0.821350\n",
      "Step 5 | Training Loss: 0.001253 | Train Accuracy: 0.999603 | Test Accuracy: 0.905873, 0.820928\n",
      "Step 6 | Training Loss: 0.000943 | Train Accuracy: 1.000000 | Test Accuracy: 0.904365, 0.818059\n",
      "Step 7 | Training Loss: 0.001625 | Train Accuracy: 1.000000 | Test Accuracy: 0.916563, 0.841266\n",
      "Step 8 | Training Loss: 0.001216 | Train Accuracy: 0.999921 | Test Accuracy: 0.911639, 0.831899\n",
      "Step 9 | Training Loss: 0.000908 | Train Accuracy: 1.000000 | Test Accuracy: 0.915055, 0.838397\n",
      "Step 10 | Training Loss: 0.001351 | Train Accuracy: 1.000000 | Test Accuracy: 0.920777, 0.849283\n",
      "Step 1 | Training Loss: 0.000913 | Train Accuracy: 1.000000 | Test Accuracy: 0.919491, 0.846835\n",
      "Step 2 | Training Loss: 0.000883 | Train Accuracy: 1.000000 | Test Accuracy: 0.920023, 0.847848\n",
      "Step 3 | Training Loss: 0.001032 | Train Accuracy: 1.000000 | Test Accuracy: 0.919890, 0.847595\n",
      "Step 4 | Training Loss: 0.001032 | Train Accuracy: 1.000000 | Test Accuracy: 0.920112, 0.848017\n",
      "Step 5 | Training Loss: 0.001374 | Train Accuracy: 1.000000 | Test Accuracy: 0.920023, 0.847848\n",
      "Step 6 | Training Loss: 0.000992 | Train Accuracy: 1.000000 | Test Accuracy: 0.919979, 0.847764\n",
      "Step 7 | Training Loss: 0.001159 | Train Accuracy: 1.000000 | Test Accuracy: 0.919846, 0.847511\n",
      "Step 8 | Training Loss: 0.001184 | Train Accuracy: 1.000000 | Test Accuracy: 0.920422, 0.848608\n",
      "Step 9 | Training Loss: 0.000996 | Train Accuracy: 1.000000 | Test Accuracy: 0.920644, 0.849030\n",
      "Step 10 | Training Loss: 0.001314 | Train Accuracy: 1.000000 | Test Accuracy: 0.920023, 0.847848\n",
      "Current Layer Attributes - epochs:10 hidden layers:3 features count:1\n",
      "Initial Accuracy, before training: 0.4509403705596924\n",
      "Step 1 | Training Loss: 0.022657 | Train Accuracy: 0.972297 | Test Accuracy: 0.756609, 0.538143\n",
      "Step 2 | Training Loss: 0.001118 | Train Accuracy: 0.999921 | Test Accuracy: 0.998359, 0.996878\n",
      "Step 3 | Training Loss: 0.000658 | Train Accuracy: 1.000000 | Test Accuracy: 0.999068, 0.998228\n",
      "Step 4 | Training Loss: 0.000650 | Train Accuracy: 1.000000 | Test Accuracy: 0.998758, 0.997637\n",
      "Step 5 | Training Loss: 0.000649 | Train Accuracy: 1.000000 | Test Accuracy: 0.998758, 0.997637\n",
      "Step 6 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998758, 0.997637\n",
      "Step 7 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 8 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 9 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 1 | Training Loss: 0.000648 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 2 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 3 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 4 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 5 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 6 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 7 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 8 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 9 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "Step 10 | Training Loss: 0.000647 | Train Accuracy: 1.000000 | Test Accuracy: 0.998714, 0.997553\n",
      "3min 8s  9.16 s per loop (mean  std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 10\n",
    "\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T20:12:38.742950Z",
     "start_time": "2017-06-16T20:12:38.705898Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-16T20:12:38.748800Z",
     "start_time": "2017-06-16T20:12:38.744607Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:40.133090Z",
     "start_time": "2017-07-17T20:38:40.116436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999182</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>18.58071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "1     11               1              3          1.0    0.999068  0.999182   \n",
       "\n",
       "   test_score_20  f1_score_20  time_taken  \n",
       "1       0.998228     0.998918    18.58071  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = df_results.groupby(by=['no_of_features'])\n",
    "idx = g['test_score'].transform(max) == df_results['test_score']\n",
    "df_results[idx].sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:40.158509Z",
     "start_time": "2017-07-17T20:38:40.134521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999182</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.998918</td>\n",
       "      <td>18.580710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.927357</td>\n",
       "      <td>0.849283</td>\n",
       "      <td>0.902490</td>\n",
       "      <td>25.294052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  no_of_features  hidden_layers  train_score  test_score  f1_score  \\\n",
       "1     11               1              3          1.0    0.999068  0.999182   \n",
       "0     11               1              1          1.0    0.920777  0.927357   \n",
       "\n",
       "   test_score_20  f1_score_20  time_taken  \n",
       "1       0.998228     0.998918   18.580710  \n",
       "0       0.849283     0.902490   25.294052  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:40.192779Z",
     "start_time": "2017-07-17T20:38:40.160044Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_lstm_nsl_kdd_predictions.pkl\")\n",
    "pd.Panel(Train.predictions_).to_pickle(\"dataset/tf_lstm_nsl_kdd_predictions__.pkl\")\n",
    "\n",
    "df_results.to_pickle(\"dataset/tf_lstm_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:40.255095Z",
     "start_time": "2017-07-17T20:38:40.194401Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = False,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:40.617365Z",
     "start_time": "2017-07-17T20:38:40.256580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 9711     0]\n",
      " [    0 12833]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM8sdBzczM8ud/w/cOukuB/Y7FQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faeb2fc9278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:40.981571Z",
     "start_time": "2017-07-17T20:38:40.621553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[2152    0]\n",
      " [   0 9698]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAGgCAYAAAAtsfn1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae284b0a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value_, pred_value = Train.pred_value_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:41.023790Z",
     "start_time": "2017-07-17T20:38:40.983301Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4.5 GB\n",
    "pd.Series(Train.pred_value).to_csv('LSTM_prediction_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:41.049123Z",
     "start_time": "2017-07-17T20:38:41.025435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"32\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.926189</td>\n",
       "      <td>0.931865</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.908330</td>\n",
       "      <td>20.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.926322</td>\n",
       "      <td>0.931996</td>\n",
       "      <td>0.859831</td>\n",
       "      <td>0.908510</td>\n",
       "      <td>46.670596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.954079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.926189</td>\n",
       "      <td>0.931865</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.908330</td>\n",
       "      <td>20.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.926322</td>\n",
       "      <td>0.931996</td>\n",
       "      <td>0.859831</td>\n",
       "      <td>0.908510</td>\n",
       "      <td>46.670596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.954079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.924326</td>\n",
       "      <td>0.930299</td>\n",
       "      <td>0.856034</td>\n",
       "      <td>0.906295</td>\n",
       "      <td>28.694453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.931383</td>\n",
       "      <td>0.858650</td>\n",
       "      <td>0.907668</td>\n",
       "      <td>57.692709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984209</td>\n",
       "      <td>0.986319</td>\n",
       "      <td>0.969958</td>\n",
       "      <td>0.981977</td>\n",
       "      <td>35.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.907115</td>\n",
       "      <td>0.912757</td>\n",
       "      <td>0.823291</td>\n",
       "      <td>0.881908</td>\n",
       "      <td>29.174704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.999286</td>\n",
       "      <td>0.907248</td>\n",
       "      <td>0.912879</td>\n",
       "      <td>0.823544</td>\n",
       "      <td>0.882071</td>\n",
       "      <td>37.597928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.823690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.914878</td>\n",
       "      <td>0.922941</td>\n",
       "      <td>0.838059</td>\n",
       "      <td>0.897011</td>\n",
       "      <td>23.061939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999961</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>26.486510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.931423</td>\n",
       "      <td>0.936965</td>\n",
       "      <td>0.869536</td>\n",
       "      <td>0.915316</td>\n",
       "      <td>21.562394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998359</td>\n",
       "      <td>0.998560</td>\n",
       "      <td>0.996878</td>\n",
       "      <td>0.998096</td>\n",
       "      <td>68.964017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998669</td>\n",
       "      <td>0.998832</td>\n",
       "      <td>0.997468</td>\n",
       "      <td>0.998456</td>\n",
       "      <td>134.621442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.922729</td>\n",
       "      <td>0.929083</td>\n",
       "      <td>0.852996</td>\n",
       "      <td>0.904778</td>\n",
       "      <td>25.245101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.927475</td>\n",
       "      <td>0.933132</td>\n",
       "      <td>0.862025</td>\n",
       "      <td>0.910071</td>\n",
       "      <td>50.364210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978708</td>\n",
       "      <td>0.981642</td>\n",
       "      <td>0.959494</td>\n",
       "      <td>0.975850</td>\n",
       "      <td>28.628547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.913591</td>\n",
       "      <td>0.920053</td>\n",
       "      <td>0.835612</td>\n",
       "      <td>0.892352</td>\n",
       "      <td>29.535652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.922951</td>\n",
       "      <td>0.929053</td>\n",
       "      <td>0.853418</td>\n",
       "      <td>0.904629</td>\n",
       "      <td>57.127875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998980</td>\n",
       "      <td>0.999105</td>\n",
       "      <td>0.998059</td>\n",
       "      <td>0.998816</td>\n",
       "      <td>19.107379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928407</td>\n",
       "      <td>0.934036</td>\n",
       "      <td>0.863797</td>\n",
       "      <td>0.911309</td>\n",
       "      <td>27.814744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930314</td>\n",
       "      <td>0.935912</td>\n",
       "      <td>0.867426</td>\n",
       "      <td>0.913885</td>\n",
       "      <td>53.374960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.999922</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>66.155075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.901969</td>\n",
       "      <td>0.910250</td>\n",
       "      <td>0.813502</td>\n",
       "      <td>0.879590</td>\n",
       "      <td>15.067757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999468</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>0.998987</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>43.296997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999512</td>\n",
       "      <td>0.999571</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999433</td>\n",
       "      <td>79.905635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.912349</td>\n",
       "      <td>0.919864</td>\n",
       "      <td>0.833249</td>\n",
       "      <td>0.892539</td>\n",
       "      <td>26.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.913902</td>\n",
       "      <td>0.920843</td>\n",
       "      <td>0.836203</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>53.601731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999024</td>\n",
       "      <td>0.999144</td>\n",
       "      <td>0.998143</td>\n",
       "      <td>0.998867</td>\n",
       "      <td>12.872745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                             \n",
       "1              1                 11     0.999682    0.926189  0.931865   \n",
       "               1                 22     0.999682    0.926322  0.931996   \n",
       "               3                 11     1.000000    1.000000  1.000000   \n",
       "               1                 11     0.999682    0.926189  0.931865   \n",
       "               1                 22     0.999682    0.926322  0.931996   \n",
       "               3                 11     1.000000    1.000000  1.000000   \n",
       "               1                 11     0.999921    0.924326  0.930299   \n",
       "               1                 22     1.000000    0.925701  0.931383   \n",
       "               3                 11     1.000000    0.984209  0.986319   \n",
       "               1                 11     0.999921    0.907115  0.912757   \n",
       "               1                 22     0.999286    0.907248  0.912879   \n",
       "               3                 11     0.999841    1.000000  1.000000   \n",
       "               1                 11     0.999762    0.914878  0.922941   \n",
       "               3                 11     1.000000    0.999956  0.999961   \n",
       "               1                 11     0.999682    0.931423  0.936965   \n",
       "               3                 11     1.000000    0.998359  0.998560   \n",
       "               3                 22     1.000000    0.998669  0.998832   \n",
       "               1                 11     0.999762    0.922729  0.929083   \n",
       "               1                 22     0.999841    0.927475  0.933132   \n",
       "               3                 11     1.000000    0.978708  0.981642   \n",
       "               1                 11     0.999841    0.913591  0.920053   \n",
       "               1                 22     0.999841    0.922951  0.929053   \n",
       "               3                 11     1.000000    0.998980  0.999105   \n",
       "               1                 11     1.000000    0.928407  0.934036   \n",
       "               1                 22     1.000000    0.930314  0.935912   \n",
       "               3                 11     1.000000    0.999911  0.999922   \n",
       "               1                 11     0.999921    0.901969  0.910250   \n",
       "               3                 11     1.000000    0.999468  0.999532   \n",
       "               3                 22     1.000000    0.999512  0.999571   \n",
       "               1                 11     0.999921    0.912349  0.919864   \n",
       "               1                 22     0.999841    0.913902  0.920843   \n",
       "               3                 11     1.000000    0.999024  0.999144   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "1              1                   0.859578     0.908330   20.351100  \n",
       "               1                   0.859831     0.908510   46.670596  \n",
       "               3                   1.000000     1.000000   24.954079  \n",
       "               1                   0.859578     0.908330   20.351100  \n",
       "               1                   0.859831     0.908510   46.670596  \n",
       "               3                   1.000000     1.000000   24.954079  \n",
       "               1                   0.856034     0.906295   28.694453  \n",
       "               1                   0.858650     0.907668   57.692709  \n",
       "               3                   0.969958     0.981977   35.610800  \n",
       "               1                   0.823291     0.881908   29.174704  \n",
       "               1                   0.823544     0.882071   37.597928  \n",
       "               3                   1.000000     1.000000   14.823690  \n",
       "               1                   0.838059     0.897011   23.061939  \n",
       "               3                   0.999916     0.999948   26.486510  \n",
       "               1                   0.869536     0.915316   21.562394  \n",
       "               3                   0.996878     0.998096   68.964017  \n",
       "               3                   0.997468     0.998456  134.621442  \n",
       "               1                   0.852996     0.904778   25.245101  \n",
       "               1                   0.862025     0.910071   50.364210  \n",
       "               3                   0.959494     0.975850   28.628547  \n",
       "               1                   0.835612     0.892352   29.535652  \n",
       "               1                   0.853418     0.904629   57.127875  \n",
       "               3                   0.998059     0.998816   19.107379  \n",
       "               1                   0.863797     0.911309   27.814744  \n",
       "               1                   0.867426     0.913885   53.374960  \n",
       "               3                   0.999831     0.999897   66.155075  \n",
       "               1                   0.813502     0.879590   15.067757  \n",
       "               3                   0.998987     0.999381   43.296997  \n",
       "               3                   0.999072     0.999433   79.905635  \n",
       "               1                   0.833249     0.892539   26.791100  \n",
       "               1                   0.836203     0.893650   53.601731  \n",
       "               3                   0.998143     0.998867   12.872745  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:41.071591Z",
     "start_time": "2017-07-17T20:38:41.051412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>15.631579</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.920495</td>\n",
       "      <td>0.926693</td>\n",
       "      <td>0.848745</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>35.302666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.692308</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.996677</td>\n",
       "      <td>0.997122</td>\n",
       "      <td>0.993677</td>\n",
       "      <td>0.996209</td>\n",
       "      <td>44.644692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                 \n",
       "1              1              15.631579     0.999804    0.920495  0.926693   \n",
       "               3              12.692308     0.999988    0.996677  0.997122   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "1              1                   0.848745     0.901408   35.302666  \n",
       "               3                   0.993677     0.996209   44.644692  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgb = past_scores.groupby(by=['no_of_features', 'hidden_layers'])\n",
    "pgb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T20:38:41.089454Z",
     "start_time": "2017-07-17T20:38:41.073480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>test_score_20</th>\n",
       "      <th>f1_score_20</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>5.579830</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.008817</td>\n",
       "      <td>0.008288</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>0.011240</td>\n",
       "      <td>14.277070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.130872</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>0.013067</td>\n",
       "      <td>0.007803</td>\n",
       "      <td>34.628124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 epoch  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                \n",
       "1              1              5.579830     0.000170    0.008817  0.008288   \n",
       "               3              4.130872     0.000044    0.006868  0.005930   \n",
       "\n",
       "                              test_score_20  f1_score_20  time_taken  \n",
       "no_of_features hidden_layers                                          \n",
       "1              1                   0.016775     0.011240   14.277070  \n",
       "               3                   0.013067     0.007803   34.628124  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgb.std()"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
