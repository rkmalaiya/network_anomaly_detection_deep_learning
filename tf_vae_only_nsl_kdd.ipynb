{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:32.680918Z",
     "start_time": "2017-05-14T20:30:32.183460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:32.776336Z",
     "start_time": "2017-05-14T20:30:32.682867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:32.783996Z",
     "start_time": "2017-05-14T20:30:32.778402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:32.789950Z",
     "start_time": "2017-05-14T20:30:32.785686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:33.774117Z",
     "start_time": "2017-05-14T20:30:32.791678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99186991653217393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:35.320543Z",
     "start_time": "2017-05-14T20:30:33.776266Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:35.871147Z",
     "start_time": "2017-05-14T20:30:35.322723Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            self.regularized_loss = tf.losses.mean_squared_error(self.x, self.x_hat) #tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "            loss = tf.where(tf.is_nan(self.regularized_loss), 1e-2, self.regularized_loss)\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:30:36.043765Z",
     "start_time": "2017-05-14T20:30:35.873374Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            Train.best_acc = 0\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    #if(train_loss > 1e9):\n",
    "                    \n",
    "                    #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "                    \n",
    "                #print(\"\")\n",
    "                valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                \n",
    "                accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                               net.pred, \n",
    "                                                               net.actual, net.y], \n",
    "                                                              feed_dict={net.x: preprocess.x_test, \n",
    "                                                                         net.y_: preprocess.y_test, \n",
    "                                                                         net.keep_prob:1})\n",
    "                #print(\"*************** \\n\")\n",
    "                print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:6f} | Test Accuracy: {:.6f}\".format(epoch, train_loss, test_loss, accuracy))\n",
    "                #print(\"*************** \\n\")\n",
    "                #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "                \n",
    "                if accuracy > Train.best_acc:\n",
    "                    Train.best_acc = accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                    net.saver.save(sess, \"dataset/tf_vae_only_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                    Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                    curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                    Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:53:34.834692Z",
     "start_time": "2017-05-14T20:30:36.046154Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.252819 | Test Loss: 1.335494 | Test Accuracy: 0.837340\n",
      "Step 2 | Training Loss: 0.246081 | Test Loss: 1.242872 | Test Accuracy: 0.804560\n",
      "Step 3 | Training Loss: 0.426666 | Test Loss: 1.170087 | Test Accuracy: 0.773510\n",
      "Step 4 | Training Loss: 2.070724 | Test Loss: 1.197806 | Test Accuracy: 0.798572\n",
      "Step 5 | Training Loss: 0.199238 | Test Loss: 1.170431 | Test Accuracy: 0.772622\n",
      "Step 6 | Training Loss: 0.284595 | Test Loss: 18363899904.000000 | Test Accuracy: 0.794225\n",
      "Step 7 | Training Loss: 0.358729 | Test Loss: 1.952253 | Test Accuracy: 0.790942\n",
      "Step 8 | Training Loss: 0.283792 | Test Loss: 1.830037 | Test Accuracy: 0.746939\n",
      "Step 9 | Training Loss: 0.481545 | Test Loss: 1.499974 | Test Accuracy: 0.760202\n",
      "Step 10 | Training Loss: 0.217502 | Test Loss: 1.522937 | Test Accuracy: 0.780429\n",
      "Step 11 | Training Loss: 0.532311 | Test Loss: 1.746535 | Test Accuracy: 0.662172\n",
      "Step 12 | Training Loss: 0.374790 | Test Loss: 1.314662 | Test Accuracy: 0.721655\n",
      "Step 13 | Training Loss: 0.273709 | Test Loss: 5.293544 | Test Accuracy: 0.752883\n",
      "Step 14 | Training Loss: 0.196119 | Test Loss: 6.968343 | Test Accuracy: 0.793337\n",
      "Step 15 | Training Loss: 0.216454 | Test Loss: 10.309164 | Test Accuracy: 0.742459\n",
      "Step 16 | Training Loss: 0.210719 | Test Loss: 1.250629 | Test Accuracy: 0.735318\n",
      "Step 17 | Training Loss: 0.247411 | Test Loss: 1.490890 | Test Accuracy: 0.743213\n",
      "Step 18 | Training Loss: 0.248127 | Test Loss: 1.365487 | Test Accuracy: 0.710610\n",
      "Step 19 | Training Loss: 0.156046 | Test Loss: 1.385780 | Test Accuracy: 0.751730\n",
      "Step 20 | Training Loss: 0.197710 | Test Loss: 1.377347 | Test Accuracy: 0.716776\n",
      "Step 21 | Training Loss: 0.209274 | Test Loss: 1.394372 | Test Accuracy: 0.737048\n",
      "Step 22 | Training Loss: 0.399665 | Test Loss: 1.592463 | Test Accuracy: 0.801544\n",
      "Step 23 | Training Loss: 0.213384 | Test Loss: 1.586436 | Test Accuracy: 0.787615\n",
      "Step 24 | Training Loss: 0.187003 | Test Loss: 1.535314 | Test Accuracy: 0.795866\n",
      "Step 25 | Training Loss: 0.524350 | Test Loss: 1.543986 | Test Accuracy: 0.776348\n",
      "Step 26 | Training Loss: 0.566066 | Test Loss: 1.557231 | Test Accuracy: 0.849051\n",
      "Step 27 | Training Loss: 0.501525 | Test Loss: 1.553038 | Test Accuracy: 0.834634\n",
      "Step 28 | Training Loss: 0.239588 | Test Loss: 1.617468 | Test Accuracy: 0.829755\n",
      "Step 29 | Training Loss: 0.535358 | Test Loss: 1.581018 | Test Accuracy: 0.829933\n",
      "Step 30 | Training Loss: 0.511091 | Test Loss: 1.574567 | Test Accuracy: 0.816581\n",
      "Step 31 | Training Loss: 0.233605 | Test Loss: 1.537031 | Test Accuracy: 0.797685\n",
      "Step 32 | Training Loss: 1.061653 | Test Loss: 1.536883 | Test Accuracy: 0.810504\n",
      "Step 33 | Training Loss: 0.411752 | Test Loss: 1.624004 | Test Accuracy: 0.784909\n",
      "Step 34 | Training Loss: 0.205201 | Test Loss: 1.833582 | Test Accuracy: 0.822835\n",
      "Step 35 | Training Loss: 0.300347 | Test Loss: 1.866467 | Test Accuracy: 0.800923\n",
      "Step 36 | Training Loss: 0.180560 | Test Loss: 1.998280 | Test Accuracy: 0.805225\n",
      "Step 37 | Training Loss: 0.216364 | Test Loss: 2.172945 | Test Accuracy: 0.793160\n",
      "Step 38 | Training Loss: 0.317977 | Test Loss: 2.026672 | Test Accuracy: 0.782559\n",
      "Step 39 | Training Loss: 0.448981 | Test Loss: 2.031461 | Test Accuracy: 0.809839\n",
      "Step 40 | Training Loss: 0.426999 | Test Loss: 2.391990 | Test Accuracy: 0.815649\n",
      "Step 41 | Training Loss: 0.470257 | Test Loss: 2.498164 | Test Accuracy: 0.760690\n",
      "Step 42 | Training Loss: 0.260953 | Test Loss: 2.352479 | Test Accuracy: 0.770804\n",
      "Step 43 | Training Loss: 0.299598 | Test Loss: 2.401186 | Test Accuracy: 0.762331\n",
      "Step 44 | Training Loss: 0.354380 | Test Loss: 2.581705 | Test Accuracy: 0.747782\n",
      "Step 45 | Training Loss: 0.211394 | Test Loss: 2.466474 | Test Accuracy: 0.765215\n",
      "Step 46 | Training Loss: 0.509491 | Test Loss: 2.760232 | Test Accuracy: 0.766102\n",
      "Step 47 | Training Loss: 0.480579 | Test Loss: 2.722065 | Test Accuracy: 0.742016\n",
      "Step 48 | Training Loss: 0.517502 | Test Loss: 2.806897 | Test Accuracy: 0.743568\n",
      "Step 49 | Training Loss: 0.514117 | Test Loss: 2.788143 | Test Accuracy: 0.746718\n",
      "Step 50 | Training Loss: 0.247744 | Test Loss: 2.654163 | Test Accuracy: 0.757984\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.647651 | Test Loss: 1.529686 | Test Accuracy: 0.787748\n",
      "Step 2 | Training Loss: 0.248376 | Test Loss: 1.422813 | Test Accuracy: 0.775639\n",
      "Step 3 | Training Loss: 0.358308 | Test Loss: 1.668413 | Test Accuracy: 0.765614\n",
      "Step 4 | Training Loss: 1.236508 | Test Loss: 1.578561 | Test Accuracy: 0.773510\n",
      "Step 5 | Training Loss: 0.241703 | Test Loss: 1.506425 | Test Accuracy: 0.635956\n",
      "Step 6 | Training Loss: 0.298728 | Test Loss: 1.386971 | Test Accuracy: 0.650018\n",
      "Step 7 | Training Loss: 0.216156 | Test Loss: 1.415820 | Test Accuracy: 0.617237\n",
      "Step 8 | Training Loss: 0.184646 | Test Loss: 1.367412 | Test Accuracy: 0.691137\n",
      "Step 9 | Training Loss: 0.211003 | Test Loss: 1.363419 | Test Accuracy: 0.620165\n",
      "Step 10 | Training Loss: 0.199876 | Test Loss: 1.382139 | Test Accuracy: 0.630101\n",
      "Step 11 | Training Loss: 0.240775 | Test Loss: 1.388604 | Test Accuracy: 0.637331\n",
      "Step 12 | Training Loss: 0.402070 | Test Loss: 1.383727 | Test Accuracy: 0.669092\n",
      "Step 13 | Training Loss: 0.391308 | Test Loss: 1.389035 | Test Accuracy: 0.694420\n",
      "Step 14 | Training Loss: 0.104690 | Test Loss: 1.377457 | Test Accuracy: 0.704888\n",
      "Step 15 | Training Loss: 0.181146 | Test Loss: 1.352694 | Test Accuracy: 0.709679\n",
      "Step 16 | Training Loss: 0.067046 | Test Loss: 1.432834 | Test Accuracy: 0.780829\n",
      "Step 17 | Training Loss: 1.097275 | Test Loss: 1.456862 | Test Accuracy: 0.706175\n",
      "Step 18 | Training Loss: 0.148366 | Test Loss: 1.459397 | Test Accuracy: 0.718772\n",
      "Step 19 | Training Loss: 0.153447 | Test Loss: 1.448164 | Test Accuracy: 0.727466\n",
      "Step 20 | Training Loss: 0.106725 | Test Loss: 1.497037 | Test Accuracy: 0.744633\n",
      "Step 21 | Training Loss: 0.224457 | Test Loss: 1.467469 | Test Accuracy: 0.726313\n",
      "Step 22 | Training Loss: 0.289618 | Test Loss: 1.479874 | Test Accuracy: 0.731902\n",
      "Step 23 | Training Loss: 0.180791 | Test Loss: 1.447267 | Test Accuracy: 0.789390\n",
      "Step 24 | Training Loss: 0.146243 | Test Loss: 1.458986 | Test Accuracy: 0.805358\n",
      "Step 25 | Training Loss: 0.344171 | Test Loss: 1.535240 | Test Accuracy: 0.760158\n",
      "Step 26 | Training Loss: 0.327219 | Test Loss: 1.493559 | Test Accuracy: 0.739753\n",
      "Step 27 | Training Loss: 0.207175 | Test Loss: 1.511909 | Test Accuracy: 0.740907\n",
      "Step 28 | Training Loss: 0.160259 | Test Loss: 1.489172 | Test Accuracy: 0.753903\n",
      "Step 29 | Training Loss: 0.206405 | Test Loss: 1.498599 | Test Accuracy: 0.777502\n",
      "Step 30 | Training Loss: 0.242934 | Test Loss: 1.486905 | Test Accuracy: 0.736116\n",
      "Step 31 | Training Loss: 0.175722 | Test Loss: 1.471128 | Test Accuracy: 0.741572\n",
      "Step 32 | Training Loss: 0.161955 | Test Loss: 1.472594 | Test Accuracy: 0.750310\n",
      "Step 33 | Training Loss: 0.108770 | Test Loss: 1.515580 | Test Accuracy: 0.791253\n",
      "Step 34 | Training Loss: 0.238078 | Test Loss: 1.499136 | Test Accuracy: 0.757674\n",
      "Step 35 | Training Loss: 0.168431 | Test Loss: 1.501354 | Test Accuracy: 0.746274\n",
      "Step 36 | Training Loss: 0.112578 | Test Loss: 1.502690 | Test Accuracy: 0.748847\n",
      "Step 37 | Training Loss: 0.102216 | Test Loss: 1.491062 | Test Accuracy: 0.778167\n",
      "Step 38 | Training Loss: 0.142476 | Test Loss: 1.488453 | Test Accuracy: 0.757097\n",
      "Step 39 | Training Loss: 0.181299 | Test Loss: 1.472673 | Test Accuracy: 0.776348\n",
      "Step 40 | Training Loss: 0.116796 | Test Loss: 1.475287 | Test Accuracy: 0.759626\n",
      "Step 41 | Training Loss: 0.238848 | Test Loss: 1.475744 | Test Accuracy: 0.757896\n",
      "Step 42 | Training Loss: 0.336220 | Test Loss: 1.482867 | Test Accuracy: 0.759936\n",
      "Step 43 | Training Loss: 0.080687 | Test Loss: 1.485196 | Test Accuracy: 0.776615\n",
      "Step 44 | Training Loss: 0.149157 | Test Loss: 1.502131 | Test Accuracy: 0.740153\n",
      "Step 45 | Training Loss: 0.353078 | Test Loss: 1.480294 | Test Accuracy: 0.759892\n",
      "Step 46 | Training Loss: 0.154095 | Test Loss: 1.491922 | Test Accuracy: 0.765392\n",
      "Step 47 | Training Loss: 0.634989 | Test Loss: 1.735566 | Test Accuracy: 0.787748\n",
      "Step 48 | Training Loss: 0.409881 | Test Loss: 1.701410 | Test Accuracy: 0.796620\n",
      "Step 49 | Training Loss: 0.240674 | Test Loss: 1.699955 | Test Accuracy: 0.807709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 | Training Loss: 0.298219 | Test Loss: 1.667264 | Test Accuracy: 0.823057\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.640810 | Test Loss: 1.818417 | Test Accuracy: 0.810016\n",
      "Step 2 | Training Loss: 0.656751 | Test Loss: 1.769570 | Test Accuracy: 0.768852\n",
      "Step 3 | Training Loss: 0.514243 | Test Loss: 1.741167 | Test Accuracy: 0.760468\n",
      "Step 4 | Training Loss: 0.805485 | Test Loss: 1.959147 | Test Accuracy: 0.430758\n",
      "Step 5 | Training Loss: 1.121098 | Test Loss: 1.954783 | Test Accuracy: 0.430758\n",
      "Step 6 | Training Loss: 2.457857 | Test Loss: 1.946828 | Test Accuracy: 0.863999\n",
      "Step 7 | Training Loss: 0.891402 | Test Loss: 1.941946 | Test Accuracy: 0.848119\n",
      "Step 8 | Training Loss: 0.630574 | Test Loss: 1.928772 | Test Accuracy: 0.832771\n",
      "Step 9 | Training Loss: 0.721098 | Test Loss: 1.913166 | Test Accuracy: 0.779143\n",
      "Step 10 | Training Loss: 0.733707 | Test Loss: 1.909401 | Test Accuracy: 0.723785\n",
      "Step 11 | Training Loss: 0.885505 | Test Loss: 1.907975 | Test Accuracy: 0.743701\n",
      "Step 12 | Training Loss: 1.115267 | Test Loss: 1.905632 | Test Accuracy: 0.748314\n",
      "Step 13 | Training Loss: 0.688883 | Test Loss: 1.909874 | Test Accuracy: 0.759670\n",
      "Step 14 | Training Loss: 0.665739 | Test Loss: 1.909490 | Test Accuracy: 0.751863\n",
      "Step 15 | Training Loss: 0.842425 | Test Loss: 1.905099 | Test Accuracy: 0.723031\n",
      "Step 16 | Training Loss: 0.814272 | Test Loss: 1.915001 | Test Accuracy: 0.770183\n",
      "Step 17 | Training Loss: 0.778283 | Test Loss: 1.907061 | Test Accuracy: 0.728753\n",
      "Step 18 | Training Loss: 0.719736 | Test Loss: 1.942060 | Test Accuracy: 0.833836\n",
      "Step 19 | Training Loss: 0.971095 | Test Loss: 1.907224 | Test Accuracy: 0.739088\n",
      "Step 20 | Training Loss: 0.647822 | Test Loss: 1.911593 | Test Accuracy: 0.744633\n",
      "Step 21 | Training Loss: 1.093544 | Test Loss: 1.911131 | Test Accuracy: 0.738511\n",
      "Step 22 | Training Loss: 0.983210 | Test Loss: 1.909795 | Test Accuracy: 0.735362\n",
      "Step 23 | Training Loss: 0.683788 | Test Loss: 1.911661 | Test Accuracy: 0.755633\n",
      "Step 24 | Training Loss: 0.728444 | Test Loss: 1.941198 | Test Accuracy: 0.820484\n",
      "Step 25 | Training Loss: 2.492656 | Test Loss: 1.930437 | Test Accuracy: 0.842575\n",
      "Step 26 | Training Loss: 0.772851 | Test Loss: 1.924284 | Test Accuracy: 0.839957\n",
      "Step 27 | Training Loss: 0.740908 | Test Loss: 1.923701 | Test Accuracy: 0.833659\n",
      "Step 28 | Training Loss: 1.286863 | Test Loss: 1.922661 | Test Accuracy: 0.829356\n",
      "Step 29 | Training Loss: 0.618859 | Test Loss: 1.921760 | Test Accuracy: 0.824166\n",
      "Step 30 | Training Loss: 1.061262 | Test Loss: 1.920233 | Test Accuracy: 0.814230\n",
      "Step 31 | Training Loss: 0.792722 | Test Loss: 1.919887 | Test Accuracy: 0.809484\n",
      "Step 32 | Training Loss: 0.803908 | Test Loss: 1.921432 | Test Accuracy: 0.815161\n",
      "Step 33 | Training Loss: 0.756759 | Test Loss: 1.923595 | Test Accuracy: 0.811214\n",
      "Step 34 | Training Loss: 0.719641 | Test Loss: 1.919446 | Test Accuracy: 0.804516\n",
      "Step 35 | Training Loss: 0.810954 | Test Loss: 1.923576 | Test Accuracy: 0.802253\n",
      "Step 36 | Training Loss: 0.644954 | Test Loss: 1.848719 | Test Accuracy: 0.768098\n",
      "Step 37 | Training Loss: 2.195459 | Test Loss: 1.840726 | Test Accuracy: 0.773421\n",
      "Step 38 | Training Loss: 0.612513 | Test Loss: 1.825127 | Test Accuracy: 0.776836\n",
      "Step 39 | Training Loss: 0.661012 | Test Loss: 1.815067 | Test Accuracy: 0.770582\n",
      "Step 40 | Training Loss: 0.608252 | Test Loss: 1.808942 | Test Accuracy: 0.755101\n",
      "Step 41 | Training Loss: 0.521066 | Test Loss: 1.810652 | Test Accuracy: 0.764771\n",
      "Step 42 | Training Loss: 0.595066 | Test Loss: 1.808961 | Test Accuracy: 0.772356\n",
      "Step 43 | Training Loss: 0.933301 | Test Loss: 1.810373 | Test Accuracy: 0.771868\n",
      "Step 44 | Training Loss: 0.564240 | Test Loss: 1.809608 | Test Accuracy: 0.778566\n",
      "Step 45 | Training Loss: 0.593370 | Test Loss: 1.856792 | Test Accuracy: 0.764549\n",
      "Step 46 | Training Loss: 0.889166 | Test Loss: 1.811478 | Test Accuracy: 0.776215\n",
      "Step 47 | Training Loss: 0.626948 | Test Loss: 1.801342 | Test Accuracy: 0.764372\n",
      "Step 48 | Training Loss: 0.610604 | Test Loss: 1.805226 | Test Accuracy: 0.775284\n",
      "Step 49 | Training Loss: 0.724489 | Test Loss: 1.811515 | Test Accuracy: 0.775816\n",
      "Step 50 | Training Loss: 0.577510 | Test Loss: 1.800648 | Test Accuracy: 0.794446\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.480526 | Test Loss: 1.522807 | Test Accuracy: 0.772756\n",
      "Step 2 | Training Loss: 0.519968 | Test Loss: 1.494273 | Test Accuracy: 0.782248\n",
      "Step 3 | Training Loss: 0.311777 | Test Loss: 1.892719 | Test Accuracy: 0.778744\n",
      "Step 4 | Training Loss: 0.320148 | Test Loss: 1.373193 | Test Accuracy: 0.790632\n",
      "Step 5 | Training Loss: 0.241560 | Test Loss: 1.365514 | Test Accuracy: 0.791075\n",
      "Step 6 | Training Loss: 0.532209 | Test Loss: 1.450513 | Test Accuracy: 0.783357\n",
      "Step 7 | Training Loss: 0.518208 | Test Loss: 1.373240 | Test Accuracy: 0.766146\n",
      "Step 8 | Training Loss: 0.680586 | Test Loss: 1.465004 | Test Accuracy: 0.782958\n",
      "Step 9 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 10 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 11 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 12 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 13 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 14 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 15 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 16 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 18 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 21 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 22 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 23 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 24 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 25 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 26 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 27 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 28 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 29 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 30 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 31 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 32 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 33 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 34 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 35 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 36 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 37 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 38 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 39 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.439850 | Test Loss: 1.685965 | Test Accuracy: 0.800656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 | Training Loss: 0.575972 | Test Loss: 1.606207 | Test Accuracy: 0.791829\n",
      "Step 3 | Training Loss: 0.475631 | Test Loss: 1.496930 | Test Accuracy: 0.783579\n",
      "Step 4 | Training Loss: 0.251908 | Test Loss: 1.451527 | Test Accuracy: 0.773731\n",
      "Step 5 | Training Loss: 0.191188 | Test Loss: 1.435974 | Test Accuracy: 0.771469\n",
      "Step 6 | Training Loss: 0.353266 | Test Loss: 1.433434 | Test Accuracy: 0.770538\n",
      "Step 7 | Training Loss: 0.321228 | Test Loss: 1.485521 | Test Accuracy: 0.832638\n",
      "Step 8 | Training Loss: 0.206283 | Test Loss: 2.024056 | Test Accuracy: 0.793692\n",
      "Step 9 | Training Loss: 0.774410 | Test Loss: 1.471002 | Test Accuracy: 0.775994\n",
      "Step 10 | Training Loss: 0.342147 | Test Loss: 1.488425 | Test Accuracy: 0.768054\n",
      "Step 11 | Training Loss: 0.618674 | Test Loss: 1.682157 | Test Accuracy: 0.783623\n",
      "Step 12 | Training Loss: 0.224216 | Test Loss: 1.642806 | Test Accuracy: 0.779542\n",
      "Step 13 | Training Loss: 0.467796 | Test Loss: 1.769953 | Test Accuracy: 0.778211\n",
      "Step 14 | Training Loss: 0.516571 | Test Loss: 1.780505 | Test Accuracy: 0.772179\n",
      "Step 15 | Training Loss: 0.421892 | Test Loss: 1.742107 | Test Accuracy: 0.839780\n",
      "Step 16 | Training Loss: 0.361383 | Test Loss: 1.771219 | Test Accuracy: 0.801011\n",
      "Step 17 | Training Loss: 0.322532 | Test Loss: 1.716613 | Test Accuracy: 0.814141\n",
      "Step 18 | Training Loss: 0.413314 | Test Loss: 1.745873 | Test Accuracy: 0.845103\n",
      "Step 19 | Training Loss: 0.392832 | Test Loss: 1.759104 | Test Accuracy: 0.838494\n",
      "Step 20 | Training Loss: 0.419309 | Test Loss: 1.721176 | Test Accuracy: 0.862225\n",
      "Step 21 | Training Loss: 2.040164 | Test Loss: 1.768887 | Test Accuracy: 0.848119\n",
      "Step 22 | Training Loss: 0.539628 | Test Loss: 1407414430787507847168.000000 | Test Accuracy: 0.807976\n",
      "Step 23 | Training Loss: 0.653970 | Test Loss: 1.768582 | Test Accuracy: 0.814230\n",
      "Step 24 | Training Loss: 0.671783 | Test Loss: 1.864697 | Test Accuracy: 0.821372\n",
      "Step 25 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 26 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 27 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 28 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 29 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 30 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 31 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 32 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 33 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 34 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 35 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 36 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 37 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 38 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 39 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.687609 | Test Loss: 1.712515 | Test Accuracy: 0.775106\n",
      "Step 2 | Training Loss: 0.536711 | Test Loss: 1.642681 | Test Accuracy: 0.796088\n",
      "Step 3 | Training Loss: 0.479563 | Test Loss: 1.785131 | Test Accuracy: 0.765703\n",
      "Step 4 | Training Loss: 2.114659 | Test Loss: 1.555757 | Test Accuracy: 0.775106\n",
      "Step 5 | Training Loss: 0.214237 | Test Loss: 1.500784 | Test Accuracy: 0.768852\n",
      "Step 6 | Training Loss: 0.375449 | Test Loss: 1.520641 | Test Accuracy: 0.760025\n",
      "Step 7 | Training Loss: 0.182978 | Test Loss: 1.473938 | Test Accuracy: 0.774796\n",
      "Step 8 | Training Loss: 0.302228 | Test Loss: 1.440969 | Test Accuracy: 0.759005\n",
      "Step 9 | Training Loss: 0.454837 | Test Loss: 1.553156 | Test Accuracy: 0.793559\n",
      "Step 10 | Training Loss: 0.274933 | Test Loss: 1.535198 | Test Accuracy: 0.800390\n",
      "Step 11 | Training Loss: 0.207277 | Test Loss: 1.507606 | Test Accuracy: 0.769251\n",
      "Step 12 | Training Loss: 0.256847 | Test Loss: 1.521974 | Test Accuracy: 0.800213\n",
      "Step 13 | Training Loss: 0.357441 | Test Loss: 1.714970 | Test Accuracy: 0.835832\n",
      "Step 14 | Training Loss: 1.270264 | Test Loss: 1.744204 | Test Accuracy: 0.793870\n",
      "Step 15 | Training Loss: 0.534095 | Test Loss: 1.747715 | Test Accuracy: 0.820440\n",
      "Step 16 | Training Loss: 0.687281 | Test Loss: 1.722548 | Test Accuracy: 0.841776\n",
      "Step 17 | Training Loss: 0.507813 | Test Loss: 1.785764 | Test Accuracy: 0.831086\n",
      "Step 18 | Training Loss: 0.476218 | Test Loss: 1.763784 | Test Accuracy: 0.814452\n",
      "Step 19 | Training Loss: 0.733416 | Test Loss: 1.882557 | Test Accuracy: 0.748447\n",
      "Step 20 | Training Loss: 0.713452 | Test Loss: 1.900469 | Test Accuracy: 0.715667\n",
      "Step 21 | Training Loss: 0.534483 | Test Loss: 1.876076 | Test Accuracy: 0.738999\n",
      "Step 22 | Training Loss: 1.256088 | Test Loss: 1.852168 | Test Accuracy: 0.791696\n",
      "Step 23 | Training Loss: 0.683620 | Test Loss: 1.861421 | Test Accuracy: 0.781228\n",
      "Step 24 | Training Loss: 0.707110 | Test Loss: 1.844667 | Test Accuracy: 0.801011\n",
      "Step 25 | Training Loss: 0.607645 | Test Loss: 1.826169 | Test Accuracy: 0.788857\n",
      "Step 26 | Training Loss: 0.648606 | Test Loss: 1.808926 | Test Accuracy: 0.807443\n",
      "Step 27 | Training Loss: 1.947586 | Test Loss: 1.822314 | Test Accuracy: 0.781538\n",
      "Step 28 | Training Loss: 0.545690 | Test Loss: 1.819891 | Test Accuracy: 0.792229\n",
      "Step 29 | Training Loss: 0.855211 | Test Loss: 1.795151 | Test Accuracy: 0.859164\n",
      "Step 30 | Training Loss: 0.780835 | Test Loss: 1.789843 | Test Accuracy: 0.870076\n",
      "Step 31 | Training Loss: 0.736644 | Test Loss: 1.896767 | Test Accuracy: 0.803407\n",
      "Step 32 | Training Loss: 0.778576 | Test Loss: 1.979521 | Test Accuracy: 0.580997\n",
      "Step 33 | Training Loss: 0.786332 | Test Loss: 1.898422 | Test Accuracy: 0.564452\n",
      "Step 34 | Training Loss: 0.688478 | Test Loss: 1.868310 | Test Accuracy: 0.575896\n",
      "Step 35 | Training Loss: 0.611908 | Test Loss: 1.869251 | Test Accuracy: 0.608809\n",
      "Step 36 | Training Loss: 0.780369 | Test Loss: 1.837425 | Test Accuracy: 0.843151\n",
      "Step 37 | Training Loss: 0.629807 | Test Loss: 1.840902 | Test Accuracy: 0.849982\n",
      "Step 38 | Training Loss: 0.712064 | Test Loss: 1.842965 | Test Accuracy: 0.838538\n",
      "Step 39 | Training Loss: 0.995650 | Test Loss: 1.987934 | Test Accuracy: 0.799991\n",
      "Step 40 | Training Loss: 0.990920 | Test Loss: 1.979651 | Test Accuracy: 0.804383\n",
      "Step 41 | Training Loss: 0.859670 | Test Loss: 1.938985 | Test Accuracy: 0.848652\n",
      "Step 42 | Training Loss: 0.750287 | Test Loss: 1.953056 | Test Accuracy: 0.777945\n",
      "Step 43 | Training Loss: 1.153443 | Test Loss: 1.937406 | Test Accuracy: 0.831086\n",
      "Step 44 | Training Loss: 0.823253 | Test Loss: 1.939234 | Test Accuracy: 0.823368\n",
      "Step 45 | Training Loss: 0.865236 | Test Loss: 1.935105 | Test Accuracy: 0.850648\n",
      "Step 46 | Training Loss: 0.828616 | Test Loss: 1.934091 | Test Accuracy: 0.851047\n",
      "Step 47 | Training Loss: 0.814777 | Test Loss: 1.945720 | Test Accuracy: 0.811746\n",
      "Step 48 | Training Loss: 0.936246 | Test Loss: 1.946123 | Test Accuracy: 0.807931\n",
      "Step 49 | Training Loss: 0.929456 | Test Loss: 1.944845 | Test Accuracy: 0.810149\n",
      "Step 50 | Training Loss: 0.925657 | Test Loss: 1.949200 | Test Accuracy: 0.797862\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.446053 | Test Loss: 1.687583 | Test Accuracy: 0.807399\n",
      "Step 2 | Training Loss: 0.314846 | Test Loss: 1.531240 | Test Accuracy: 0.788591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 | Training Loss: 1.220841 | Test Loss: 1.673403 | Test Accuracy: 0.807443\n",
      "Step 4 | Training Loss: 0.503419 | Test Loss: 1.592077 | Test Accuracy: 0.783490\n",
      "Step 5 | Training Loss: 0.388768 | Test Loss: 1.698916 | Test Accuracy: 0.802032\n",
      "Step 6 | Training Loss: 0.375373 | Test Loss: 2.612799 | Test Accuracy: 0.735096\n",
      "Step 7 | Training Loss: 0.362716 | Test Loss: 1.649842 | Test Accuracy: 0.772977\n",
      "Step 8 | Training Loss: 0.339815 | Test Loss: 1.615099 | Test Accuracy: 0.737935\n",
      "Step 9 | Training Loss: 1.305730 | Test Loss: 1.556346 | Test Accuracy: 0.754924\n",
      "Step 10 | Training Loss: 0.882117 | Test Loss: 1.540031 | Test Accuracy: 0.781538\n",
      "Step 11 | Training Loss: 0.501514 | Test Loss: 1.636748 | Test Accuracy: 0.691137\n",
      "Step 12 | Training Loss: 0.538716 | Test Loss: 1.594422 | Test Accuracy: 0.773643\n",
      "Step 13 | Training Loss: 0.639882 | Test Loss: 1.797948 | Test Accuracy: 0.810060\n",
      "Step 14 | Training Loss: 0.653053 | Test Loss: 1.636152 | Test Accuracy: 0.731059\n",
      "Step 15 | Training Loss: 0.496802 | Test Loss: 1.632208 | Test Accuracy: 0.751774\n",
      "Step 16 | Training Loss: 1.100016 | Test Loss: 1.601534 | Test Accuracy: 0.773953\n",
      "Step 17 | Training Loss: 0.452337 | Test Loss: 1.591328 | Test Accuracy: 0.782381\n",
      "Step 18 | Training Loss: 0.577206 | Test Loss: 1.597049 | Test Accuracy: 0.797951\n",
      "Step 19 | Training Loss: 0.512591 | Test Loss: 1.586684 | Test Accuracy: 0.796309\n",
      "Step 20 | Training Loss: 0.509726 | Test Loss: 1.646955 | Test Accuracy: 0.854684\n",
      "Step 21 | Training Loss: 0.587419 | Test Loss: 1.603877 | Test Accuracy: 0.770493\n",
      "Step 22 | Training Loss: 0.515602 | Test Loss: 2454558354092931143434240.000000 | Test Accuracy: 0.740197\n",
      "Step 23 | Training Loss: 0.936060 | Test Loss: 1.971633 | Test Accuracy: 0.778877\n",
      "Step 24 | Training Loss: 0.758063 | Test Loss: 1.946549 | Test Accuracy: 0.831529\n",
      "Step 25 | Training Loss: 0.895476 | Test Loss: 1.943194 | Test Accuracy: 0.856414\n",
      "Step 26 | Training Loss: 1.183485 | Test Loss: 1.940610 | Test Accuracy: 0.856015\n",
      "Step 27 | Training Loss: 0.848206 | Test Loss: 1.944261 | Test Accuracy: 0.854640\n",
      "Step 28 | Training Loss: 1.054506 | Test Loss: 1.943853 | Test Accuracy: 0.855971\n",
      "Step 29 | Training Loss: 1.040231 | Test Loss: 1.939269 | Test Accuracy: 0.856104\n",
      "Step 30 | Training Loss: 0.893501 | Test Loss: 1.940122 | Test Accuracy: 0.856059\n",
      "Step 31 | Training Loss: 0.851828 | Test Loss: 1.940513 | Test Accuracy: 0.856325\n",
      "Step 32 | Training Loss: 0.845287 | Test Loss: 1.939397 | Test Accuracy: 0.857479\n",
      "Step 33 | Training Loss: 0.857662 | Test Loss: 1.941815 | Test Accuracy: 0.857523\n",
      "Step 34 | Training Loss: 1.129161 | Test Loss: 1.940930 | Test Accuracy: 0.857656\n",
      "Step 35 | Training Loss: 0.967440 | Test Loss: 1.941366 | Test Accuracy: 0.857567\n",
      "Step 36 | Training Loss: 0.773123 | Test Loss: 1.942314 | Test Accuracy: 0.856769\n",
      "Step 37 | Training Loss: 0.738968 | Test Loss: 1.940143 | Test Accuracy: 0.857213\n",
      "Step 38 | Training Loss: 0.684051 | Test Loss: 1.942402 | Test Accuracy: 0.867947\n",
      "Step 39 | Training Loss: 1.018774 | Test Loss: 1.941425 | Test Accuracy: 0.867903\n",
      "Step 40 | Training Loss: 1.084100 | Test Loss: 1.939004 | Test Accuracy: 0.857567\n",
      "Step 41 | Training Loss: 0.873066 | Test Loss: 1.948846 | Test Accuracy: 0.863999\n",
      "Step 42 | Training Loss: 0.993334 | Test Loss: 1.951358 | Test Accuracy: 0.864044\n",
      "Step 43 | Training Loss: 0.740028 | Test Loss: 1.948237 | Test Accuracy: 0.844881\n",
      "Step 44 | Training Loss: 0.891144 | Test Loss: 1.951765 | Test Accuracy: 0.848385\n",
      "Step 45 | Training Loss: 1.036194 | Test Loss: 1.951134 | Test Accuracy: 0.757541\n",
      "Step 46 | Training Loss: 0.811751 | Test Loss: 1.982956 | Test Accuracy: 0.430758\n",
      "Step 47 | Training Loss: 0.858320 | Test Loss: 1.985385 | Test Accuracy: 0.430758\n",
      "Step 48 | Training Loss: 0.876112 | Test Loss: 1.984697 | Test Accuracy: 0.430758\n",
      "Step 49 | Training Loss: 1.908192 | Test Loss: 1.984435 | Test Accuracy: 0.430758\n",
      "Step 50 | Training Loss: 0.846357 | Test Loss: 1.985491 | Test Accuracy: 0.430758\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.466644 | Test Loss: 1.704938 | Test Accuracy: 0.783934\n",
      "Step 2 | Training Loss: 0.538310 | Test Loss: 1.549969 | Test Accuracy: 0.785353\n",
      "Step 3 | Training Loss: 0.342966 | Test Loss: 1.592874 | Test Accuracy: 0.780518\n",
      "Step 4 | Training Loss: 0.428411 | Test Loss: 1.987881 | Test Accuracy: 0.773864\n",
      "Step 5 | Training Loss: 0.433028 | Test Loss: 1.547525 | Test Accuracy: 0.783268\n",
      "Step 6 | Training Loss: 0.738774 | Test Loss: 537.509033 | Test Accuracy: 0.841643\n",
      "Step 7 | Training Loss: 0.639187 | Test Loss: 1.885759 | Test Accuracy: 0.858277\n",
      "Step 8 | Training Loss: 0.600060 | Test Loss: 1.886469 | Test Accuracy: 0.858588\n",
      "Step 9 | Training Loss: 0.489119 | Test Loss: 1.882695 | Test Accuracy: 0.857612\n",
      "Step 10 | Training Loss: 0.669103 | Test Loss: 1.878415 | Test Accuracy: 0.847853\n",
      "Step 11 | Training Loss: 0.687957 | Test Loss: 1.881619 | Test Accuracy: 0.856237\n",
      "Step 12 | Training Loss: 0.662885 | Test Loss: 1.877844 | Test Accuracy: 0.855483\n",
      "Step 13 | Training Loss: 0.929510 | Test Loss: 1.984324 | Test Accuracy: 0.835832\n",
      "Step 14 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 15 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 16 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 17 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 18 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 19 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 20 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 21 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 22 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 23 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 24 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 25 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 26 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 27 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 28 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 29 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 30 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 31 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 32 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 33 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 34 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 35 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 36 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 37 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 38 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 39 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 40 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 42 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 45 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 46 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 47 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 48 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 49 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n",
      "Step 50 | Training Loss: nan | Test Loss:    nan | Test Accuracy: 0.569242\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4]\n",
    "\n",
    "    epochs = [50]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:53:34.840290Z",
     "start_time": "2017-05-14T20:53:34.836247Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:53:34.862569Z",
     "start_time": "2017-05-14T20:53:34.841698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.948404</td>\n",
       "      <td>0.870076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.831640</td>\n",
       "      <td>0.867947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.830529</td>\n",
       "      <td>0.867903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.793459</td>\n",
       "      <td>0.864044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.865137</td>\n",
       "      <td>0.863999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.794094</td>\n",
       "      <td>0.863999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.928481</td>\n",
       "      <td>0.862225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.941816</td>\n",
       "      <td>0.859164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.914351</td>\n",
       "      <td>0.858588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.911494</td>\n",
       "      <td>0.858277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.827909</td>\n",
       "      <td>0.857656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.926338</td>\n",
       "      <td>0.857612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.829973</td>\n",
       "      <td>0.857567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.826957</td>\n",
       "      <td>0.857567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.821876</td>\n",
       "      <td>0.857523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.825607</td>\n",
       "      <td>0.857479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.827750</td>\n",
       "      <td>0.857213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.461740</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.462931</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.463169</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.469440</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.465788</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470472</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.467296</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.468487</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.469598</td>\n",
       "      <td>0.569242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.670186</td>\n",
       "      <td>0.564452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.533497</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.527623</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.534053</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.539451</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.532624</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.537546</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "279     50               8              4     0.948404    0.870076\n",
       "337     50              16              4     0.831640    0.867947\n",
       "338     50              16              4     0.830529    0.867903\n",
       "341     50              16              4     0.793459    0.864044\n",
       "105     50              16              2     0.865137    0.863999\n",
       "340     50              16              4     0.794094    0.863999\n",
       "219     50               4              4     0.928481    0.862225\n",
       "278     50               8              4     0.941816    0.859164\n",
       "357     50              32              4     0.914351    0.858588\n",
       "356     50              32              4     0.911494    0.858277\n",
       "333     50              16              4     0.827909    0.857656\n",
       "358     50              32              4     0.926338    0.857612\n",
       "334     50              16              4     0.829973    0.857567\n",
       "339     50              16              4     0.826957    0.857567\n",
       "332     50              16              4     0.821876    0.857523\n",
       "331     50              16              4     0.825607    0.857479\n",
       "336     50              16              4     0.827750    0.857213\n",
       "..     ...             ...            ...          ...         ...\n",
       "181     50              32              2     0.461740    0.569242\n",
       "182     50              32              2     0.462931    0.569242\n",
       "183     50              32              2     0.463169    0.569242\n",
       "184     50              32              2     0.469440    0.569242\n",
       "185     50              32              2     0.465788    0.569242\n",
       "186     50              32              2     0.470472    0.569242\n",
       "160     50              32              2     0.467296    0.569242\n",
       "187     50              32              2     0.468487    0.569242\n",
       "199     50              32              2     0.469598    0.569242\n",
       "282     50               8              4     0.670186    0.564452\n",
       "345     50              16              4     0.533497    0.430758\n",
       "346     50              16              4     0.527623    0.430758\n",
       "347     50              16              4     0.532227    0.430758\n",
       "348     50              16              4     0.534053    0.430758\n",
       "349     50              16              4     0.539451    0.430758\n",
       "103     50              16              2     0.532624    0.430758\n",
       "104     50              16              2     0.537546    0.430758\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:53:34.872932Z",
     "start_time": "2017-05-14T20:53:34.863930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:53:34.938636Z",
     "start_time": "2017-05-14T20:53:34.874441Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:53:35.226407Z",
     "start_time": "2017-05-14T20:53:34.940125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8399  0.1601]\n",
      " [ 0.1168  0.8832]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDm5mZpY7/w8gSJYoRcYh9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b4cee0a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
