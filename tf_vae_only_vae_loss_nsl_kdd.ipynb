{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:06.156846Z",
     "start_time": "2017-05-14T20:16:05.582870Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:06.261987Z",
     "start_time": "2017-05-14T20:16:06.158972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:06.269608Z",
     "start_time": "2017-05-14T20:16:06.263865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:06.277290Z",
     "start_time": "2017-05-14T20:16:06.271714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:07.334647Z",
     "start_time": "2017-05-14T20:16:06.280224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99186991653217393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    x_train = np.hstack((x_train, y_train))\n",
    "    x_test = np.hstack((x_test, np.random.normal(size = (x_test.shape[0], y_train.shape[1]))))\n",
    "    #x_test = np.hstack((x_test, y_test))\n",
    "    \n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:08.821458Z",
     "start_time": "2017-05-14T20:16:07.336796Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:09.396111Z",
     "start_time": "2017-05-14T20:16:08.823459Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 124\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 124\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 124\n",
    "    lam = 0.01\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            self.x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "            self.y = tf.slice(self.x_hat, [0,input_dim-2], [-1,-1])\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:16:09.541721Z",
     "start_time": "2017-05-14T20:16:09.398344Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            Train.best_acc = 0\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                #print(\"Step {} | Training Loss:\".format(epoch), end = \" \" )\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.1)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    #if(train_loss > 1e9):\n",
    "                    \n",
    "                    #print(\"{:.6f}\".format(train_loss), end = \", \" )\n",
    "                    \n",
    "                #print(\"\")\n",
    "                valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                \n",
    "                accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, \n",
    "                                                               net.pred, \n",
    "                                                               net.actual, net.y], \n",
    "                                                              feed_dict={net.x: preprocess.x_test, \n",
    "                                                                         net.y_: preprocess.y_test, \n",
    "                                                                         net.keep_prob:1})\n",
    "                #print(\"*************** \\n\")\n",
    "                print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:6f} | Test Accuracy: {:.6f}\".format(epoch, train_loss, test_loss, accuracy))\n",
    "                #print(\"*************** \\n\")\n",
    "                #print(\"Accuracy on Test data: {}\".format(accuracy))\n",
    "\n",
    "                \n",
    "\n",
    "                if accuracy > Train.best_acc:\n",
    "                    Train.best_acc = accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                    net.saver.save(sess, \"dataset/tf_vae_only_vae_loss_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                    Train.results.append(Train.result(epochs, f, h,valid_accuracy, accuracy))\n",
    "                    curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                    Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:42:33.810667Z",
     "start_time": "2017-05-14T20:16:09.543654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.002049 | Test Loss: 0.006900 | Test Accuracy: 0.762287\n",
      "Step 2 | Training Loss: 0.000913 | Test Loss: 0.006516 | Test Accuracy: 0.783224\n",
      "Step 3 | Training Loss: 0.000574 | Test Loss: 0.005305 | Test Accuracy: 0.774885\n",
      "Step 4 | Training Loss: 0.000685 | Test Loss: 0.005682 | Test Accuracy: 0.775683\n",
      "Step 5 | Training Loss: 0.000423 | Test Loss: 0.005936 | Test Accuracy: 0.779054\n",
      "Step 6 | Training Loss: 0.000265 | Test Loss: 0.008684 | Test Accuracy: 0.728220\n",
      "Step 7 | Training Loss: 0.000312 | Test Loss: 0.006923 | Test Accuracy: 0.767477\n",
      "Step 8 | Training Loss: 0.000815 | Test Loss: 0.006486 | Test Accuracy: 0.766812\n",
      "Step 9 | Training Loss: 0.000328 | Test Loss: 0.007451 | Test Accuracy: 0.752528\n",
      "Step 10 | Training Loss: 0.000414 | Test Loss: 0.008112 | Test Accuracy: 0.765348\n",
      "Step 11 | Training Loss: 0.000395 | Test Loss: 0.010227 | Test Accuracy: 0.731636\n",
      "Step 12 | Training Loss: 0.000834 | Test Loss: 0.010347 | Test Accuracy: 0.718950\n",
      "Step 13 | Training Loss: 0.000244 | Test Loss: 0.011586 | Test Accuracy: 0.727732\n",
      "Step 14 | Training Loss: 0.000924 | Test Loss: 0.010997 | Test Accuracy: 0.729241\n",
      "Step 15 | Training Loss: 0.000129 | Test Loss: 0.010142 | Test Accuracy: 0.731680\n",
      "Step 16 | Training Loss: 0.000144 | Test Loss: 0.011124 | Test Accuracy: 0.735096\n",
      "Step 17 | Training Loss: 0.000712 | Test Loss: 0.010597 | Test Accuracy: 0.742814\n",
      "Step 18 | Training Loss: 0.000037 | Test Loss: 0.010418 | Test Accuracy: 0.733943\n",
      "Step 19 | Training Loss: 0.000321 | Test Loss: 0.009205 | Test Accuracy: 0.752795\n",
      "Step 20 | Training Loss: 0.000092 | Test Loss: 0.009652 | Test Accuracy: 0.748492\n",
      "Step 21 | Training Loss: 0.000402 | Test Loss: 0.009881 | Test Accuracy: 0.731503\n",
      "Step 22 | Training Loss: 0.000075 | Test Loss: 0.007491 | Test Accuracy: 0.768275\n",
      "Step 23 | Training Loss: 0.000354 | Test Loss: 0.008351 | Test Accuracy: 0.798705\n",
      "Step 24 | Training Loss: 0.000147 | Test Loss: 0.007336 | Test Accuracy: 0.776836\n",
      "Step 25 | Training Loss: 0.000936 | Test Loss: 0.008798 | Test Accuracy: 0.778877\n",
      "Step 26 | Training Loss: 0.000008 | Test Loss: 0.009094 | Test Accuracy: 0.774796\n",
      "Step 27 | Training Loss: 0.000427 | Test Loss: 0.010420 | Test Accuracy: 0.733144\n",
      "Step 28 | Training Loss: 0.000824 | Test Loss: 0.009541 | Test Accuracy: 0.800124\n",
      "Step 29 | Training Loss: 0.000586 | Test Loss: 0.007441 | Test Accuracy: 0.789878\n",
      "Step 30 | Training Loss: 0.000603 | Test Loss: 0.008974 | Test Accuracy: 0.769562\n",
      "Step 31 | Training Loss: 0.000198 | Test Loss: 0.007891 | Test Accuracy: 0.794890\n",
      "Step 32 | Training Loss: 0.000154 | Test Loss: 0.008066 | Test Accuracy: 0.767965\n",
      "Step 33 | Training Loss: 0.000113 | Test Loss: 0.009691 | Test Accuracy: 0.779720\n",
      "Step 34 | Training Loss: 0.000436 | Test Loss: 0.009430 | Test Accuracy: 0.796088\n",
      "Step 35 | Training Loss: 0.000162 | Test Loss: 0.007801 | Test Accuracy: 0.790765\n",
      "Step 36 | Training Loss: 0.000095 | Test Loss: 0.007429 | Test Accuracy: 0.801411\n",
      "Step 37 | Training Loss: 0.000548 | Test Loss: 0.008065 | Test Accuracy: 0.789744\n",
      "Step 38 | Training Loss: 0.000194 | Test Loss: 0.009010 | Test Accuracy: 0.769118\n",
      "Step 39 | Training Loss: 0.000055 | Test Loss: 0.008143 | Test Accuracy: 0.808552\n",
      "Step 40 | Training Loss: 0.000332 | Test Loss: 0.006789 | Test Accuracy: 0.795511\n",
      "Step 41 | Training Loss: 0.000010 | Test Loss: 0.006622 | Test Accuracy: 0.810947\n",
      "Step 42 | Training Loss: 0.000566 | Test Loss: 0.007145 | Test Accuracy: 0.787615\n",
      "Step 43 | Training Loss: 0.000337 | Test Loss: 0.006699 | Test Accuracy: 0.800213\n",
      "Step 44 | Training Loss: 0.000051 | Test Loss: 0.007221 | Test Accuracy: 0.797507\n",
      "Step 45 | Training Loss: 0.000016 | Test Loss: 0.006780 | Test Accuracy: 0.812500\n",
      "Step 46 | Training Loss: 0.000413 | Test Loss: 0.005339 | Test Accuracy: 0.827138\n",
      "Step 47 | Training Loss: 0.000035 | Test Loss: 0.006483 | Test Accuracy: 0.840091\n",
      "Step 48 | Training Loss: 0.000232 | Test Loss: 0.006287 | Test Accuracy: 0.811657\n",
      "Step 49 | Training Loss: 0.000016 | Test Loss: 0.006103 | Test Accuracy: 0.824477\n",
      "Step 50 | Training Loss: 0.000099 | Test Loss: 0.007054 | Test Accuracy: 0.814452\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.000142 | Test Loss: 4.848848 | Test Accuracy: 0.780341\n",
      "Step 2 | Training Loss: 0.001123 | Test Loss: 0.006568 | Test Accuracy: 0.744677\n",
      "Step 3 | Training Loss: 0.000087 | Test Loss: 0.005606 | Test Accuracy: 0.767743\n",
      "Step 4 | Training Loss: 0.000153 | Test Loss: 0.007841 | Test Accuracy: 0.721301\n",
      "Step 5 | Training Loss: 0.000218 | Test Loss: 0.006386 | Test Accuracy: 0.750044\n",
      "Step 6 | Training Loss: 0.000032 | Test Loss: 0.006528 | Test Accuracy: 0.744899\n",
      "Step 7 | Training Loss: 0.000156 | Test Loss: 0.007981 | Test Accuracy: 0.735894\n",
      "Step 8 | Training Loss: 0.000053 | Test Loss: 0.007483 | Test Accuracy: 0.729462\n",
      "Step 9 | Training Loss: 0.000351 | Test Loss: 0.008259 | Test Accuracy: 0.719260\n",
      "Step 10 | Training Loss: 0.000672 | Test Loss: 0.006678 | Test Accuracy: 0.728664\n",
      "Step 11 | Training Loss: 0.000236 | Test Loss: 0.008839 | Test Accuracy: 0.706663\n",
      "Step 12 | Training Loss: 0.000035 | Test Loss: 0.008049 | Test Accuracy: 0.710344\n",
      "Step 13 | Training Loss: 0.000292 | Test Loss: 0.009443 | Test Accuracy: 0.705864\n",
      "Step 14 | Training Loss: 0.000108 | Test Loss: 0.009014 | Test Accuracy: 0.705509\n",
      "Step 15 | Training Loss: 0.000924 | Test Loss: 0.009537 | Test Accuracy: 0.706529\n",
      "Step 16 | Training Loss: 0.000908 | Test Loss: 0.009565 | Test Accuracy: 0.707638\n",
      "Step 17 | Training Loss: 0.000805 | Test Loss: 0.010218 | Test Accuracy: 0.688476\n",
      "Step 18 | Training Loss: 0.000433 | Test Loss: 0.009454 | Test Accuracy: 0.703957\n",
      "Step 19 | Training Loss: 0.000099 | Test Loss: 0.009912 | Test Accuracy: 0.695396\n",
      "Step 20 | Training Loss: 0.000361 | Test Loss: 0.011313 | Test Accuracy: 0.702715\n",
      "Step 21 | Training Loss: 0.000451 | Test Loss: 0.010018 | Test Accuracy: 0.696105\n",
      "Step 22 | Training Loss: 0.000027 | Test Loss: 0.009447 | Test Accuracy: 0.723119\n",
      "Step 23 | Training Loss: 0.000196 | Test Loss: 0.010138 | Test Accuracy: 0.703158\n",
      "Step 24 | Training Loss: 0.000562 | Test Loss: 0.011219 | Test Accuracy: 0.714558\n",
      "Step 25 | Training Loss: 0.000322 | Test Loss: 0.011442 | Test Accuracy: 0.704489\n",
      "Step 26 | Training Loss: 0.000053 | Test Loss: 0.010037 | Test Accuracy: 0.712340\n",
      "Step 27 | Training Loss: 0.000909 | Test Loss: 0.010244 | Test Accuracy: 0.688077\n",
      "Step 28 | Training Loss: 0.000192 | Test Loss: 0.009860 | Test Accuracy: 0.690073\n",
      "Step 29 | Training Loss: 0.000127 | Test Loss: 0.009310 | Test Accuracy: 0.698235\n",
      "Step 30 | Training Loss: 0.000335 | Test Loss: 0.008937 | Test Accuracy: 0.703203\n",
      "Step 31 | Training Loss: 0.000423 | Test Loss: 0.009688 | Test Accuracy: 0.692512\n",
      "Step 32 | Training Loss: 0.000701 | Test Loss: 0.009889 | Test Accuracy: 0.687944\n",
      "Step 33 | Training Loss: 0.000040 | Test Loss: 0.011883 | Test Accuracy: 0.675523\n",
      "Step 34 | Training Loss: 0.000264 | Test Loss: 0.011173 | Test Accuracy: 0.680625\n",
      "Step 35 | Training Loss: 0.000172 | Test Loss: 0.011023 | Test Accuracy: 0.683552\n",
      "Step 36 | Training Loss: 0.000137 | Test Loss: 0.011371 | Test Accuracy: 0.688653\n",
      "Step 37 | Training Loss: 0.001044 | Test Loss: 0.013050 | Test Accuracy: 0.686036\n",
      "Step 38 | Training Loss: 0.000283 | Test Loss: 0.012625 | Test Accuracy: 0.671309\n",
      "Step 39 | Training Loss: 0.000537 | Test Loss: 0.012113 | Test Accuracy: 0.702936\n",
      "Step 40 | Training Loss: 0.000078 | Test Loss: 0.009852 | Test Accuracy: 0.693178\n",
      "Step 41 | Training Loss: 0.000273 | Test Loss: 0.009919 | Test Accuracy: 0.685371\n",
      "Step 42 | Training Loss: 0.000126 | Test Loss: 0.011422 | Test Accuracy: 0.700231\n",
      "Step 43 | Training Loss: 0.000038 | Test Loss: 0.009374 | Test Accuracy: 0.696549\n",
      "Step 44 | Training Loss: 0.000119 | Test Loss: 0.009242 | Test Accuracy: 0.698323\n",
      "Step 45 | Training Loss: 0.000341 | Test Loss: 0.009972 | Test Accuracy: 0.701428\n",
      "Step 46 | Training Loss: 0.000360 | Test Loss: 0.009879 | Test Accuracy: 0.707195\n",
      "Step 47 | Training Loss: 0.000275 | Test Loss: 0.009472 | Test Accuracy: 0.702182\n",
      "Step 48 | Training Loss: 0.000209 | Test Loss: 0.009195 | Test Accuracy: 0.699831\n",
      "Step 49 | Training Loss: 0.000432 | Test Loss: 0.009965 | Test Accuracy: 0.706973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50 | Training Loss: 0.000390 | Test Loss: 0.009858 | Test Accuracy: 0.698856\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.001455 | Test Loss: 0.017737 | Test Accuracy: 0.783801\n",
      "Step 2 | Training Loss: 0.001043 | Test Loss: 0.010940 | Test Accuracy: 0.811568\n",
      "Step 3 | Training Loss: 0.002049 | Test Loss: 0.011264 | Test Accuracy: 0.835477\n",
      "Step 4 | Training Loss: 0.000360 | Test Loss: 0.008973 | Test Accuracy: 0.786684\n",
      "Step 5 | Training Loss: 0.000949 | Test Loss: 0.014045 | Test Accuracy: 0.806955\n",
      "Step 6 | Training Loss: 0.000870 | Test Loss: 0.008680 | Test Accuracy: 0.779187\n",
      "Step 7 | Training Loss: 0.002761 | Test Loss: 0.013877 | Test Accuracy: 0.835655\n",
      "Step 8 | Training Loss: 0.001230 | Test Loss: 0.008530 | Test Accuracy: 0.812323\n",
      "Step 9 | Training Loss: 0.000078 | Test Loss: 0.008805 | Test Accuracy: 0.781050\n",
      "Step 10 | Training Loss: 0.003821 | Test Loss: 0.007726 | Test Accuracy: 0.796088\n",
      "Step 11 | Training Loss: 0.000648 | Test Loss: 0.014316 | Test Accuracy: 0.759005\n",
      "Step 12 | Training Loss: 0.001821 | Test Loss: 0.014650 | Test Accuracy: 0.755855\n",
      "Step 13 | Training Loss: 0.000454 | Test Loss: 0.020412 | Test Accuracy: 0.760069\n",
      "Step 14 | Training Loss: 0.000215 | Test Loss: 0.014364 | Test Accuracy: 0.745121\n",
      "Step 15 | Training Loss: 0.000353 | Test Loss: 0.012145 | Test Accuracy: 0.748669\n",
      "Step 16 | Training Loss: 0.000379 | Test Loss: 0.012907 | Test Accuracy: 0.725781\n",
      "Step 17 | Training Loss: 0.006609 | Test Loss: 0.103096 | Test Accuracy: 0.754968\n",
      "Step 18 | Training Loss: 0.000619 | Test Loss: 0.032882 | Test Accuracy: 0.775506\n",
      "Step 19 | Training Loss: 0.000839 | Test Loss: 0.028423 | Test Accuracy: 0.765836\n",
      "Step 20 | Training Loss: 0.000528 | Test Loss: 0.022822 | Test Accuracy: 0.722942\n",
      "Step 21 | Training Loss: 0.000205 | Test Loss: 0.026421 | Test Accuracy: 0.749379\n",
      "Step 22 | Training Loss: 0.000276 | Test Loss: 0.026651 | Test Accuracy: 0.732567\n",
      "Step 23 | Training Loss: 0.000427 | Test Loss: 0.022780 | Test Accuracy: 0.748935\n",
      "Step 24 | Training Loss: 0.000596 | Test Loss: 0.022346 | Test Accuracy: 0.741661\n",
      "Step 25 | Training Loss: 0.000387 | Test Loss: 0.020098 | Test Accuracy: 0.710877\n",
      "Step 26 | Training Loss: 0.000706 | Test Loss: 0.022339 | Test Accuracy: 0.741705\n",
      "Step 27 | Training Loss: 0.000766 | Test Loss: 0.018920 | Test Accuracy: 0.741217\n",
      "Step 28 | Training Loss: 0.000126 | Test Loss: 0.017980 | Test Accuracy: 0.730216\n",
      "Step 29 | Training Loss: 0.000019 | Test Loss: 0.018963 | Test Accuracy: 0.719171\n",
      "Step 30 | Training Loss: 0.000821 | Test Loss: 0.022771 | Test Accuracy: 0.766235\n",
      "Step 31 | Training Loss: 0.000812 | Test Loss: 0.014675 | Test Accuracy: 0.704267\n",
      "Step 32 | Training Loss: 0.000058 | Test Loss: 0.016943 | Test Accuracy: 0.732479\n",
      "Step 33 | Training Loss: 0.000656 | Test Loss: 0.016787 | Test Accuracy: 0.722232\n",
      "Step 34 | Training Loss: 0.000118 | Test Loss: 0.016383 | Test Accuracy: 0.729906\n",
      "Step 35 | Training Loss: 0.000139 | Test Loss: 0.015301 | Test Accuracy: 0.719571\n",
      "Step 36 | Training Loss: 0.000367 | Test Loss: 0.015144 | Test Accuracy: 0.719837\n",
      "Step 37 | Training Loss: 0.000127 | Test Loss: 0.015269 | Test Accuracy: 0.711276\n",
      "Step 38 | Training Loss: 0.000420 | Test Loss: 0.017035 | Test Accuracy: 0.724228\n",
      "Step 39 | Training Loss: 0.000175 | Test Loss: 0.015707 | Test Accuracy: 0.712296\n",
      "Step 40 | Training Loss: 0.000237 | Test Loss: 0.014478 | Test Accuracy: 0.713006\n",
      "Step 41 | Training Loss: 0.000068 | Test Loss: 0.015874 | Test Accuracy: 0.693045\n",
      "Step 42 | Training Loss: 0.000564 | Test Loss: 0.013995 | Test Accuracy: 0.696682\n",
      "Step 43 | Training Loss: 0.000491 | Test Loss: 0.016820 | Test Accuracy: 0.712074\n",
      "Step 44 | Training Loss: 0.000157 | Test Loss: 0.014134 | Test Accuracy: 0.694154\n",
      "Step 45 | Training Loss: 0.000439 | Test Loss: 0.018706 | Test Accuracy: 0.682488\n",
      "Step 46 | Training Loss: 0.000930 | Test Loss: 0.019603 | Test Accuracy: 0.703025\n",
      "Step 47 | Training Loss: 0.000103 | Test Loss: 0.018345 | Test Accuracy: 0.674281\n",
      "Step 48 | Training Loss: 0.000299 | Test Loss: 0.015358 | Test Accuracy: 0.702049\n",
      "Step 49 | Training Loss: 0.000131 | Test Loss: 0.013345 | Test Accuracy: 0.699166\n",
      "Step 50 | Training Loss: 0.000288 | Test Loss: 0.013114 | Test Accuracy: 0.694597\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.001110 | Test Loss: 0.008302 | Test Accuracy: 0.749246\n",
      "Step 2 | Training Loss: 0.000300 | Test Loss: 0.006314 | Test Accuracy: 0.791785\n",
      "Step 3 | Training Loss: 0.000428 | Test Loss: 0.005592 | Test Accuracy: 0.805004\n",
      "Step 4 | Training Loss: 0.001020 | Test Loss: 0.009101 | Test Accuracy: 0.775994\n",
      "Step 5 | Training Loss: 0.000602 | Test Loss: 0.009331 | Test Accuracy: 0.774840\n",
      "Step 6 | Training Loss: 0.000483 | Test Loss: 0.009868 | Test Accuracy: 0.763130\n",
      "Step 7 | Training Loss: 0.000044 | Test Loss: 0.013320 | Test Accuracy: 0.772933\n",
      "Step 8 | Training Loss: 0.001041 | Test Loss: 0.012039 | Test Accuracy: 0.752440\n",
      "Step 9 | Training Loss: 0.000145 | Test Loss: 0.010244 | Test Accuracy: 0.729418\n",
      "Step 10 | Training Loss: 0.000076 | Test Loss: 0.009051 | Test Accuracy: 0.734785\n",
      "Step 11 | Training Loss: 0.000322 | Test Loss: 0.011967 | Test Accuracy: 0.737802\n",
      "Step 12 | Training Loss: 0.000402 | Test Loss: 0.010983 | Test Accuracy: 0.741128\n",
      "Step 13 | Training Loss: 0.000376 | Test Loss: 0.011179 | Test Accuracy: 0.733100\n",
      "Step 14 | Training Loss: 0.000342 | Test Loss: 0.010376 | Test Accuracy: 0.737935\n",
      "Step 15 | Training Loss: 0.000331 | Test Loss: 0.015979 | Test Accuracy: 0.756920\n",
      "Step 16 | Training Loss: 0.000550 | Test Loss: 0.012207 | Test Accuracy: 0.725115\n",
      "Step 17 | Training Loss: 0.000173 | Test Loss: 0.013731 | Test Accuracy: 0.697303\n",
      "Step 18 | Training Loss: 0.000093 | Test Loss: 0.014467 | Test Accuracy: 0.704356\n",
      "Step 19 | Training Loss: 0.000355 | Test Loss: 0.012080 | Test Accuracy: 0.707195\n",
      "Step 20 | Training Loss: 0.000277 | Test Loss: 0.009928 | Test Accuracy: 0.732834\n",
      "Step 21 | Training Loss: 0.000193 | Test Loss: 0.011851 | Test Accuracy: 0.755412\n",
      "Step 22 | Training Loss: 0.000421 | Test Loss: 0.009484 | Test Accuracy: 0.741439\n",
      "Step 23 | Training Loss: 0.000514 | Test Loss: 0.007918 | Test Accuracy: 0.748226\n",
      "Step 24 | Training Loss: 0.000038 | Test Loss: 0.010493 | Test Accuracy: 0.731503\n",
      "Step 25 | Training Loss: 0.000041 | Test Loss: 0.009643 | Test Accuracy: 0.734076\n",
      "Step 26 | Training Loss: 0.000951 | Test Loss: 0.008498 | Test Accuracy: 0.741439\n",
      "Step 27 | Training Loss: 0.000188 | Test Loss: 0.007156 | Test Accuracy: 0.732301\n",
      "Step 28 | Training Loss: 0.000093 | Test Loss: 0.008959 | Test Accuracy: 0.743479\n",
      "Step 29 | Training Loss: 0.000118 | Test Loss: 0.009222 | Test Accuracy: 0.728265\n",
      "Step 30 | Training Loss: 0.000153 | Test Loss: 0.008000 | Test Accuracy: 0.745963\n",
      "Step 31 | Training Loss: 0.000448 | Test Loss: 0.007892 | Test Accuracy: 0.762997\n",
      "Step 32 | Training Loss: 0.000094 | Test Loss: 0.008545 | Test Accuracy: 0.755456\n",
      "Step 33 | Training Loss: 0.000100 | Test Loss: 0.007755 | Test Accuracy: 0.741217\n",
      "Step 34 | Training Loss: 0.000025 | Test Loss: 0.008632 | Test Accuracy: 0.729418\n",
      "Step 35 | Training Loss: 0.000420 | Test Loss: 0.007476 | Test Accuracy: 0.746984\n",
      "Step 36 | Training Loss: 0.000369 | Test Loss: 0.007353 | Test Accuracy: 0.752351\n",
      "Step 37 | Training Loss: 0.000057 | Test Loss: 0.008787 | Test Accuracy: 0.738112\n",
      "Step 38 | Training Loss: 0.000510 | Test Loss: 0.007137 | Test Accuracy: 0.742903\n",
      "Step 39 | Training Loss: 0.000054 | Test Loss: 0.007212 | Test Accuracy: 0.748403\n",
      "Step 40 | Training Loss: 0.000065 | Test Loss: 0.007857 | Test Accuracy: 0.759404\n",
      "Step 41 | Training Loss: 0.000441 | Test Loss: 0.007156 | Test Accuracy: 0.740507\n",
      "Step 42 | Training Loss: 0.000094 | Test Loss: 0.007131 | Test Accuracy: 0.741084\n",
      "Step 43 | Training Loss: 0.000535 | Test Loss: 0.007876 | Test Accuracy: 0.742858\n",
      "Step 44 | Training Loss: 0.000141 | Test Loss: 0.007435 | Test Accuracy: 0.733588\n",
      "Step 45 | Training Loss: 0.000346 | Test Loss: 0.007647 | Test Accuracy: 0.723563\n",
      "Step 46 | Training Loss: 0.000451 | Test Loss: 0.007851 | Test Accuracy: 0.725426\n",
      "Step 47 | Training Loss: 0.000202 | Test Loss: 0.007447 | Test Accuracy: 0.730261\n",
      "Step 48 | Training Loss: 0.000001 | Test Loss: 0.007550 | Test Accuracy: 0.732434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49 | Training Loss: 0.000290 | Test Loss: 0.007548 | Test Accuracy: 0.738911\n",
      "Step 50 | Training Loss: 0.000163 | Test Loss: 0.007832 | Test Accuracy: 0.734874\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.002049 | Test Loss: 0.003780 | Test Accuracy: 0.786329\n",
      "Step 2 | Training Loss: 0.001114 | Test Loss: 0.002870 | Test Accuracy: 0.776836\n",
      "Step 3 | Training Loss: 0.000301 | Test Loss: 0.003780 | Test Accuracy: 0.729374\n",
      "Step 4 | Training Loss: 0.000683 | Test Loss: 0.003420 | Test Accuracy: 0.818932\n",
      "Step 5 | Training Loss: 0.000392 | Test Loss: 0.003944 | Test Accuracy: 0.852821\n",
      "Step 6 | Training Loss: 0.000789 | Test Loss: 0.003064 | Test Accuracy: 0.834856\n",
      "Step 7 | Training Loss: 0.000924 | Test Loss: 0.005637 | Test Accuracy: 0.659776\n",
      "Step 8 | Training Loss: 0.000529 | Test Loss: 0.006301 | Test Accuracy: 0.670245\n",
      "Step 9 | Training Loss: 0.000160 | Test Loss: 0.006024 | Test Accuracy: 0.645360\n",
      "Step 10 | Training Loss: 0.000400 | Test Loss: 0.005357 | Test Accuracy: 0.702182\n",
      "Step 11 | Training Loss: 0.000499 | Test Loss: 0.003597 | Test Accuracy: 0.698589\n",
      "Step 12 | Training Loss: 0.000154 | Test Loss: 0.003942 | Test Accuracy: 0.702670\n",
      "Step 13 | Training Loss: 0.000562 | Test Loss: 0.006538 | Test Accuracy: 0.558109\n",
      "Step 14 | Training Loss: 0.001007 | Test Loss: 0.005656 | Test Accuracy: 0.545910\n",
      "Step 15 | Training Loss: 0.000234 | Test Loss: 0.005050 | Test Accuracy: 0.540144\n",
      "Step 16 | Training Loss: 0.000137 | Test Loss: 0.004980 | Test Accuracy: 0.534998\n",
      "Step 17 | Training Loss: 0.000632 | Test Loss: 0.005324 | Test Accuracy: 0.600248\n",
      "Step 18 | Training Loss: 0.001068 | Test Loss: 0.003837 | Test Accuracy: 0.563786\n",
      "Step 19 | Training Loss: 0.000056 | Test Loss: 0.005630 | Test Accuracy: 0.571993\n",
      "Step 20 | Training Loss: 0.000154 | Test Loss: 0.004329 | Test Accuracy: 0.681157\n",
      "Step 21 | Training Loss: 0.000347 | Test Loss: 0.003840 | Test Accuracy: 0.667362\n",
      "Step 22 | Training Loss: 0.000465 | Test Loss: 0.003962 | Test Accuracy: 0.671531\n",
      "Step 23 | Training Loss: 0.000491 | Test Loss: 0.003616 | Test Accuracy: 0.671265\n",
      "Step 24 | Training Loss: 0.000258 | Test Loss: 0.003261 | Test Accuracy: 0.670067\n",
      "Step 25 | Training Loss: 0.000055 | Test Loss: 0.003105 | Test Accuracy: 0.569242\n",
      "Step 26 | Training Loss: 0.000776 | Test Loss: 0.002918 | Test Accuracy: 0.600825\n",
      "Step 27 | Training Loss: 0.000956 | Test Loss: 0.001776 | Test Accuracy: 0.591687\n",
      "Step 28 | Training Loss: 0.000770 | Test Loss: 0.001857 | Test Accuracy: 0.569242\n",
      "Step 29 | Training Loss: 0.001102 | Test Loss: 0.000341 | Test Accuracy: 0.605749\n",
      "Step 30 | Training Loss: 0.000273 | Test Loss: 0.000989 | Test Accuracy: 0.608543\n",
      "Step 31 | Training Loss: 0.000225 | Test Loss: 0.000128 | Test Accuracy: 0.602821\n",
      "Step 32 | Training Loss: 0.008241 | Test Loss: 0.008609 | Test Accuracy: 0.446815\n",
      "Step 33 | Training Loss: 0.000844 | Test Loss: 0.008317 | Test Accuracy: 0.528345\n",
      "Step 34 | Training Loss: 0.000525 | Test Loss: 0.003440 | Test Accuracy: 0.466155\n",
      "Step 35 | Training Loss: 0.000198 | Test Loss: 0.002873 | Test Accuracy: 0.462784\n",
      "Step 36 | Training Loss: 0.000744 | Test Loss: 0.001392 | Test Accuracy: 0.569242\n",
      "Step 37 | Training Loss: 0.001698 | Test Loss: 0.001992 | Test Accuracy: 0.460167\n",
      "Step 38 | Training Loss: 0.000751 | Test Loss: 0.001556 | Test Accuracy: 0.460300\n",
      "Step 39 | Training Loss: 0.000073 | Test Loss: 0.001769 | Test Accuracy: 0.569198\n",
      "Step 40 | Training Loss: 0.000376 | Test Loss: 0.001776 | Test Accuracy: 0.569242\n",
      "Step 41 | Training Loss: 0.000432 | Test Loss: 0.002449 | Test Accuracy: 0.460433\n",
      "Step 42 | Training Loss: 0.001085 | Test Loss: 0.002387 | Test Accuracy: 0.569242\n",
      "Step 43 | Training Loss: 0.000327 | Test Loss: 0.001969 | Test Accuracy: 0.569242\n",
      "Step 44 | Training Loss: 0.000683 | Test Loss: 0.001267 | Test Accuracy: 0.460477\n",
      "Step 45 | Training Loss: 0.000082 | Test Loss: 0.001429 | Test Accuracy: 0.460699\n",
      "Step 46 | Training Loss: 0.000046 | Test Loss: 0.003220 | Test Accuracy: 0.460522\n",
      "Step 47 | Training Loss: 0.000102 | Test Loss: 0.000235 | Test Accuracy: 0.460433\n",
      "Step 48 | Training Loss: 0.000543 | Test Loss: 0.000419 | Test Accuracy: 0.460566\n",
      "Step 49 | Training Loss: 0.000216 | Test Loss: 0.000907 | Test Accuracy: 0.461187\n",
      "Step 50 | Training Loss: 0.000887 | Test Loss: 0.002480 | Test Accuracy: 0.458259\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.002856 | Test Loss: 0.005592 | Test Accuracy: 0.671176\n",
      "Step 2 | Training Loss: 0.000010 | Test Loss: 0.003066 | Test Accuracy: 0.744145\n",
      "Step 3 | Training Loss: 0.000863 | Test Loss: 0.003080 | Test Accuracy: 0.784377\n",
      "Step 4 | Training Loss: 0.000131 | Test Loss: 0.004344 | Test Accuracy: 0.718683\n",
      "Step 5 | Training Loss: 0.000065 | Test Loss: 0.004414 | Test Accuracy: 0.735850\n",
      "Step 6 | Training Loss: 0.001522 | Test Loss: 0.005653 | Test Accuracy: 0.710743\n",
      "Step 7 | Training Loss: 0.000592 | Test Loss: 0.004792 | Test Accuracy: 0.767699\n",
      "Step 8 | Training Loss: 0.000843 | Test Loss: 0.004037 | Test Accuracy: 0.801721\n",
      "Step 9 | Training Loss: 0.000156 | Test Loss: 0.005448 | Test Accuracy: 0.769029\n",
      "Step 10 | Training Loss: 0.000607 | Test Loss: 0.005564 | Test Accuracy: 0.747072\n",
      "Step 11 | Training Loss: 0.000589 | Test Loss: 0.004206 | Test Accuracy: 0.742903\n",
      "Step 12 | Training Loss: 0.000757 | Test Loss: 0.005034 | Test Accuracy: 0.760202\n",
      "Step 13 | Training Loss: 0.000501 | Test Loss: 0.003793 | Test Accuracy: 0.785220\n",
      "Step 14 | Training Loss: 0.000213 | Test Loss: 0.003882 | Test Accuracy: 0.812722\n",
      "Step 15 | Training Loss: 0.000036 | Test Loss: 0.003753 | Test Accuracy: 0.797773\n",
      "Step 16 | Training Loss: 0.000256 | Test Loss: 0.003099 | Test Accuracy: 0.818045\n",
      "Step 17 | Training Loss: 0.000079 | Test Loss: 0.003230 | Test Accuracy: 0.783357\n",
      "Step 18 | Training Loss: 0.000141 | Test Loss: 0.004850 | Test Accuracy: 0.744367\n",
      "Step 19 | Training Loss: 0.000091 | Test Loss: 0.004640 | Test Accuracy: 0.732124\n",
      "Step 20 | Training Loss: 0.000269 | Test Loss: 0.004322 | Test Accuracy: 0.759847\n",
      "Step 21 | Training Loss: 0.000024 | Test Loss: 0.004265 | Test Accuracy: 0.767078\n",
      "Step 22 | Training Loss: 0.000159 | Test Loss: 0.004007 | Test Accuracy: 0.783268\n",
      "Step 23 | Training Loss: 0.000088 | Test Loss: 0.004273 | Test Accuracy: 0.781405\n",
      "Step 24 | Training Loss: 0.000356 | Test Loss: 0.005122 | Test Accuracy: 0.748581\n",
      "Step 25 | Training Loss: 0.000314 | Test Loss: 0.005775 | Test Accuracy: 0.724184\n",
      "Step 26 | Training Loss: 0.000019 | Test Loss: 0.004777 | Test Accuracy: 0.760646\n",
      "Step 27 | Training Loss: 0.000606 | Test Loss: 0.004128 | Test Accuracy: 0.779143\n",
      "Step 28 | Training Loss: 0.000916 | Test Loss: 0.003177 | Test Accuracy: 0.789478\n",
      "Step 29 | Training Loss: 0.000251 | Test Loss: 0.004215 | Test Accuracy: 0.786994\n",
      "Step 30 | Training Loss: 0.000029 | Test Loss: 0.003617 | Test Accuracy: 0.783490\n",
      "Step 31 | Training Loss: 0.000219 | Test Loss: 0.003667 | Test Accuracy: 0.781139\n",
      "Step 32 | Training Loss: 0.000639 | Test Loss: 0.003800 | Test Accuracy: 0.789789\n",
      "Step 33 | Training Loss: 0.000139 | Test Loss: 0.003488 | Test Accuracy: 0.796886\n",
      "Step 34 | Training Loss: 0.000252 | Test Loss: 0.002726 | Test Accuracy: 0.794624\n",
      "Step 35 | Training Loss: 0.000004 | Test Loss: 0.002770 | Test Accuracy: 0.808996\n",
      "Step 36 | Training Loss: 0.000038 | Test Loss: 0.002709 | Test Accuracy: 0.796221\n",
      "Step 37 | Training Loss: 0.000263 | Test Loss: 0.002336 | Test Accuracy: 0.758384\n",
      "Step 38 | Training Loss: 0.000789 | Test Loss: 0.003174 | Test Accuracy: 0.789123\n",
      "Step 39 | Training Loss: 0.000389 | Test Loss: 0.002815 | Test Accuracy: 0.812278\n",
      "Step 40 | Training Loss: 0.000460 | Test Loss: 0.002906 | Test Accuracy: 0.807709\n",
      "Step 41 | Training Loss: 0.000368 | Test Loss: 0.003773 | Test Accuracy: 0.747072\n",
      "Step 42 | Training Loss: 0.000182 | Test Loss: 0.003197 | Test Accuracy: 0.769296\n",
      "Step 43 | Training Loss: 0.000254 | Test Loss: 0.003106 | Test Accuracy: 0.756964\n",
      "Step 44 | Training Loss: 0.000381 | Test Loss: 0.004916 | Test Accuracy: 0.727866\n",
      "Step 45 | Training Loss: 0.000162 | Test Loss: 0.003642 | Test Accuracy: 0.736205\n",
      "Step 46 | Training Loss: 0.000101 | Test Loss: 0.003828 | Test Accuracy: 0.731769\n",
      "Step 47 | Training Loss: 0.000159 | Test Loss: 0.004382 | Test Accuracy: 0.748492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 48 | Training Loss: 0.000149 | Test Loss: 0.004741 | Test Accuracy: 0.750133\n",
      "Step 49 | Training Loss: 0.000159 | Test Loss: 0.004348 | Test Accuracy: 0.728442\n",
      "Step 50 | Training Loss: 0.000088 | Test Loss: 0.004851 | Test Accuracy: 0.685016\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.000406 | Test Loss: 0.005080 | Test Accuracy: 0.786462\n",
      "Step 2 | Training Loss: 0.000012 | Test Loss: 0.004697 | Test Accuracy: 0.814319\n",
      "Step 3 | Training Loss: 0.000037 | Test Loss: 0.005162 | Test Accuracy: 0.836231\n",
      "Step 4 | Training Loss: 0.000367 | Test Loss: 0.005833 | Test Accuracy: 0.749068\n",
      "Step 5 | Training Loss: 0.000300 | Test Loss: 0.516254 | Test Accuracy: 0.784776\n",
      "Step 6 | Training Loss: 0.000425 | Test Loss: 0.004624 | Test Accuracy: 0.787970\n",
      "Step 7 | Training Loss: 0.001137 | Test Loss: 0.004218 | Test Accuracy: 0.795156\n",
      "Step 8 | Training Loss: 0.000422 | Test Loss: 0.004489 | Test Accuracy: 0.780252\n",
      "Step 9 | Training Loss: 0.000237 | Test Loss: 0.004094 | Test Accuracy: 0.792406\n",
      "Step 10 | Training Loss: 0.000133 | Test Loss: 0.005336 | Test Accuracy: 0.766545\n",
      "Step 11 | Training Loss: 0.000481 | Test Loss: 0.006324 | Test Accuracy: 0.754524\n",
      "Step 12 | Training Loss: 0.000084 | Test Loss: 0.004956 | Test Accuracy: 0.751996\n",
      "Step 13 | Training Loss: 0.000319 | Test Loss: 0.005530 | Test Accuracy: 0.778921\n",
      "Step 14 | Training Loss: 0.000069 | Test Loss: 0.005827 | Test Accuracy: 0.771691\n",
      "Step 15 | Training Loss: 0.000419 | Test Loss: 0.005956 | Test Accuracy: 0.777280\n",
      "Step 16 | Training Loss: 0.000025 | Test Loss: 0.006011 | Test Accuracy: 0.785397\n",
      "Step 17 | Training Loss: 0.000936 | Test Loss: 0.005586 | Test Accuracy: 0.808242\n",
      "Step 18 | Training Loss: 0.000068 | Test Loss: 0.005350 | Test Accuracy: 0.759005\n",
      "Step 19 | Training Loss: 0.000026 | Test Loss: 0.004268 | Test Accuracy: 0.711852\n",
      "Step 20 | Training Loss: 0.000110 | Test Loss: 0.004673 | Test Accuracy: 0.724273\n",
      "Step 21 | Training Loss: 0.000221 | Test Loss: 0.004930 | Test Accuracy: 0.719571\n",
      "Step 22 | Training Loss: 0.002699 | Test Loss: 0.006676 | Test Accuracy: 0.688831\n",
      "Step 23 | Training Loss: 0.000680 | Test Loss: 0.004442 | Test Accuracy: 0.719925\n",
      "Step 24 | Training Loss: 0.000503 | Test Loss: 0.004704 | Test Accuracy: 0.723474\n",
      "Step 25 | Training Loss: 0.000193 | Test Loss: 0.004185 | Test Accuracy: 0.724051\n",
      "Step 26 | Training Loss: 0.000488 | Test Loss: 0.003215 | Test Accuracy: 0.734963\n",
      "Step 27 | Training Loss: 0.000170 | Test Loss: 0.002835 | Test Accuracy: 0.733100\n",
      "Step 28 | Training Loss: 0.000320 | Test Loss: 0.003174 | Test Accuracy: 0.730216\n",
      "Step 29 | Training Loss: 0.000890 | Test Loss: 0.003811 | Test Accuracy: 0.732346\n",
      "Step 30 | Training Loss: 0.000259 | Test Loss: 0.003900 | Test Accuracy: 0.728575\n",
      "Step 31 | Training Loss: 0.000090 | Test Loss: 0.004931 | Test Accuracy: 0.756521\n",
      "Step 32 | Training Loss: 0.000067 | Test Loss: 0.004232 | Test Accuracy: 0.742193\n",
      "Step 33 | Training Loss: 0.000169 | Test Loss: 0.005005 | Test Accuracy: 0.734874\n",
      "Step 34 | Training Loss: 0.000426 | Test Loss: 0.003858 | Test Accuracy: 0.746451\n",
      "Step 35 | Training Loss: 0.000008 | Test Loss: 0.004009 | Test Accuracy: 0.732745\n",
      "Step 36 | Training Loss: 0.000376 | Test Loss: 0.004346 | Test Accuracy: 0.729950\n",
      "Step 37 | Training Loss: 0.000504 | Test Loss: 0.005257 | Test Accuracy: 0.731281\n",
      "Step 38 | Training Loss: 0.000182 | Test Loss: 0.004190 | Test Accuracy: 0.734031\n",
      "Step 39 | Training Loss: 0.001000 | Test Loss: 0.004058 | Test Accuracy: 0.733055\n",
      "Step 40 | Training Loss: 0.000827 | Test Loss: 0.004222 | Test Accuracy: 0.718772\n",
      "Step 41 | Training Loss: 0.000283 | Test Loss: 0.004607 | Test Accuracy: 0.645449\n",
      "Step 42 | Training Loss: 0.000391 | Test Loss: 0.007365 | Test Accuracy: 0.685859\n",
      "Step 43 | Training Loss: 0.000317 | Test Loss: 0.004172 | Test Accuracy: 0.674326\n",
      "Step 44 | Training Loss: 0.000179 | Test Loss: 0.003693 | Test Accuracy: 0.741971\n",
      "Step 45 | Training Loss: 0.000256 | Test Loss: 0.003417 | Test Accuracy: 0.738778\n",
      "Step 46 | Training Loss: 0.000712 | Test Loss: 0.003819 | Test Accuracy: 0.735672\n",
      "Step 47 | Training Loss: 0.000118 | Test Loss: 0.004033 | Test Accuracy: 0.720192\n",
      "Step 48 | Training Loss: 0.000054 | Test Loss: 0.003089 | Test Accuracy: 0.723075\n",
      "Step 49 | Training Loss: 0.000280 | Test Loss: 0.003397 | Test Accuracy: 0.735318\n",
      "Step 50 | Training Loss: 0.000237 | Test Loss: 0.003294 | Test Accuracy: 0.731104\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.000842 | Test Loss: 0.003112 | Test Accuracy: 0.820351\n",
      "Step 2 | Training Loss: 0.000187 | Test Loss: 0.005723 | Test Accuracy: 0.758384\n",
      "Step 3 | Training Loss: 0.000361 | Test Loss: 0.006651 | Test Accuracy: 0.797241\n",
      "Step 4 | Training Loss: 0.000196 | Test Loss: 0.005500 | Test Accuracy: 0.805669\n",
      "Step 5 | Training Loss: 0.000410 | Test Loss: 0.004748 | Test Accuracy: 0.819375\n",
      "Step 6 | Training Loss: 0.000728 | Test Loss: 0.006804 | Test Accuracy: 0.776482\n",
      "Step 7 | Training Loss: 0.000082 | Test Loss: 0.003345 | Test Accuracy: 0.849184\n",
      "Step 8 | Training Loss: 0.000702 | Test Loss: 0.001888 | Test Accuracy: 0.873625\n",
      "Step 9 | Training Loss: 0.000011 | Test Loss: 0.002981 | Test Accuracy: 0.872250\n",
      "Step 10 | Training Loss: 0.000055 | Test Loss: 0.004733 | Test Accuracy: 0.854329\n",
      "Step 11 | Training Loss: 0.000278 | Test Loss: 0.002078 | Test Accuracy: 0.868435\n",
      "Step 12 | Training Loss: 0.000142 | Test Loss: 0.002167 | Test Accuracy: 0.865774\n",
      "Step 13 | Training Loss: 0.000128 | Test Loss: 0.002474 | Test Accuracy: 0.865729\n",
      "Step 14 | Training Loss: 0.000182 | Test Loss: 0.002142 | Test Accuracy: 0.869544\n",
      "Step 15 | Training Loss: 0.000092 | Test Loss: 0.001628 | Test Accuracy: 0.882408\n",
      "Step 16 | Training Loss: 0.000032 | Test Loss: 0.002651 | Test Accuracy: 0.867060\n",
      "Step 17 | Training Loss: 0.000151 | Test Loss: 0.001501 | Test Accuracy: 0.889771\n",
      "Step 18 | Training Loss: 0.000622 | Test Loss: 0.001635 | Test Accuracy: 0.886977\n",
      "Step 19 | Training Loss: 0.000244 | Test Loss: 0.001339 | Test Accuracy: 0.887376\n",
      "Step 20 | Training Loss: 0.000251 | Test Loss: 0.002496 | Test Accuracy: 0.860584\n",
      "Step 21 | Training Loss: 0.000219 | Test Loss: 0.003257 | Test Accuracy: 0.835743\n",
      "Step 22 | Training Loss: 0.000334 | Test Loss: 0.013677 | Test Accuracy: 0.873581\n",
      "Step 23 | Training Loss: 0.000502 | Test Loss: 0.002818 | Test Accuracy: 0.852688\n",
      "Step 24 | Training Loss: 0.000014 | Test Loss: 0.002953 | Test Accuracy: 0.851890\n",
      "Step 25 | Training Loss: 0.000573 | Test Loss: 0.002993 | Test Accuracy: 0.856991\n",
      "Step 26 | Training Loss: 0.000070 | Test Loss: 0.003593 | Test Accuracy: 0.853398\n",
      "Step 27 | Training Loss: 0.000801 | Test Loss: 0.003196 | Test Accuracy: 0.852910\n",
      "Step 28 | Training Loss: 0.000374 | Test Loss: 0.002802 | Test Accuracy: 0.855083\n",
      "Step 29 | Training Loss: 0.000511 | Test Loss: 0.003314 | Test Accuracy: 0.855216\n",
      "Step 30 | Training Loss: 0.000094 | Test Loss: 0.002726 | Test Accuracy: 0.853265\n",
      "Step 31 | Training Loss: 0.000119 | Test Loss: 0.003598 | Test Accuracy: 0.838671\n",
      "Step 32 | Training Loss: 0.000450 | Test Loss: 0.003101 | Test Accuracy: 0.774131\n",
      "Step 33 | Training Loss: 0.000299 | Test Loss: 0.002281 | Test Accuracy: 0.798572\n",
      "Step 34 | Training Loss: 0.000650 | Test Loss: 0.002642 | Test Accuracy: 0.848918\n",
      "Step 35 | Training Loss: 0.000144 | Test Loss: 0.003216 | Test Accuracy: 0.862491\n",
      "Step 36 | Training Loss: 0.000810 | Test Loss: 0.002764 | Test Accuracy: 0.857080\n",
      "Step 37 | Training Loss: 0.000358 | Test Loss: 0.002444 | Test Accuracy: 0.851180\n",
      "Step 38 | Training Loss: 0.000031 | Test Loss: 0.002541 | Test Accuracy: 0.851668\n",
      "Step 39 | Training Loss: 0.000492 | Test Loss: 0.002705 | Test Accuracy: 0.862846\n",
      "Step 40 | Training Loss: 0.000056 | Test Loss: 0.003795 | Test Accuracy: 0.863290\n",
      "Step 41 | Training Loss: 0.000087 | Test Loss: 0.002200 | Test Accuracy: 0.864487\n",
      "Step 42 | Training Loss: 0.000183 | Test Loss: 0.002350 | Test Accuracy: 0.864532\n",
      "Step 43 | Training Loss: 0.000268 | Test Loss: 0.001866 | Test Accuracy: 0.858987\n",
      "Step 44 | Training Loss: 0.000357 | Test Loss: 0.003342 | Test Accuracy: 0.862713\n",
      "Step 45 | Training Loss: 0.000253 | Test Loss: 0.002402 | Test Accuracy: 0.861338\n",
      "Step 46 | Training Loss: 0.000005 | Test Loss: 0.002428 | Test Accuracy: 0.853176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47 | Training Loss: 0.000045 | Test Loss: 0.002617 | Test Accuracy: 0.859475\n",
      "Step 48 | Training Loss: 0.000634 | Test Loss: 0.002633 | Test Accuracy: 0.856946\n",
      "Step 49 | Training Loss: 0.003376 | Test Loss: 0.007935 | Test Accuracy: 0.437101\n",
      "Step 50 | Training Loss: 0.000531 | Test Loss: 0.004849 | Test Accuracy: 0.603797\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4]\n",
    "\n",
    "    epochs = [50]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:42:33.819520Z",
     "start_time": "2017-05-14T20:42:33.813444Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:42:33.848463Z",
     "start_time": "2017-05-14T20:42:33.822211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.906731</td>\n",
       "      <td>0.889771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.906890</td>\n",
       "      <td>0.887376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.910859</td>\n",
       "      <td>0.886977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.911811</td>\n",
       "      <td>0.882408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.912050</td>\n",
       "      <td>0.873625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.914828</td>\n",
       "      <td>0.873581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.916812</td>\n",
       "      <td>0.872250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.920067</td>\n",
       "      <td>0.869544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.900778</td>\n",
       "      <td>0.868435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.889268</td>\n",
       "      <td>0.867060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.906334</td>\n",
       "      <td>0.865774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.908716</td>\n",
       "      <td>0.865729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.918003</td>\n",
       "      <td>0.864532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.919749</td>\n",
       "      <td>0.864487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.919194</td>\n",
       "      <td>0.863290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.922448</td>\n",
       "      <td>0.862846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.917923</td>\n",
       "      <td>0.862713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.801953</td>\n",
       "      <td>0.540144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.820924</td>\n",
       "      <td>0.534998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.524051</td>\n",
       "      <td>0.528345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.587712</td>\n",
       "      <td>0.466155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.596603</td>\n",
       "      <td>0.462784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.573583</td>\n",
       "      <td>0.461187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.582473</td>\n",
       "      <td>0.460699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.576599</td>\n",
       "      <td>0.460566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.573742</td>\n",
       "      <td>0.460522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.578108</td>\n",
       "      <td>0.460477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.574139</td>\n",
       "      <td>0.460433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.584299</td>\n",
       "      <td>0.460433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.584696</td>\n",
       "      <td>0.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.574933</td>\n",
       "      <td>0.460167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.582077</td>\n",
       "      <td>0.458259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.543578</td>\n",
       "      <td>0.446815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.537546</td>\n",
       "      <td>0.437101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "366     50              32              4     0.906731    0.889771\n",
       "368     50              32              4     0.906890    0.887376\n",
       "367     50              32              4     0.910859    0.886977\n",
       "364     50              32              4     0.911811    0.882408\n",
       "357     50              32              4     0.912050    0.873625\n",
       "371     50              32              4     0.914828    0.873581\n",
       "358     50              32              4     0.916812    0.872250\n",
       "363     50              32              4     0.920067    0.869544\n",
       "360     50              32              4     0.900778    0.868435\n",
       "365     50              32              4     0.889268    0.867060\n",
       "361     50              32              4     0.906334    0.865774\n",
       "362     50              32              4     0.908716    0.865729\n",
       "391     50              32              4     0.918003    0.864532\n",
       "390     50              32              4     0.919749    0.864487\n",
       "389     50              32              4     0.919194    0.863290\n",
       "388     50              32              4     0.922448    0.862846\n",
       "393     50              32              4     0.917923    0.862713\n",
       "..     ...             ...            ...          ...         ...\n",
       "214     50               4              4     0.801953    0.540144\n",
       "215     50               4              4     0.820924    0.534998\n",
       "232     50               4              4     0.524051    0.528345\n",
       "233     50               4              4     0.587712    0.466155\n",
       "234     50               4              4     0.596603    0.462784\n",
       "248     50               4              4     0.573583    0.461187\n",
       "244     50               4              4     0.582473    0.460699\n",
       "247     50               4              4     0.576599    0.460566\n",
       "245     50               4              4     0.573742    0.460522\n",
       "243     50               4              4     0.578108    0.460477\n",
       "240     50               4              4     0.574139    0.460433\n",
       "246     50               4              4     0.584299    0.460433\n",
       "237     50               4              4     0.584696    0.460300\n",
       "236     50               4              4     0.574933    0.460167\n",
       "249     50               4              4     0.582077    0.458259\n",
       "231     50               4              4     0.543578    0.446815\n",
       "398     50              32              4     0.537546    0.437101\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:42:33.861731Z",
     "start_time": "2017-05-14T20:42:33.850492Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_only_vae_loss_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:42:33.937188Z",
     "start_time": "2017-05-14T20:42:33.864262Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T21:09:42.180135Z",
     "start_time": "2017-05-14T21:09:41.899857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8528  0.1472]\n",
      " [ 0.0614  0.9386]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzPLHQc3MzPLHQc3MzPLHQc3MzPLnf8PoliqejyLv2MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f23b81c80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
