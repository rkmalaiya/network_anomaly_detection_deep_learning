{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:16.092692Z",
     "start_time": "2017-07-19T22:06:15.685906Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:16.105503Z",
     "start_time": "2017-07-19T22:06:16.094258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:16.194849Z",
     "start_time": "2017-07-19T22:06:16.107335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140115': {'x': 'dataset/Kyoto2016/2014/01/20140115_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140115_y.csv'}, '20140107': {'y': 'dataset/Kyoto2016/2014/01/20140107_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140107_x.csv'}, '20140109': {'x': 'dataset/Kyoto2016/2014/01/20140109_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140109_y.csv'}, '20140104': {'y': 'dataset/Kyoto2016/2014/01/20140104_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140104_x.csv'}, '20140119': {'x': 'dataset/Kyoto2016/2014/01/20140119_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140119_y.csv'}, '20140120': {'x': 'dataset/Kyoto2016/2014/01/20140120_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140120_y.csv'}, '20140123': {'x': 'dataset/Kyoto2016/2014/01/20140123_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140123_y.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20151224': {'y': 'dataset/Kyoto2016/2015/12/20151224_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151224_x.csv'}, '20151204': {'y': 'dataset/Kyoto2016/2015/12/20151204_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151204_x.csv'}, '20151216': {'x': 'dataset/Kyoto2016/2015/12/20151216_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151216_y.csv'}, '20151222': {'x': 'dataset/Kyoto2016/2015/12/20151222_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151222_y.csv'}, '20151214': {'x': 'dataset/Kyoto2016/2015/12/20151214_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151214_y.csv'}, '20151202': {'x': 'dataset/Kyoto2016/2015/12/20151202_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151202_y.csv'}, '20151227': {'y': 'dataset/Kyoto2016/2015/12/20151227_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151227_x.csv'}, '20151203': {'y': 'dataset/Kyoto2016/2015/12/20151203_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151203_x.csv'}, '20151223': {'y': 'dataset/Kyoto2016/2015/12/20151223_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151223_x.csv'}, '20151205': {'x': 'dataset/Kyoto2016/2015/12/20151205_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151205_y.csv'}, '20151229': {'x': 'dataset/Kyoto2016/2015/12/20151229_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151229_y.csv'}, '20151208': {'x': 'dataset/Kyoto2016/2015/12/20151208_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151208_y.csv'}, '20151219': {'y': 'dataset/Kyoto2016/2015/12/20151219_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151219_x.csv'}, '20151206': {'y': 'dataset/Kyoto2016/2015/12/20151206_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151206_x.csv'}, '20151225': {'x': 'dataset/Kyoto2016/2015/12/20151225_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151225_y.csv'}, '20151210': {'x': 'dataset/Kyoto2016/2015/12/20151210_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151210_y.csv'}, '20151217': {'x': 'dataset/Kyoto2016/2015/12/20151217_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151217_y.csv'}, '20151207': {'x': 'dataset/Kyoto2016/2015/12/20151207_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151207_y.csv'}, '20151215': {'x': 'dataset/Kyoto2016/2015/12/20151215_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151215_y.csv'}, '20151213': {'y': 'dataset/Kyoto2016/2015/12/20151213_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151213_x.csv'}, '20151209': {'x': 'dataset/Kyoto2016/2015/12/20151209_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151209_y.csv'}, '20151228': {'y': 'dataset/Kyoto2016/2015/12/20151228_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151228_x.csv'}, '20151226': {'y': 'dataset/Kyoto2016/2015/12/20151226_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151226_x.csv'}, '20151218': {'x': 'dataset/Kyoto2016/2015/12/20151218_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151218_y.csv'}, '20151231': {'x': 'dataset/Kyoto2016/2015/12/20151231_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151231_y.csv'}, '20151212': {'x': 'dataset/Kyoto2016/2015/12/20151212_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151212_y.csv'}, '20151211': {'x': 'dataset/Kyoto2016/2015/12/20151211_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151211_y.csv'}, '20151221': {'y': 'dataset/Kyoto2016/2015/12/20151221_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151221_x.csv'}, '20151201': {'x': 'dataset/Kyoto2016/2015/12/20151201_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151201_y.csv'}, '20151220': {'x': 'dataset/Kyoto2016/2015/12/20151220_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151220_y.csv'}, '20151230': {'y': 'dataset/Kyoto2016/2015/12/20151230_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151230_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/12\")\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/11\"))\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/10\"))\n",
    "\n",
    "paths = {}\n",
    "keys = train_paths.keys()\n",
    "for key in list(keys)[0:7]:\n",
    "    paths.update({key: train_paths[key]})\n",
    "train_paths = paths\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "#test_paths = test_paths.popitem()\n",
    "#test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:17.509626Z",
     "start_time": "2017-07-19T22:06:16.196380Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:17.943677Z",
     "start_time": "2017-07-19T22:06:17.511234Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 42\n",
    "    lam = 0.001\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr #1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:18.155039Z",
     "start_time": "2017-07-19T22:06:17.945609Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['key', 'no_of_features','hidden_layers','train_score', 'test_score', 'f1_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 1000\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}\".format(h,f),\n",
    "                    exist_ok = True)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            Train.best_acc = 0\n",
    "            for lr in lrs:\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    \n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_train, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1)\n",
    "                    \n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            def train_batch():\n",
    "                                nonlocal train_loss\n",
    "                                _, train_loss = sess.run([net.train_op, \n",
    "                                                                       net.regularized_loss, \n",
    "                                                                       ], #net.summary_op\n",
    "                                                                      feed_dict={net.x: x_train[i,:], \n",
    "                                                                                 net.y_: y_train[i,:], \n",
    "                                                                                 net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                            train_batch()\n",
    "                            count = 10\n",
    "\n",
    "                            while((train_loss > 1e9 or np.isnan(train_loss)) and epoch > 1 and count > 1):\n",
    "                                print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                                net.saver.restore(sess, \n",
    "                                                  tf.train.latest_checkpoint('dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                             .format(h,f)))\n",
    "                                train_batch()\n",
    "                                count -= 1\n",
    "\n",
    "                        valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], #net.summary_op\n",
    "                                                                  feed_dict={net.x: x_valid, \n",
    "                                                                             net.y_: y_valid, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                        \n",
    "                    end_time = time.perf_counter()    \n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "                        test_accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                          net.actual, net.y], #net.summary_op \n",
    "                                                                                          feed_dict={net.x: x_test, \n",
    "                                                                                         net.y_: y_test, \n",
    "                                                                                         net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        \n",
    "                        f1_score = me.f1_score(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        print(\"Key {} | Training Loss: {:.6f} | Train Accuracy: {:.6f} | Test Accuracy: {:.6f}, f1_score: {}, recall: {}, prec: {}\".format(key, train_loss, valid_accuracy, test_accuracy, f1_score, recall, prec))\n",
    "\n",
    "                        if test_accuracy > Train.best_acc_global:\n",
    "                            Train.best_acc_global = test_accuracy\n",
    "                            Train.pred_value = pred_value\n",
    "                            Train.actual_value = actual_value\n",
    "\n",
    "                            Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(key,f,h):\n",
    "                                                  (curr_pred, \n",
    "                                                   Train.result(key, f, h,valid_accuracy, test_accuracy, f1_score, end_time - start_time))})\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, test_accuracy))\n",
    "                print(\"Best Accuracy on Test data: {}\".format(Train.best_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:06:18.214144Z",
     "start_time": "2017-07-19T22:06:18.157073Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        features_arr = [1, 4, 8, 16, 42]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "\n",
    "        epochs = [1]\n",
    "        lrs = [1e-4]\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "\n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "            \n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl\")\n",
    "\n",
    "        past_scores.append(df_results).to_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:25:16.764099Z",
     "start_time": "2017-07-19T22:06:18.215867Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:1\n",
      "Key 20151224 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.982835, f1_score: 0.9913428743554591, recall: 0.9999186606762647, prec: 0.9829129373474369\n",
      "Key 20151204 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.986732, f1_score: 0.9933217853994942, recall: 0.999960080274938, prec: 0.9867710467131745\n",
      "Key 20151216 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.978833, f1_score: 0.9893033783589055, recall: 0.9999955182474555, prec: 0.9788374643562184\n",
      "Key 20151222 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.986753, f1_score: 0.9933320122130233, recall: 0.9999852225666185, prec: 0.9867667484479182\n",
      "Key 20151214 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.984951, f1_score: 0.9924185020542869, recall: 0.9999745500663819, prec: 0.9849757882906408\n",
      "Key 20151202 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.953475, f1_score: 0.9761834297792135, recall: 0.999952699158045, prec: 0.9535179330733683\n",
      "Key 20151227 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.988185, f1_score: 0.994057453234006, recall: 0.9999600011199686, prec: 0.9882241793687939\n",
      "Key 20151203 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.906209, f1_score: 0.9507971493954476, recall: 0.9999512452408706, prec: 0.906249121298911\n",
      "Key 20151223 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.965192, f1_score: 0.9822875333806068, recall: 0.9999590434600085, prec: 0.9652297671936232\n",
      "Key 20151205 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.986490, f1_score: 0.9931991471563528, recall: 0.9999775805698272, prec: 0.9865119913889073\n",
      "Key 20151229 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.972739, f1_score: 0.986181014168097, recall: 0.9999557515597575, prec: 0.9727806235920393\n",
      "Key 20151208 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.983332, f1_score: 0.9915959470623799, recall: 0.9999808471925177, prec: 0.9833504934582256\n",
      "Key 20151219 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.688822, f1_score: 0.8157387076323644, recall: 0.9999199546409632, prec: 0.6888544749370439\n",
      "Key 20151206 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.985285, f1_score: 0.992588092018421, recall: 0.9999945830873153, prec: 0.9852905071465933\n",
      "Key 20151225 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.985851, f1_score: 0.9928745657476532, recall: 0.999965259856275, prec: 0.985883722978461\n",
      "Key 20151210 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.986623, f1_score: 0.9932663135153182, recall: 0.9999296515013542, prec: 0.9866911940517235\n",
      "Key 20151217 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.966377, f1_score: 0.9829009141784781, recall: 0.9999869897544316, prec: 0.9663889054784229\n",
      "Key 20151207 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.964202, f1_score: 0.9817746965778618, recall: 0.9999782981401506, prec: 0.9642220025006408\n",
      "Key 20151215 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.980801, f1_score: 0.9903076320598729, recall: 0.999992677023796, prec: 0.980808389147259\n",
      "Key 20151213 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.990592, f1_score: 0.9952736336991412, recall: 0.99997247867097, prec: 0.9906187416969048\n",
      "Key 20151209 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.982262, f1_score: 0.9910514684142572, recall: 0.999965660962094, prec: 0.982294802708697\n",
      "Key 20151228 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.988677, f1_score: 0.9943063828157276, recall: 0.9999807386984313, prec: 0.9886960614411909\n",
      "Key 20151226 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.902461, f1_score: 0.9487299011829952, recall: 0.9999811027788363, prec: 0.9024760453766368\n",
      "Key 20151218 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.978825, f1_score: 0.9892990441248579, recall: 0.999988337920418, prec: 0.9788358580381502\n",
      "Key 20151231 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.925382, f1_score: 0.9612443745357626, recall: 0.9999755250745611, prec: 0.9254016275420233\n",
      "Key 20151212 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.978158, f1_score: 0.9889583734985988, recall: 0.9999659329636429, prec: 0.9781905163922893\n",
      "Key 20151211 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.989958, f1_score: 0.9949534167254369, recall: 0.9999791088429283, prec: 0.9899779882111359\n",
      "Key 20151221 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.986680, f1_score: 0.9932952523201923, recall: 0.9999476586774775, prec: 0.9867307746485486\n",
      "Key 20151201 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.969516, f1_score: 0.9845218197395958, recall: 0.9999619578397759, prec: 0.9695512451069441\n",
      "Key 20151220 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.938254, f1_score: 0.9681423315522261, recall: 0.9999312467606647, prec: 0.9383123489446598\n",
      "Key 20151230 | Training Loss: 0.000019 | Train Accuracy: 0.995310 | Test Accuracy: 0.948897, f1_score: 0.9737779800098038, recall: 0.9999729123307585, prec: 0.9489204009939165\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:4\n",
      "Key 20151224 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.931137, f1_score: 0.9642264704085994, recall: 0.9442107932497931, prec: 0.985109123220383\n",
      "Key 20151204 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.900914, f1_score: 0.9477257785401758, recall: 0.9102677161925292, prec: 0.9883989896402662\n",
      "Key 20151216 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.898421, f1_score: 0.9462884395463687, recall: 0.914152030009815, prec: 0.9807666416633009\n",
      "Key 20151222 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.877486, f1_score: 0.9346005355252507, recall: 0.8871632130573401, prec: 0.9873974630455788\n",
      "Key 20151214 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.905433, f1_score: 0.9502237566934588, recall: 0.916409693031384, prec: 0.9866287937601037\n",
      "Key 20151202 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.900148, f1_score: 0.9469557205618735, recall: 0.9347377383317423, prec: 0.9594973361641647\n",
      "Key 20151227 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.923591, f1_score: 0.9601822499881563, recall: 0.9322858959949122, prec: 0.9897995583489043\n",
      "Key 20151203 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.881795, f1_score: 0.935363705451079, recall: 0.9437547369680745, prec: 0.9271205702168792\n",
      "Key 20151223 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.919340, f1_score: 0.9577024635341492, recall: 0.9460602368311924, prec: 0.9696347990345262\n",
      "Key 20151205 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.906603, f1_score: 0.9508977626185793, recall: 0.9167080803362915, prec: 0.987736532732104\n",
      "Key 20151229 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.904971, f1_score: 0.9498513124445144, recall: 0.9251537633298427, prec: 0.9759036613210006\n",
      "Key 20151208 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.931840, f1_score: 0.9646108563214456, recall: 0.9446547706451304, prec: 0.9854282936185517\n",
      "Key 20151219 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.735650, f1_score: 0.835459329596156, recall: 0.9742520761765, prec: 0.7312805107022156\n",
      "Key 20151206 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.926277, f1_score: 0.961615948291433, recall: 0.9372613172848266, prec: 0.987270050668736\n",
      "Key 20151225 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.934409, f1_score: 0.9659898116577367, recall: 0.9448574661531742, prec: 0.9880890595806519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151210 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.904207, f1_score: 0.9495637767170264, recall: 0.9139090706572504, prec: 0.9881134483254034\n",
      "Key 20151217 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.886127, f1_score: 0.9391244732119453, recall: 0.9089087656529516, prec: 0.9714182419889733\n",
      "Key 20151207 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.915807, f1_score: 0.9557555697288262, recall: 0.9431031489398641, prec: 0.9687520898816291\n",
      "Key 20151215 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.903924, f1_score: 0.9492685054376168, recall: 0.9164558259767935, prec: 0.9845180779760219\n",
      "Key 20151213 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.850316, f1_score: 0.919007572882456, recall: 0.8572793915284346, prec: 0.9903149449272681\n",
      "Key 20151209 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.902758, f1_score: 0.9487428519358947, recall: 0.9161741160917024, prec: 0.9837124843300642\n",
      "Key 20151228 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.883746, f1_score: 0.9381717902010687, recall: 0.8921064434556977, prec: 0.9892534799563061\n",
      "Key 20151226 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.816392, f1_score: 0.8967813788794667, recall: 0.8838072861385733, prec: 0.9101420601971977\n",
      "Key 20151218 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.928313, f1_score: 0.9626623859211034, recall: 0.9441328077622801, prec: 0.9819338484117069\n",
      "Key 20151231 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.855197, f1_score: 0.9204093174130628, recall: 0.9047820507889667, prec: 0.9365858955102336\n",
      "Key 20151212 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.876316, f1_score: 0.9338054907120805, recall: 0.8918560856975226, prec: 0.9798959446701795\n",
      "Key 20151211 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.885792, f1_score: 0.9393558589723676, recall: 0.8934730056406124, prec: 0.9902062929777037\n",
      "Key 20151221 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.875055, f1_score: 0.9332014699090344, recall: 0.8845207676092863, prec: 0.9875526607980535\n",
      "Key 20151201 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.921995, f1_score: 0.9590957993351467, recall: 0.943222075865578, prec: 0.9755129541833061\n",
      "Key 20151220 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.878274, f1_score: 0.9341787504427438, recall: 0.92062174083202, prec: 0.9481410067867142\n",
      "Key 20151230 | Training Loss: 0.000009 | Train Accuracy: 0.923411 | Test Accuracy: 0.910996, f1_score: 0.9528767073574576, recall: 0.9483167270872178, prec: 0.9574807528386429\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.017113, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.013250, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.021162, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.013258, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.015028, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.046480, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.011803, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.093747, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.034788, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.013488, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.027239, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.016649, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.311141, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.014715, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.014154, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.013308, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.033611, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.035782, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.019191, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.009403, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.017705, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.011314, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.097530, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.021169, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151231 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.074615, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.021809, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.010022, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.013287, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.030448, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.061712, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Training Loss: 1.882938 | Train Accuracy: 0.003552 | Test Accuracy: 0.051090, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:16\n",
      "Key 20151224 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.017113, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.013250, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.021162, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.013258, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.015028, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.046480, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.011803, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.093747, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.034788, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.013488, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.027239, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.016649, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.311141, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.014715, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.014154, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.013308, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.033611, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.035782, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.019191, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.009403, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.017705, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.011314, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.097530, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.021169, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151231 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.074615, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.021809, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.010022, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.013287, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.030448, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.061712, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Training Loss: 0.835236 | Train Accuracy: 0.003414 | Test Accuracy: 0.051090, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.017113, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.013250, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.021162, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.013258, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.015028, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.046480, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.011803, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.093747, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.034788, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.013488, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.027239, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.016649, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.311141, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.014715, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.014154, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.013308, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.033611, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.035782, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.019191, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.009403, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.017705, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.011314, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.097530, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.021169, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151231 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.074615, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.021809, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.010022, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.013287, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.030448, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.061712, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Training Loss: 1.076506 | Train Accuracy: 0.003759 | Test Accuracy: 0.051090, f1_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:1\n",
      "Key 20151224 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.798302, f1_score: 0.8870887488910009, recall: 0.8061157602116736, prec: 0.9861454175105066\n",
      "Key 20151204 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.777936, f1_score: 0.8745427766808015, recall: 0.7843790486766611, prec: 0.9881271858641736\n",
      "Key 20151216 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.804471, f1_score: 0.8908555294624689, recall: 0.8152218243421908, prec: 0.9819585402720795\n",
      "Key 20151222 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.803908, f1_score: 0.8909645196860608, recall: 0.8119312997122095, prec: 0.987043083431764\n",
      "Key 20151214 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.787374, f1_score: 0.8804365956627048, recall: 0.7948099102041509, prec: 0.9867403896787783\n",
      "Key 20151202 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.779770, f1_score: 0.8733906287567711, recall: 0.7966364801293463, prec: 0.9665118948247079\n",
      "Key 20151227 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.792378, f1_score: 0.8836840578490619, recall: 0.7981016531537117, prec: 0.9898254804496434\n",
      "Key 20151203 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.762333, f1_score: 0.8589148395865374, recall: 0.7982838324786476, prec: 0.9295129176429301\n",
      "Key 20151223 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.791846, f1_score: 0.88241861919197, recall: 0.8092244367195859, prec: 0.9701703237686052\n",
      "Key 20151205 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.793408, f1_score: 0.8842583509732046, recall: 0.7999626342830453, prec: 0.9884118190212373\n",
      "Key 20151229 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.756494, f1_score: 0.8602535679217435, recall: 0.7704759657222083, prec: 0.9737127838539361\n",
      "Key 20151208 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.820500, f1_score: 0.9007615041268388, recall: 0.8284291505729882, prec: 0.9869332745153219\n",
      "Key 20151219 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.734823, f1_score: 0.8170909505819113, recall: 0.8598272354334122, prec: 0.7784017826409899\n",
      "Key 20151206 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.822127, f1_score: 0.901841701445852, recall: 0.8293184982151273, prec: 0.9882646079165484\n",
      "Key 20151225 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.799148, f1_score: 0.8877489636767328, recall: 0.8056338587366498, prec: 0.9885032273779076\n",
      "Key 20151210 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.795006, f1_score: 0.8852890593137499, recall: 0.8016953988173636, prec: 0.988344864802359\n",
      "Key 20151217 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.780438, f1_score: 0.8749803124239344, recall: 0.7950496015612295, prec: 0.9727791529700173\n",
      "Key 20151207 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.798824, f1_score: 0.886379483543107, recall: 0.8138305952820156, prec: 0.9731290222129957\n",
      "Key 20151215 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.787310, f1_score: 0.8800552485068929, recall: 0.79553884289653, prec: 0.9846639112464651\n",
      "Key 20151213 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.729157, f1_score: 0.8429168780877664, recall: 0.7335785233556006, prec: 0.9905574005317586\n",
      "Key 20151209 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.790711, f1_score: 0.8824277623684517, recall: 0.7995544509831696, prec: 0.9844671585311714\n",
      "Key 20151228 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.745050, f1_score: 0.8533647940942228, recall: 0.7503487671391189, prec: 0.9891686012768427\n",
      "Key 20151226 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.729018, f1_score: 0.8360963784996519, recall: 0.7658539811720686, prec: 0.9205248374079149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151218 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.813921, f1_score: 0.8965633209386143, recall: 0.8238851051919578, prec: 0.9833045681040002\n",
      "Key 20151231 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.718285, f1_score: 0.8312390610372317, recall: 0.7497403909694518, prec: 0.9326168647031193\n",
      "Key 20151212 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.739998, f1_score: 0.8494890085393876, recall: 0.7500880065105892, prec: 0.9792596326330927\n",
      "Key 20151211 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.758971, f1_score: 0.8625887465575178, recall: 0.7641776345241292, prec: 0.9900933820544052\n",
      "Key 20151221 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.811662, f1_score: 0.8956815262109463, recall: 0.8194271955995223, prec: 0.987584229390681\n",
      "Key 20151201 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.792123, f1_score: 0.8827024636436953, recall: 0.8067363155216769, prec: 0.9744625122776384\n",
      "Key 20151220 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.783781, f1_score: 0.8754060484055453, recall: 0.8095429496197417, prec: 0.95293531718857\n",
      "Key 20151230 | Training Loss: 0.000050 | Train Accuracy: 0.734887 | Test Accuracy: 0.779466, f1_score: 0.8735740581108925, recall: 0.8029417208796269, prec: 0.9578315848430667\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:4\n",
      "Key 20151224 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.881024, f1_score: 0.9363852840521913, recall: 0.8908856895421553, prec: 0.9867825533944565\n",
      "Key 20151204 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.853871, f1_score: 0.9208438000096988, recall: 0.8613914564530235, prec: 0.9891112296434584\n",
      "Key 20151216 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.870832, f1_score: 0.9304624656964854, recall: 0.8828649154965558, prec: 0.9834846903878701\n",
      "Key 20151222 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.864151, f1_score: 0.9268849692950618, recall: 0.8726517734767237, prec: 0.9883057818390256\n",
      "Key 20151214 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.859559, f1_score: 0.9241280309852861, recall: 0.8683390100824153, prec: 0.9875779094224572\n",
      "Key 20151202 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.855808, f1_score: 0.9207513154781354, recall: 0.8784798369410975, prec: 0.9672965563610021\n",
      "Key 20151227 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.865561, f1_score: 0.9276888144299211, recall: 0.8726635654201682, prec: 0.9901202183808413\n",
      "Key 20151203 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.841303, f1_score: 0.9106351601075753, recall: 0.8922120920667143, prec: 0.9298350963092984\n",
      "Key 20151223 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.865207, f1_score: 0.9268360718174845, recall: 0.8845486333314562, prec: 0.9733697642319935\n",
      "Key 20151205 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.860744, f1_score: 0.9248686424329688, recall: 0.8688425969173283, prec: 0.9886182456707242\n",
      "Key 20151229 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.836640, f1_score: 0.9102797220706408, recall: 0.851907845248455, prec: 0.9772391768711799\n",
      "Key 20151208 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.895378, f1_score: 0.9445152870122822, recall: 0.9055702748427874, prec: 0.9869605755716055\n",
      "Key 20151219 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.768624, f1_score: 0.8465163347293598, recall: 0.9262582129873596, prec: 0.7794161395158258\n",
      "Key 20151206 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.887038, f1_score: 0.9398672049685912, recall: 0.8959736088013998, prec: 0.9882830238283024\n",
      "Key 20151225 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.883663, f1_score: 0.9379679535007487, recall: 0.892176519633144, prec: 0.988714237001023\n",
      "Key 20151210 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.864338, f1_score: 0.9269539506741914, recall: 0.8723878234565344, prec: 0.9888015238432745\n",
      "Key 20151217 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.851026, f1_score: 0.9186961884388879, recall: 0.8709416165230118, prec: 0.9719914188328306\n",
      "Key 20151207 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.862489, f1_score: 0.9251298819669143, recall: 0.8811009353501595, prec: 0.9737905643633223\n",
      "Key 20151215 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.857644, f1_score: 0.9228814779363986, recall: 0.8684573784477487, prec: 0.9845828784438421\n",
      "Key 20151213 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.791065, f1_score: 0.8830688226988129, recall: 0.7964322349821111, prec: 0.9908548732506599\n",
      "Key 20151209 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.858840, f1_score: 0.9236216466551532, recall: 0.8688892608951329, prec: 0.9857128944292949\n",
      "Key 20151228 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.813661, f1_score: 0.8969325799583185, recall: 0.8200774304323062, prec: 0.9896826082047672\n",
      "Key 20151226 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.795097, f1_score: 0.8810940358157847, recall: 0.8412098000988955, prec: 0.9249485735657739\n",
      "Key 20151218 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.883359, f1_score: 0.9376194284528178, recall: 0.8955485842235388, prec: 0.9838379050132602\n",
      "Key 20151231 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.799491, f1_score: 0.8856241660037762, recall: 0.8388745729999615, prec: 0.937891889356246\n",
      "Key 20151212 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.807872, f1_score: 0.8930209735711975, recall: 0.8197853776709503, prec: 0.9806252094143643\n",
      "Key 20151211 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.827056, f1_score: 0.9051119738687113, recall: 0.8331841107828215, prec: 0.9906321522984937\n",
      "Key 20151221 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.868951, f1_score: 0.9296462652934683, recall: 0.8774927554851327, prec: 0.9883909764764524\n",
      "Key 20151201 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.869528, f1_score: 0.9295408540547808, recall: 0.8876615008583263, prec: 0.9755675641776068\n",
      "Key 20151220 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.827608, f1_score: 0.9034517258629314, recall: 0.8596270401201596, prec: 0.9519849125560801\n",
      "Key 20151230 | Training Loss: 0.000068 | Train Accuracy: 0.864168 | Test Accuracy: 0.864871, f1_score: 0.9260694187602689, recall: 0.891893112057173, prec: 0.9629692816128372\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:8\n",
      "Key 20151224 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.941939, f1_score: 0.970039507275357, recall: 0.956287290491433, prec: 0.9841930321309861\n",
      "Key 20151204 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.924323, f1_score: 0.9606091389481728, recall: 0.9351413339720489, prec: 0.9875029700086609\n",
      "Key 20151216 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.925282, f1_score: 0.9610997674939132, recall: 0.9429741806235911, prec: 0.9799358207078351\n",
      "Key 20151222 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.912536, f1_score: 0.9542152397054419, recall: 0.9236745565846396, prec: 0.9868445981149055\n",
      "Key 20151214 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.916521, f1_score: 0.9563629772151014, recall: 0.9287317025581425, prec: 0.985688818460929\n",
      "Key 20151202 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.912424, f1_score: 0.9538763196935309, recall: 0.949719205001849, prec: 0.9580699874635076\n",
      "Key 20151227 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.930168, f1_score: 0.9637682496908395, recall: 0.9398616838728515, prec: 0.9889227454083265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151203 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.887800, f1_score: 0.9391619494834876, recall: 0.9556021434365014, prec: 0.9232778629484665\n",
      "Key 20151223 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.925542, f1_score: 0.961160543979214, recall: 0.9545126427719386, prec: 0.9679016960239218\n",
      "Key 20151205 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.923272, f1_score: 0.9600325653467076, recall: 0.934113031293788, prec: 0.9874315688024836\n",
      "Key 20151229 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.903300, f1_score: 0.9489995327306125, recall: 0.924873523208307, prec: 0.9744179451219275\n",
      "Key 20151208 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.944766, f1_score: 0.9715424275945944, recall: 0.9588023111054362, prec: 0.9846256728316953\n",
      "Key 20151219 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.728877, f1_score: 0.8311634055752359, recall: 0.9687823099756528, prec: 0.7277797543583602\n",
      "Key 20151206 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.943036, f1_score: 0.9706397739870544, recall: 0.9556679865877242, prec: 0.9860881327132892\n",
      "Key 20151225 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.948525, f1_score: 0.9735382665187767, recall: 0.9604954937070711, prec: 0.986940136769048\n",
      "Key 20151210 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.923519, f1_score: 0.9601695000351451, recall: 0.9342710527652822, prec: 0.9875447192088108\n",
      "Key 20151217 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.908972, f1_score: 0.951955618670949, recall: 0.9331858838835583, prec: 0.9714959062188903\n",
      "Key 20151207 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.927243, f1_score: 0.9620501850012552, recall: 0.9564443672822761, prec: 0.9677221026744544\n",
      "Key 20151215 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.918522, f1_score: 0.957387985695826, recall: 0.9332107955315199, prec: 0.9828512372791812\n",
      "Key 20151213 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.862322, f1_score: 0.9260021499618362, recall: 0.8696239585678901, prec: 0.9901971676908219\n",
      "Key 20151209 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.924607, f1_score: 0.9607481148953453, recall: 0.9393100428808736, prec: 0.9831876140070269\n",
      "Key 20151228 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.884418, f1_score: 0.938572800037013, recall: 0.8931217892098189, prec: 0.988897859699901\n",
      "Key 20151226 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.829472, f1_score: 0.9051217942331383, recall: 0.9013061129360928, prec: 0.9089699202744338\n",
      "Key 20151218 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.941001, f1_score: 0.969531731270872, recall: 0.9590136213089518, prec: 0.9802831172701535\n",
      "Key 20151231 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.866783, f1_score: 0.927612366711499, recall: 0.9223760257616073, prec: 0.9329085006612962\n",
      "Key 20151212 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.885639, f1_score: 0.9391276927018664, recall: 0.9018377273501523, prec: 0.9796344647519583\n",
      "Key 20151211 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.906687, f1_score: 0.9510104872271746, recall: 0.914892410541081, prec: 0.9900975069359439\n",
      "Key 20151221 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.919681, f1_score: 0.9581056823163221, recall: 0.9307952550211982, prec: 0.9870671820282776\n",
      "Key 20151201 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.938385, f1_score: 0.9680166187219092, recall: 0.9617010551944192, prec: 0.9744156801526387\n",
      "Key 20151220 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.891533, f1_score: 0.9419497737268149, recall: 0.9378946700373383, prec: 0.9460400951700151\n",
      "Key 20151230 | Training Loss: 0.000105 | Train Accuracy: 0.948895 | Test Accuracy: 0.918523, f1_score: 0.9572278982321284, recall: 0.960799627996009, prec: 0.9536826256071985\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:16\n",
      "Key 20151224 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.952657, f1_score: 0.9757161293435129, recall: 0.967670011148272, prec: 0.9838971754380844\n",
      "Key 20151204 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.948122, f1_score: 0.9733298232113565, recall: 0.9593508326891741, prec: 0.9877222218070677\n",
      "Key 20151216 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.940623, f1_score: 0.9693551564166251, recall: 0.959404285451783, prec: 0.9795146101964803\n",
      "Key 20151222 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.936377, f1_score: 0.9671071106160749, recall: 0.9478762981051636, prec: 0.9871344039273925\n",
      "Key 20151214 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.939679, f1_score: 0.9688511291951086, recall: 0.9524171074453781, prec: 0.9858622491317577\n",
      "Key 20151202 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.929813, f1_score: 0.9634669478105352, recall: 0.9706218770694118, prec: 0.9564167316362158\n",
      "Key 20151227 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.953315, f1_score: 0.9760656568215209, recall: 0.963301027571228, prec: 0.98917311515271\n",
      "Key 20151203 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.890797, f1_score: 0.9415125797325933, recall: 0.9698828556105648, prec: 0.9147548658952579\n",
      "Key 20151223 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.942313, f1_score: 0.9701941440199352, recall: 0.9727024660956642, prec: 0.9676987251641294\n",
      "Key 20151205 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.950074, f1_score: 0.9743660596333574, recall: 0.9618271835590846, prec: 0.9872361805190671\n",
      "Key 20151229 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.931307, f1_score: 0.9642847152157819, recall: 0.9532994586940811, prec: 0.9755260983257678\n",
      "Key 20151208 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.947755, f1_score: 0.9731350799943185, recall: 0.962268969259744, prec: 0.984249397598229\n",
      "Key 20151219 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.713304, f1_score: 0.8235197614986663, recall: 0.9710435913684421, prec: 0.7149087051751741\n",
      "Key 20151206 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.953251, f1_score: 0.976032748038757, recall: 0.966100960418619, prec: 0.9861708598285872\n",
      "Key 20151225 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.958501, f1_score: 0.9787842860644627, recall: 0.9710267201334022, prec: 0.986666801141693\n",
      "Key 20151210 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.942396, f1_score: 0.9702992345163536, recall: 0.9536364311424987, prec: 0.9875546885434331\n",
      "Key 20151217 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.920482, f1_score: 0.9584458504983607, recall: 0.9489315335826963, prec: 0.9681528873786279\n",
      "Key 20151207 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.938741, f1_score: 0.968281570807127, recall: 0.9697367564400269, prec: 0.9668307459295721\n",
      "Key 20151215 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.932611, f1_score: 0.9650574926679392, recall: 0.9488014118698122, prec: 0.9818803237442784\n",
      "Key 20151213 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.883358, f1_score: 0.9380055825667951, recall: 0.8907978683479697, prec: 0.9904968299583541\n",
      "Key 20151209 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.939090, f1_score: 0.9685261359873112, recall: 0.9540629520412411, prec: 0.9834345811966568\n",
      "Key 20151228 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.919493, f1_score: 0.9579893727720938, recall: 0.9284277549852376, prec: 0.9894954163416365\n",
      "Key 20151226 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.854428, f1_score: 0.9204801468202151, recall: 0.9335825666835692, prec: 0.9077404102331678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151218 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.949693, f1_score: 0.9741310996190398, recall: 0.9676727153986099, prec: 0.980676271406791\n",
      "Key 20151231 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.896909, f1_score: 0.9451566706370791, recall: 0.9599450363103001, prec: 0.9308170343473795\n",
      "Key 20151212 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.914224, f1_score: 0.9550618031145977, recall: 0.9318129341181369, prec: 0.9795004834416269\n",
      "Key 20151211 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.929753, f1_score: 0.9635605689063267, recall: 0.9381681439699168, prec: 0.9903657729750165\n",
      "Key 20151221 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.941227, f1_score: 0.9696894855106152, recall: 0.952788127084731, prec: 0.9872012936750924\n",
      "Key 20151201 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.944527, f1_score: 0.9714001835028119, recall: 0.9716681011731251, prec: 0.9711324135374437\n",
      "Key 20151220 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.908356, f1_score: 0.9515512112197195, recall: 0.9591552871241049, prec: 0.9440667555074335\n",
      "Key 20151230 | Training Loss: 0.000024 | Train Accuracy: 0.973275 | Test Accuracy: 0.929743, f1_score: 0.9634078860468022, recall: 0.9746685146476571, prec: 0.9524044803049219\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:42\n",
      "Key 20151224 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.972663, f1_score: 0.9861338053198672, recall: 0.9890096219635313, prec: 0.9832746646370469\n",
      "Key 20151204 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.972559, f1_score: 0.9860798009064569, recall: 0.984986554310786, prec: 0.9871754770096965\n",
      "Key 20151216 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.969836, f1_score: 0.9846759347991781, recall: 0.9900863633715328, prec: 0.979324316536557\n",
      "Key 20151222 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.970082, f1_score: 0.9848068880870972, recall: 0.9826475988517934, prec: 0.9869756879508416\n",
      "Key 20151214 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.970625, f1_score: 0.9850830499233043, recall: 0.9847257981735431, prec: 0.9854405609840993\n",
      "Key 20151202 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.943786, f1_score: 0.9710111219182138, recall: 0.9873620750449358, prec: 0.9551928980889071\n",
      "Key 20151227 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.975995, f1_score: 0.9878442997512015, recall: 0.9870283632058302, prec: 0.9886615864163402\n",
      "Key 20151203 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.897477, f1_score: 0.9457328740910831, recall: 0.985768042585066, prec: 0.9088227001360734\n",
      "Key 20151223 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.953703, f1_score: 0.9762796314722124, recall: 0.9870730920651823, prec: 0.9657196666132393\n",
      "Key 20151205 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.972497, f1_score: 0.9860483380861109, recall: 0.9851732835123774, prec: 0.9869249485307879\n",
      "Key 20151229 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.954030, f1_score: 0.9764509597115903, recall: 0.9797526512190445, prec: 0.9731714463612058\n",
      "Key 20151208 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.970550, f1_score: 0.9850462701744654, recall: 0.9864015066875219, prec: 0.9836947525244165\n",
      "Key 20151219 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.703388, f1_score: 0.8220004577454108, recall: 0.9942167228095921, prec: 0.7006374217325085\n",
      "Key 20151206 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.974088, f1_score: 0.9868657055591477, recall: 0.9880123722285721, prec: 0.985721697399425\n",
      "Key 20151225 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.976603, f1_score: 0.9881571517727123, recall: 0.9901089847937428, prec: 0.9862129990311036\n",
      "Key 20151210 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.974190, f1_score: 0.9869166697617322, recall: 0.9865868862582025, prec: 0.9872466738105108\n",
      "Key 20151217 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.953961, f1_score: 0.9764037033636039, recall: 0.9856659619450318, prec: 0.9673138982964284\n",
      "Key 20151207 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.952217, f1_score: 0.97548630195809, recall: 0.9860077258621064, prec: 0.9651870499012173\n",
      "Key 20151215 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.970383, f1_score: 0.9849561929153465, recall: 0.9885139118240436, prec: 0.9814239910428012\n",
      "Key 20151213 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.957027, f1_score: 0.9780350218458993, recall: 0.9658160074057395, prec: 0.9905671755157133\n",
      "Key 20151209 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.969364, f1_score: 0.984429844298443, recall: 0.9859510411167055, prec: 0.9829133342747475\n",
      "Key 20151228 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.964117, f1_score: 0.9817229988138655, recall: 0.9747346755708912, prec: 0.9888122506971408\n",
      "Key 20151226 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.882249, f1_score: 0.9372201182947731, recall: 0.9739155357204723, prec: 0.9031895317930894\n",
      "Key 20151218 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.967969, f1_score: 0.9837136008265086, recall: 0.9882737789802678, prec: 0.9791953133124573\n",
      "Key 20151231 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.898013, f1_score: 0.9459322699664318, recall: 0.964084795127392, prec: 0.9284506909463136\n",
      "Key 20151212 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.950640, f1_score: 0.9746729868624787, recall: 0.9709559588924428, prec: 0.9784185833619408\n",
      "Key 20151211 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.963322, f1_score: 0.9813127724656182, recall: 0.9727668845315904, prec: 0.9900101447602617\n",
      "Key 20151221 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.972351, f1_score: 0.9859766682621213, recall: 0.9850969979872383, prec: 0.9868579109941654\n",
      "Key 20151201 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.959340, f1_score: 0.9792335315241066, recall: 0.9887537863837598, prec: 0.9698948605759812\n",
      "Key 20151220 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.924726, f1_score: 0.9607548399949292, recall: 0.9819813625834294, prec: 0.9404265664490445\n",
      "Key 20151230 | Training Loss: 0.000070 | Train Accuracy: 0.986448 | Test Accuracy: 0.941674, f1_score: 0.9699234998751876, recall: 0.9911017006541673, prec: 0.9496314496314496\n",
      "Best Accuracy on Test data: 0\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 1\n",
    "\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:26:04.162629Z",
     "start_time": "2017-07-19T22:25:16.765648Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:26:04.237052Z",
     "start_time": "2017-07-19T22:26:04.164204Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Positive\", \"\\n False Negative \\n Type II Error\"],\n",
    "             [\"\\n False Positive \\n Type I Error\", \"\\n True Negative\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:26:04.502302Z",
     "start_time": "2017-07-19T22:26:04.238418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:26:04.564026Z",
     "start_time": "2017-07-19T22:26:04.503766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.991343</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986732</td>\n",
       "      <td>0.993322</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978833</td>\n",
       "      <td>0.989303</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986753</td>\n",
       "      <td>0.993332</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.984951</td>\n",
       "      <td>0.992419</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.953475</td>\n",
       "      <td>0.976183</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.988185</td>\n",
       "      <td>0.994057</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.906209</td>\n",
       "      <td>0.950797</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.965192</td>\n",
       "      <td>0.982288</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986490</td>\n",
       "      <td>0.993199</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.972739</td>\n",
       "      <td>0.986181</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.983332</td>\n",
       "      <td>0.991596</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.688822</td>\n",
       "      <td>0.815739</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.985285</td>\n",
       "      <td>0.992588</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.985851</td>\n",
       "      <td>0.992875</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986623</td>\n",
       "      <td>0.993266</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.966377</td>\n",
       "      <td>0.982901</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.964202</td>\n",
       "      <td>0.981775</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.980801</td>\n",
       "      <td>0.990308</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.982262</td>\n",
       "      <td>0.991051</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.988677</td>\n",
       "      <td>0.994306</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.902461</td>\n",
       "      <td>0.948730</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978825</td>\n",
       "      <td>0.989299</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.925382</td>\n",
       "      <td>0.961244</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978158</td>\n",
       "      <td>0.988958</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.989958</td>\n",
       "      <td>0.994953</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986680</td>\n",
       "      <td>0.993295</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.969516</td>\n",
       "      <td>0.984522</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.938254</td>\n",
       "      <td>0.968142</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.948897</td>\n",
       "      <td>0.973778</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20151224</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.931137</td>\n",
       "      <td>0.964226</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20151204</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.900914</td>\n",
       "      <td>0.947726</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20151216</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.898421</td>\n",
       "      <td>0.946288</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20151222</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.877486</td>\n",
       "      <td>0.934601</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>20151214</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.905433</td>\n",
       "      <td>0.950224</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20151202</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.900148</td>\n",
       "      <td>0.946956</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20151227</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.923591</td>\n",
       "      <td>0.960182</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.881795</td>\n",
       "      <td>0.935364</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20151223</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.919340</td>\n",
       "      <td>0.957702</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20151205</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.906603</td>\n",
       "      <td>0.950898</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20151229</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.904971</td>\n",
       "      <td>0.949851</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20151208</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.931840</td>\n",
       "      <td>0.964611</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20151219</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.735650</td>\n",
       "      <td>0.835459</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20151206</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.926277</td>\n",
       "      <td>0.961616</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>20151225</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.934409</td>\n",
       "      <td>0.965990</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20151210</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.904207</td>\n",
       "      <td>0.949564</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20151217</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.886127</td>\n",
       "      <td>0.939124</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20151207</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.915807</td>\n",
       "      <td>0.955756</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20151215</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.903924</td>\n",
       "      <td>0.949269</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>20151219</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.713304</td>\n",
       "      <td>0.823520</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>20151206</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.953251</td>\n",
       "      <td>0.976033</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>20151225</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.958501</td>\n",
       "      <td>0.978784</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>20151210</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.942396</td>\n",
       "      <td>0.970299</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>20151217</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.920482</td>\n",
       "      <td>0.958446</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>20151207</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.938741</td>\n",
       "      <td>0.968282</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>20151215</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.932611</td>\n",
       "      <td>0.965057</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>20151213</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.883358</td>\n",
       "      <td>0.938006</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>20151209</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.939090</td>\n",
       "      <td>0.968526</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>20151228</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.919493</td>\n",
       "      <td>0.957989</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>20151226</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.854428</td>\n",
       "      <td>0.920480</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>20151218</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.949693</td>\n",
       "      <td>0.974131</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>20151231</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.896909</td>\n",
       "      <td>0.945157</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>20151212</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.914224</td>\n",
       "      <td>0.955062</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>20151211</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.929753</td>\n",
       "      <td>0.963561</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>20151221</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.941227</td>\n",
       "      <td>0.969689</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>20151201</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.944527</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>20151220</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.908356</td>\n",
       "      <td>0.951551</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.929743</td>\n",
       "      <td>0.963408</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>20151224</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.972663</td>\n",
       "      <td>0.986134</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>20151204</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.972559</td>\n",
       "      <td>0.986080</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>20151216</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.969836</td>\n",
       "      <td>0.984676</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>20151222</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.970082</td>\n",
       "      <td>0.984807</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>20151214</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.970625</td>\n",
       "      <td>0.985083</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>20151202</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.943786</td>\n",
       "      <td>0.971011</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.975995</td>\n",
       "      <td>0.987844</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>20151203</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.897477</td>\n",
       "      <td>0.945733</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>20151223</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.953703</td>\n",
       "      <td>0.976280</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>20151205</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.972497</td>\n",
       "      <td>0.986048</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>20151229</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.954030</td>\n",
       "      <td>0.976451</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>20151208</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.970550</td>\n",
       "      <td>0.985046</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>20151219</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.703388</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>20151206</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.974088</td>\n",
       "      <td>0.986866</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>20151225</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.976603</td>\n",
       "      <td>0.988157</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>20151210</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.974190</td>\n",
       "      <td>0.986917</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>20151217</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.953961</td>\n",
       "      <td>0.976404</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>20151207</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.952217</td>\n",
       "      <td>0.975486</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>20151215</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.970383</td>\n",
       "      <td>0.984956</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>20151213</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.957027</td>\n",
       "      <td>0.978035</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>20151209</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.969364</td>\n",
       "      <td>0.984430</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.964117</td>\n",
       "      <td>0.981723</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>20151226</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.882249</td>\n",
       "      <td>0.937220</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>20151218</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.967969</td>\n",
       "      <td>0.983714</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>20151231</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.898013</td>\n",
       "      <td>0.945932</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>20151212</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.950640</td>\n",
       "      <td>0.974673</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>20151211</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.981313</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>20151221</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.972351</td>\n",
       "      <td>0.985977</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>20151201</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.959340</td>\n",
       "      <td>0.979234</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>20151220</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.924726</td>\n",
       "      <td>0.960755</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>20151230</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.941674</td>\n",
       "      <td>0.969923</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "0    20151224               1              1     0.995310    0.982835   \n",
       "1    20151204               1              1     0.995310    0.986732   \n",
       "2    20151216               1              1     0.995310    0.978833   \n",
       "3    20151222               1              1     0.995310    0.986753   \n",
       "4    20151214               1              1     0.995310    0.984951   \n",
       "5    20151202               1              1     0.995310    0.953475   \n",
       "6    20151227               1              1     0.995310    0.988185   \n",
       "7    20151203               1              1     0.995310    0.906209   \n",
       "8    20151223               1              1     0.995310    0.965192   \n",
       "9    20151205               1              1     0.995310    0.986490   \n",
       "10   20151229               1              1     0.995310    0.972739   \n",
       "11   20151208               1              1     0.995310    0.983332   \n",
       "12   20151219               1              1     0.995310    0.688822   \n",
       "13   20151206               1              1     0.995310    0.985285   \n",
       "14   20151225               1              1     0.995310    0.985851   \n",
       "15   20151210               1              1     0.995310    0.986623   \n",
       "16   20151217               1              1     0.995310    0.966377   \n",
       "17   20151207               1              1     0.995310    0.964202   \n",
       "18   20151215               1              1     0.995310    0.980801   \n",
       "19   20151213               1              1     0.995310    0.990592   \n",
       "20   20151209               1              1     0.995310    0.982262   \n",
       "21   20151228               1              1     0.995310    0.988677   \n",
       "22   20151226               1              1     0.995310    0.902461   \n",
       "23   20151218               1              1     0.995310    0.978825   \n",
       "24   20151231               1              1     0.995310    0.925382   \n",
       "25   20151212               1              1     0.995310    0.978158   \n",
       "26   20151211               1              1     0.995310    0.989958   \n",
       "27   20151221               1              1     0.995310    0.986680   \n",
       "28   20151201               1              1     0.995310    0.969516   \n",
       "29   20151220               1              1     0.995310    0.938254   \n",
       "30   20151230               1              1     0.995310    0.948897   \n",
       "31   20151224               4              1     0.923411    0.931137   \n",
       "32   20151204               4              1     0.923411    0.900914   \n",
       "33   20151216               4              1     0.923411    0.898421   \n",
       "34   20151222               4              1     0.923411    0.877486   \n",
       "35   20151214               4              1     0.923411    0.905433   \n",
       "36   20151202               4              1     0.923411    0.900148   \n",
       "37   20151227               4              1     0.923411    0.923591   \n",
       "38   20151203               4              1     0.923411    0.881795   \n",
       "39   20151223               4              1     0.923411    0.919340   \n",
       "40   20151205               4              1     0.923411    0.906603   \n",
       "41   20151229               4              1     0.923411    0.904971   \n",
       "42   20151208               4              1     0.923411    0.931840   \n",
       "43   20151219               4              1     0.923411    0.735650   \n",
       "44   20151206               4              1     0.923411    0.926277   \n",
       "45   20151225               4              1     0.923411    0.934409   \n",
       "46   20151210               4              1     0.923411    0.904207   \n",
       "47   20151217               4              1     0.923411    0.886127   \n",
       "48   20151207               4              1     0.923411    0.915807   \n",
       "49   20151215               4              1     0.923411    0.903924   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "260  20151219              16              3     0.973275    0.713304   \n",
       "261  20151206              16              3     0.973275    0.953251   \n",
       "262  20151225              16              3     0.973275    0.958501   \n",
       "263  20151210              16              3     0.973275    0.942396   \n",
       "264  20151217              16              3     0.973275    0.920482   \n",
       "265  20151207              16              3     0.973275    0.938741   \n",
       "266  20151215              16              3     0.973275    0.932611   \n",
       "267  20151213              16              3     0.973275    0.883358   \n",
       "268  20151209              16              3     0.973275    0.939090   \n",
       "269  20151228              16              3     0.973275    0.919493   \n",
       "270  20151226              16              3     0.973275    0.854428   \n",
       "271  20151218              16              3     0.973275    0.949693   \n",
       "272  20151231              16              3     0.973275    0.896909   \n",
       "273  20151212              16              3     0.973275    0.914224   \n",
       "274  20151211              16              3     0.973275    0.929753   \n",
       "275  20151221              16              3     0.973275    0.941227   \n",
       "276  20151201              16              3     0.973275    0.944527   \n",
       "277  20151220              16              3     0.973275    0.908356   \n",
       "278  20151230              16              3     0.973275    0.929743   \n",
       "279  20151224              42              3     0.986448    0.972663   \n",
       "280  20151204              42              3     0.986448    0.972559   \n",
       "281  20151216              42              3     0.986448    0.969836   \n",
       "282  20151222              42              3     0.986448    0.970082   \n",
       "283  20151214              42              3     0.986448    0.970625   \n",
       "284  20151202              42              3     0.986448    0.943786   \n",
       "285  20151227              42              3     0.986448    0.975995   \n",
       "286  20151203              42              3     0.986448    0.897477   \n",
       "287  20151223              42              3     0.986448    0.953703   \n",
       "288  20151205              42              3     0.986448    0.972497   \n",
       "289  20151229              42              3     0.986448    0.954030   \n",
       "290  20151208              42              3     0.986448    0.970550   \n",
       "291  20151219              42              3     0.986448    0.703388   \n",
       "292  20151206              42              3     0.986448    0.974088   \n",
       "293  20151225              42              3     0.986448    0.976603   \n",
       "294  20151210              42              3     0.986448    0.974190   \n",
       "295  20151217              42              3     0.986448    0.953961   \n",
       "296  20151207              42              3     0.986448    0.952217   \n",
       "297  20151215              42              3     0.986448    0.970383   \n",
       "298  20151213              42              3     0.986448    0.957027   \n",
       "299  20151209              42              3     0.986448    0.969364   \n",
       "300  20151228              42              3     0.986448    0.964117   \n",
       "301  20151226              42              3     0.986448    0.882249   \n",
       "302  20151218              42              3     0.986448    0.967969   \n",
       "303  20151231              42              3     0.986448    0.898013   \n",
       "304  20151212              42              3     0.986448    0.950640   \n",
       "305  20151211              42              3     0.986448    0.963322   \n",
       "306  20151221              42              3     0.986448    0.972351   \n",
       "307  20151201              42              3     0.986448    0.959340   \n",
       "308  20151220              42              3     0.986448    0.924726   \n",
       "309  20151230              42              3     0.986448    0.941674   \n",
       "\n",
       "     f1_score  time_taken  \n",
       "0    0.991343   30.252922  \n",
       "1    0.993322   30.252922  \n",
       "2    0.989303   30.252922  \n",
       "3    0.993332   30.252922  \n",
       "4    0.992419   30.252922  \n",
       "5    0.976183   30.252922  \n",
       "6    0.994057   30.252922  \n",
       "7    0.950797   30.252922  \n",
       "8    0.982288   30.252922  \n",
       "9    0.993199   30.252922  \n",
       "10   0.986181   30.252922  \n",
       "11   0.991596   30.252922  \n",
       "12   0.815739   30.252922  \n",
       "13   0.992588   30.252922  \n",
       "14   0.992875   30.252922  \n",
       "15   0.993266   30.252922  \n",
       "16   0.982901   30.252922  \n",
       "17   0.981775   30.252922  \n",
       "18   0.990308   30.252922  \n",
       "19   0.995274   30.252922  \n",
       "20   0.991051   30.252922  \n",
       "21   0.994306   30.252922  \n",
       "22   0.948730   30.252922  \n",
       "23   0.989299   30.252922  \n",
       "24   0.961244   30.252922  \n",
       "25   0.988958   30.252922  \n",
       "26   0.994953   30.252922  \n",
       "27   0.993295   30.252922  \n",
       "28   0.984522   30.252922  \n",
       "29   0.968142   30.252922  \n",
       "30   0.973778   30.252922  \n",
       "31   0.964226   32.132440  \n",
       "32   0.947726   32.132440  \n",
       "33   0.946288   32.132440  \n",
       "34   0.934601   32.132440  \n",
       "35   0.950224   32.132440  \n",
       "36   0.946956   32.132440  \n",
       "37   0.960182   32.132440  \n",
       "38   0.935364   32.132440  \n",
       "39   0.957702   32.132440  \n",
       "40   0.950898   32.132440  \n",
       "41   0.949851   32.132440  \n",
       "42   0.964611   32.132440  \n",
       "43   0.835459   32.132440  \n",
       "44   0.961616   32.132440  \n",
       "45   0.965990   32.132440  \n",
       "46   0.949564   32.132440  \n",
       "47   0.939124   32.132440  \n",
       "48   0.955756   32.132440  \n",
       "49   0.949269   32.132440  \n",
       "..        ...         ...  \n",
       "260  0.823520   42.956834  \n",
       "261  0.976033   42.956834  \n",
       "262  0.978784   42.956834  \n",
       "263  0.970299   42.956834  \n",
       "264  0.958446   42.956834  \n",
       "265  0.968282   42.956834  \n",
       "266  0.965057   42.956834  \n",
       "267  0.938006   42.956834  \n",
       "268  0.968526   42.956834  \n",
       "269  0.957989   42.956834  \n",
       "270  0.920480   42.956834  \n",
       "271  0.974131   42.956834  \n",
       "272  0.945157   42.956834  \n",
       "273  0.955062   42.956834  \n",
       "274  0.963561   42.956834  \n",
       "275  0.969689   42.956834  \n",
       "276  0.971400   42.956834  \n",
       "277  0.951551   42.956834  \n",
       "278  0.963408   42.956834  \n",
       "279  0.986134   44.915621  \n",
       "280  0.986080   44.915621  \n",
       "281  0.984676   44.915621  \n",
       "282  0.984807   44.915621  \n",
       "283  0.985083   44.915621  \n",
       "284  0.971011   44.915621  \n",
       "285  0.987844   44.915621  \n",
       "286  0.945733   44.915621  \n",
       "287  0.976280   44.915621  \n",
       "288  0.986048   44.915621  \n",
       "289  0.976451   44.915621  \n",
       "290  0.985046   44.915621  \n",
       "291  0.822000   44.915621  \n",
       "292  0.986866   44.915621  \n",
       "293  0.988157   44.915621  \n",
       "294  0.986917   44.915621  \n",
       "295  0.976404   44.915621  \n",
       "296  0.975486   44.915621  \n",
       "297  0.984956   44.915621  \n",
       "298  0.978035   44.915621  \n",
       "299  0.984430   44.915621  \n",
       "300  0.981723   44.915621  \n",
       "301  0.937220   44.915621  \n",
       "302  0.983714   44.915621  \n",
       "303  0.945932   44.915621  \n",
       "304  0.974673   44.915621  \n",
       "305  0.981313   44.915621  \n",
       "306  0.985977   44.915621  \n",
       "307  0.979234   44.915621  \n",
       "308  0.960755   44.915621  \n",
       "309  0.969923   44.915621  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:26:04.605685Z",
     "start_time": "2017-07-19T22:26:04.565492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.989958</td>\n",
       "      <td>0.994953</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.989958</td>\n",
       "      <td>0.994953</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.988677</td>\n",
       "      <td>0.994306</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.988677</td>\n",
       "      <td>0.994306</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.988185</td>\n",
       "      <td>0.994057</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.988185</td>\n",
       "      <td>0.994057</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986753</td>\n",
       "      <td>0.993332</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986753</td>\n",
       "      <td>0.993332</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986732</td>\n",
       "      <td>0.993322</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986732</td>\n",
       "      <td>0.993322</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986680</td>\n",
       "      <td>0.993295</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986680</td>\n",
       "      <td>0.993295</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986623</td>\n",
       "      <td>0.993266</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986623</td>\n",
       "      <td>0.993266</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986490</td>\n",
       "      <td>0.993199</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.986490</td>\n",
       "      <td>0.993199</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.985851</td>\n",
       "      <td>0.992875</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.985851</td>\n",
       "      <td>0.992875</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.985285</td>\n",
       "      <td>0.992588</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.985285</td>\n",
       "      <td>0.992588</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.984951</td>\n",
       "      <td>0.992419</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.984951</td>\n",
       "      <td>0.992419</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.983332</td>\n",
       "      <td>0.991596</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.983332</td>\n",
       "      <td>0.991596</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.991343</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.982835</td>\n",
       "      <td>0.991343</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.982262</td>\n",
       "      <td>0.991051</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.982262</td>\n",
       "      <td>0.991051</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.980801</td>\n",
       "      <td>0.990308</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.980801</td>\n",
       "      <td>0.990308</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978833</td>\n",
       "      <td>0.989303</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978833</td>\n",
       "      <td>0.989303</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978825</td>\n",
       "      <td>0.989299</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978825</td>\n",
       "      <td>0.989299</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978158</td>\n",
       "      <td>0.988958</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.978158</td>\n",
       "      <td>0.988958</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>20151225</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.976603</td>\n",
       "      <td>0.988157</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>20151225</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.976603</td>\n",
       "      <td>0.988157</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.975995</td>\n",
       "      <td>0.987844</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.975995</td>\n",
       "      <td>0.987844</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>20151210</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.974190</td>\n",
       "      <td>0.986917</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>20151210</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.974190</td>\n",
       "      <td>0.986917</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>20151206</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.974088</td>\n",
       "      <td>0.986866</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>20151206</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.974088</td>\n",
       "      <td>0.986866</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.972739</td>\n",
       "      <td>0.986181</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.972739</td>\n",
       "      <td>0.986181</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>20151224</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.972663</td>\n",
       "      <td>0.986134</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>20151224</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.972663</td>\n",
       "      <td>0.986134</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>20151223</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>20151203</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>20151202</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>20151214</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>20151222</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>20151216</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>20151204</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>20151229</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>20151205</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>20151223</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>20151203</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>20151207</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20151217</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.033611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>20151210</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20151225</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>20151206</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.014715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.311141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>20151208</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.016649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>20151229</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>20151205</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>20151223</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.093747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>20151227</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>20151208</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.016649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>20151214</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>20151222</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>20151216</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>20151204</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>20151215</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.019191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20151213</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.009403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20151209</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.017705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>20151230</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.051090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>20151227</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>20151202</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20151214</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>20151222</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>20151216</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.021162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>20151204</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>20151224</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>20151220</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.061712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>20151228</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>20151201</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.030448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>20151221</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.013287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>20151211</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>20151212</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.021809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>20151231</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.074615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>20151218</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.021169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>20151226</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.097530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>20151202</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.046480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "19   20151213               1              1     0.995310    0.990592   \n",
       "19   20151213               1              1     0.995310    0.990592   \n",
       "26   20151211               1              1     0.995310    0.989958   \n",
       "26   20151211               1              1     0.995310    0.989958   \n",
       "21   20151228               1              1     0.995310    0.988677   \n",
       "21   20151228               1              1     0.995310    0.988677   \n",
       "6    20151227               1              1     0.995310    0.988185   \n",
       "6    20151227               1              1     0.995310    0.988185   \n",
       "3    20151222               1              1     0.995310    0.986753   \n",
       "3    20151222               1              1     0.995310    0.986753   \n",
       "1    20151204               1              1     0.995310    0.986732   \n",
       "1    20151204               1              1     0.995310    0.986732   \n",
       "27   20151221               1              1     0.995310    0.986680   \n",
       "27   20151221               1              1     0.995310    0.986680   \n",
       "15   20151210               1              1     0.995310    0.986623   \n",
       "15   20151210               1              1     0.995310    0.986623   \n",
       "9    20151205               1              1     0.995310    0.986490   \n",
       "9    20151205               1              1     0.995310    0.986490   \n",
       "14   20151225               1              1     0.995310    0.985851   \n",
       "14   20151225               1              1     0.995310    0.985851   \n",
       "13   20151206               1              1     0.995310    0.985285   \n",
       "13   20151206               1              1     0.995310    0.985285   \n",
       "4    20151214               1              1     0.995310    0.984951   \n",
       "4    20151214               1              1     0.995310    0.984951   \n",
       "11   20151208               1              1     0.995310    0.983332   \n",
       "11   20151208               1              1     0.995310    0.983332   \n",
       "0    20151224               1              1     0.995310    0.982835   \n",
       "0    20151224               1              1     0.995310    0.982835   \n",
       "20   20151209               1              1     0.995310    0.982262   \n",
       "20   20151209               1              1     0.995310    0.982262   \n",
       "18   20151215               1              1     0.995310    0.980801   \n",
       "18   20151215               1              1     0.995310    0.980801   \n",
       "2    20151216               1              1     0.995310    0.978833   \n",
       "2    20151216               1              1     0.995310    0.978833   \n",
       "23   20151218               1              1     0.995310    0.978825   \n",
       "23   20151218               1              1     0.995310    0.978825   \n",
       "25   20151212               1              1     0.995310    0.978158   \n",
       "25   20151212               1              1     0.995310    0.978158   \n",
       "293  20151225              42              3     0.986448    0.976603   \n",
       "293  20151225              42              3     0.986448    0.976603   \n",
       "285  20151227              42              3     0.986448    0.975995   \n",
       "285  20151227              42              3     0.986448    0.975995   \n",
       "294  20151210              42              3     0.986448    0.974190   \n",
       "294  20151210              42              3     0.986448    0.974190   \n",
       "292  20151206              42              3     0.986448    0.974088   \n",
       "292  20151206              42              3     0.986448    0.974088   \n",
       "10   20151229               1              1     0.995310    0.972739   \n",
       "10   20151229               1              1     0.995310    0.972739   \n",
       "279  20151224              42              3     0.986448    0.972663   \n",
       "279  20151224              42              3     0.986448    0.972663   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "132  20151223              42              1     0.003759    0.034788   \n",
       "131  20151203              42              1     0.003759    0.093747   \n",
       "130  20151227              42              1     0.003759    0.011803   \n",
       "129  20151202              42              1     0.003759    0.046480   \n",
       "128  20151214              42              1     0.003759    0.015028   \n",
       "127  20151222              42              1     0.003759    0.013258   \n",
       "126  20151216              42              1     0.003759    0.021162   \n",
       "125  20151204              42              1     0.003759    0.013250   \n",
       "103  20151229              16              1     0.003414    0.027239   \n",
       "102  20151205              16              1     0.003414    0.013488   \n",
       "101  20151223              16              1     0.003414    0.034788   \n",
       "100  20151203              16              1     0.003414    0.093747   \n",
       "79   20151207               8              1     0.003552    0.035782   \n",
       "78   20151217               8              1     0.003552    0.033611   \n",
       "77   20151210               8              1     0.003552    0.013308   \n",
       "76   20151225               8              1     0.003552    0.014154   \n",
       "75   20151206               8              1     0.003552    0.014715   \n",
       "74   20151219               8              1     0.003552    0.311141   \n",
       "73   20151208               8              1     0.003552    0.016649   \n",
       "72   20151229               8              1     0.003552    0.027239   \n",
       "71   20151205               8              1     0.003552    0.013488   \n",
       "70   20151223               8              1     0.003552    0.034788   \n",
       "69   20151203               8              1     0.003552    0.093747   \n",
       "68   20151227               8              1     0.003552    0.011803   \n",
       "135  20151208              42              1     0.003759    0.016649   \n",
       "66   20151214               8              1     0.003552    0.015028   \n",
       "65   20151222               8              1     0.003552    0.013258   \n",
       "64   20151216               8              1     0.003552    0.021162   \n",
       "63   20151204               8              1     0.003552    0.013250   \n",
       "80   20151215               8              1     0.003552    0.019191   \n",
       "81   20151213               8              1     0.003552    0.009403   \n",
       "82   20151209               8              1     0.003552    0.017705   \n",
       "92   20151230               8              1     0.003552    0.051090   \n",
       "99   20151227              16              1     0.003414    0.011803   \n",
       "98   20151202              16              1     0.003414    0.046480   \n",
       "97   20151214              16              1     0.003414    0.015028   \n",
       "96   20151222              16              1     0.003414    0.013258   \n",
       "95   20151216              16              1     0.003414    0.021162   \n",
       "94   20151204              16              1     0.003414    0.013250   \n",
       "93   20151224              16              1     0.003414    0.017113   \n",
       "91   20151220               8              1     0.003552    0.061712   \n",
       "83   20151228               8              1     0.003552    0.011314   \n",
       "90   20151201               8              1     0.003552    0.030448   \n",
       "89   20151221               8              1     0.003552    0.013287   \n",
       "88   20151211               8              1     0.003552    0.010022   \n",
       "87   20151212               8              1     0.003552    0.021809   \n",
       "86   20151231               8              1     0.003552    0.074615   \n",
       "85   20151218               8              1     0.003552    0.021169   \n",
       "84   20151226               8              1     0.003552    0.097530   \n",
       "67   20151202               8              1     0.003552    0.046480   \n",
       "\n",
       "     f1_score  time_taken  \n",
       "19   0.995274   30.252922  \n",
       "19   0.995274   30.252922  \n",
       "26   0.994953   30.252922  \n",
       "26   0.994953   30.252922  \n",
       "21   0.994306   30.252922  \n",
       "21   0.994306   30.252922  \n",
       "6    0.994057   30.252922  \n",
       "6    0.994057   30.252922  \n",
       "3    0.993332   30.252922  \n",
       "3    0.993332   30.252922  \n",
       "1    0.993322   30.252922  \n",
       "1    0.993322   30.252922  \n",
       "27   0.993295   30.252922  \n",
       "27   0.993295   30.252922  \n",
       "15   0.993266   30.252922  \n",
       "15   0.993266   30.252922  \n",
       "9    0.993199   30.252922  \n",
       "9    0.993199   30.252922  \n",
       "14   0.992875   30.252922  \n",
       "14   0.992875   30.252922  \n",
       "13   0.992588   30.252922  \n",
       "13   0.992588   30.252922  \n",
       "4    0.992419   30.252922  \n",
       "4    0.992419   30.252922  \n",
       "11   0.991596   30.252922  \n",
       "11   0.991596   30.252922  \n",
       "0    0.991343   30.252922  \n",
       "0    0.991343   30.252922  \n",
       "20   0.991051   30.252922  \n",
       "20   0.991051   30.252922  \n",
       "18   0.990308   30.252922  \n",
       "18   0.990308   30.252922  \n",
       "2    0.989303   30.252922  \n",
       "2    0.989303   30.252922  \n",
       "23   0.989299   30.252922  \n",
       "23   0.989299   30.252922  \n",
       "25   0.988958   30.252922  \n",
       "25   0.988958   30.252922  \n",
       "293  0.988157   44.915621  \n",
       "293  0.988157   44.915621  \n",
       "285  0.987844   44.915621  \n",
       "285  0.987844   44.915621  \n",
       "294  0.986917   44.915621  \n",
       "294  0.986917   44.915621  \n",
       "292  0.986866   44.915621  \n",
       "292  0.986866   44.915621  \n",
       "10   0.986181   30.252922  \n",
       "10   0.986181   30.252922  \n",
       "279  0.986134   44.915621  \n",
       "279  0.986134   44.915621  \n",
       "..        ...         ...  \n",
       "132  0.000000   33.607306  \n",
       "131  0.000000   33.607306  \n",
       "130  0.000000   33.607306  \n",
       "129  0.000000   33.607306  \n",
       "128  0.000000   33.607306  \n",
       "127  0.000000   33.607306  \n",
       "126  0.000000   33.607306  \n",
       "125  0.000000   33.607306  \n",
       "103  0.000000   33.042446  \n",
       "102  0.000000   33.042446  \n",
       "101  0.000000   33.042446  \n",
       "100  0.000000   33.042446  \n",
       "79   0.000000   32.908966  \n",
       "78   0.000000   32.908966  \n",
       "77   0.000000   32.908966  \n",
       "76   0.000000   32.908966  \n",
       "75   0.000000   32.908966  \n",
       "74   0.000000   32.908966  \n",
       "73   0.000000   32.908966  \n",
       "72   0.000000   32.908966  \n",
       "71   0.000000   32.908966  \n",
       "70   0.000000   32.908966  \n",
       "69   0.000000   32.908966  \n",
       "68   0.000000   32.908966  \n",
       "135  0.000000   33.607306  \n",
       "66   0.000000   32.908966  \n",
       "65   0.000000   32.908966  \n",
       "64   0.000000   32.908966  \n",
       "63   0.000000   32.908966  \n",
       "80   0.000000   32.908966  \n",
       "81   0.000000   32.908966  \n",
       "82   0.000000   32.908966  \n",
       "92   0.000000   32.908966  \n",
       "99   0.000000   33.042446  \n",
       "98   0.000000   33.042446  \n",
       "97   0.000000   33.042446  \n",
       "96   0.000000   33.042446  \n",
       "95   0.000000   33.042446  \n",
       "94   0.000000   33.042446  \n",
       "93   0.000000   33.042446  \n",
       "91   0.000000   32.908966  \n",
       "83   0.000000   32.908966  \n",
       "90   0.000000   32.908966  \n",
       "89   0.000000   32.908966  \n",
       "88   0.000000   32.908966  \n",
       "87   0.000000   32.908966  \n",
       "86   0.000000   32.908966  \n",
       "85   0.000000   32.908966  \n",
       "84   0.000000   32.908966  \n",
       "67   0.000000   32.908966  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='f1_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:26:04.625782Z",
     "start_time": "2017-07-19T22:26:04.607151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20151213</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.995274</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20151225</td>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.976603</td>\n",
       "      <td>0.988157</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>20151225</td>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.958501</td>\n",
       "      <td>0.978784</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20151225</td>\n",
       "      <td>0.948895</td>\n",
       "      <td>0.948525</td>\n",
       "      <td>0.973538</td>\n",
       "      <td>43.224703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>20151225</td>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.934409</td>\n",
       "      <td>0.965990</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151208</td>\n",
       "      <td>0.864168</td>\n",
       "      <td>0.895378</td>\n",
       "      <td>0.944515</td>\n",
       "      <td>42.929413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20151206</td>\n",
       "      <td>0.734887</td>\n",
       "      <td>0.822127</td>\n",
       "      <td>0.901842</td>\n",
       "      <td>41.549513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20151223</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.034788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>20151224</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20151230</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.051090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  train_score  test_score  f1_score  \\\n",
       "no_of_features hidden_layers                                                \n",
       "1              1              20151213     0.995310    0.990592  0.995274   \n",
       "42             3              20151225     0.986448    0.976603  0.988157   \n",
       "16             3              20151225     0.973275    0.958501  0.978784   \n",
       "8              3              20151225     0.948895    0.948525  0.973538   \n",
       "4              1              20151225     0.923411    0.934409  0.965990   \n",
       "               3              20151208     0.864168    0.895378  0.944515   \n",
       "1              3              20151206     0.734887    0.822127  0.901842   \n",
       "8              1              20151223     0.003552    0.034788  0.000000   \n",
       "16             1              20151224     0.003414    0.017113  0.000000   \n",
       "42             1              20151230     0.003759    0.051090  0.000000   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1               30.252922  \n",
       "42             3               44.915621  \n",
       "16             3               42.956834  \n",
       "8              3               43.224703  \n",
       "4              1               32.132440  \n",
       "               3               42.929413  \n",
       "1              3               41.549513  \n",
       "8              1               32.908966  \n",
       "16             1               33.042446  \n",
       "42             1               33.607306  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='f1_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T22:26:07.946280Z",
     "start_time": "2017-07-19T22:26:04.627239Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key_nof_hidden '20151201_16_1'\n",
    "Train.predictions = pd.read_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:47.755495Z",
     "start_time": "2017-07-19T23:29:47.732034Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions['20151228_1_1'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:48.309731Z",
     "start_time": "2017-07-19T23:29:48.303006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train.predictions['20151219_42_1'].loc[:,'Prediction']\n",
    "df.loc[:,'Prediction'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:48.477961Z",
     "start_time": "2017-07-19T23:29:48.426169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99430638281572759"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "me.f1_score(df.loc[:,'Actual'].values.astype(int),\n",
    "            df.loc[:,'Prediction'].values.astype(int) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:48.718387Z",
     "start_time": "2017-07-19T23:29:48.705117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0      4159\n",
       "1.0    363423\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:49.708402Z",
     "start_time": "2017-07-19T23:29:49.148546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGhCAYAAADiGPptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERHROgluERHROgluERHROv8H/RjEyn1w8H8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7df5dea0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:49.724112Z",
     "start_time": "2017-07-19T23:29:49.709863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.961721</td>\n",
       "      <td>0.979581</td>\n",
       "      <td>30.252922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>0.986448</td>\n",
       "      <td>0.947723</td>\n",
       "      <td>0.972223</td>\n",
       "      <td>44.915621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>0.973275</td>\n",
       "      <td>0.923643</td>\n",
       "      <td>0.959251</td>\n",
       "      <td>42.956834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>0.948895</td>\n",
       "      <td>0.906955</td>\n",
       "      <td>0.950026</td>\n",
       "      <td>43.224703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.923411</td>\n",
       "      <td>0.892685</td>\n",
       "      <td>0.942055</td>\n",
       "      <td>32.132440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.864168</td>\n",
       "      <td>0.849429</td>\n",
       "      <td>0.916680</td>\n",
       "      <td>42.929413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>0.734887</td>\n",
       "      <td>0.779953</td>\n",
       "      <td>0.873645</td>\n",
       "      <td>41.549513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003552</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.908966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.042446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.038258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.607306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  f1_score  time_taken\n",
       "no_of_features hidden_layers                                               \n",
       "1              1                 0.995310    0.961721  0.979581   30.252922\n",
       "42             3                 0.986448    0.947723  0.972223   44.915621\n",
       "16             3                 0.973275    0.923643  0.959251   42.956834\n",
       "8              3                 0.948895    0.906955  0.950026   43.224703\n",
       "4              1                 0.923411    0.892685  0.942055   32.132440\n",
       "               3                 0.864168    0.849429  0.916680   42.929413\n",
       "1              3                 0.734887    0.779953  0.873645   41.549513\n",
       "8              1                 0.003552    0.038258  0.000000   32.908966\n",
       "16             1                 0.003414    0.038258  0.000000   33.042446\n",
       "42             1                 0.003759    0.038258  0.000000   33.607306"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:49.739380Z",
     "start_time": "2017-07-19T23:29:49.725525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055454</td>\n",
       "      <td>0.032589</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028077</td>\n",
       "      <td>0.020820</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039070</td>\n",
       "      <td>0.024784</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031096</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042118</td>\n",
       "      <td>0.026527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>0.028088</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051008</td>\n",
       "      <td>0.030530</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  f1_score  time_taken\n",
       "no_of_features hidden_layers                                               \n",
       "1              1                      0.0    0.055454  0.032589         0.0\n",
       "               3                      0.0    0.028077  0.020820         0.0\n",
       "4              1                      0.0    0.039070  0.024784         0.0\n",
       "               3                      0.0    0.031096  0.021098         0.0\n",
       "8              1                      0.0    0.055450  0.000000         0.0\n",
       "               3                      0.0    0.042118  0.026527         0.0\n",
       "16             1                      0.0    0.055450  0.000000         0.0\n",
       "               3                      0.0    0.045102  0.028088         0.0\n",
       "42             1                      0.0    0.055450  0.000000         0.0\n",
       "               3                      0.0    0.051008  0.030530         0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-19T23:29:50.296347Z",
     "start_time": "2017-07-19T23:29:50.271641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1907: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1908: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                 (0.915707847046, 1.04345506437)\n",
       "                3                (0.832838076876, 0.914452600852)\n",
       "4               1                (0.893479728224, 0.990629680824)\n",
       "                3                (0.875329482057, 0.958031449982)\n",
       "8               1                                      (nan, nan)\n",
       "                3                 (0.898033640418, 1.00201797666)\n",
       "16              1                                      (nan, nan)\n",
       "                3                 (0.904199217357, 1.01430179057)\n",
       "42              1                                      (nan, nan)\n",
       "                3                  (0.91238564696, 1.03205998597)\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.f1_score.mean(), scale=x.f1_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
