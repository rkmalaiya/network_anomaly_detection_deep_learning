{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:38.863704Z",
     "start_time": "2017-05-14T20:05:32.871353Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",35)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:40.662218Z",
     "start_time": "2017-05-14T20:05:38.865310Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    kdd_train_2labels = pd.read_pickle(\"dataset/kdd_train_2labels.pkl\")\n",
    "    kdd_test_2labels = pd.read_pickle(\"dataset/kdd_test_2labels.pkl\")\n",
    "    \n",
    "    kdd_train_5labels = pd.read_pickle(\"dataset/kdd_train_5labels.pkl\")\n",
    "    kdd_test_5labels = pd.read_pickle(\"dataset/kdd_test_5labels.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:40.668586Z",
     "start_time": "2017-05-14T20:05:40.663794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125973, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_train_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:40.673734Z",
     "start_time": "2017-05-14T20:05:40.670175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22544, 124)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.kdd_test_2labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:45.233617Z",
     "start_time": "2017-05-14T20:05:40.675143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99589320646770185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "class preprocess:\n",
    "    \n",
    "    output_columns_2labels = ['is_Attack','is_Normal']\n",
    "    \n",
    "    x_input = dataset.kdd_train_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_output = dataset.kdd_train_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    x_test_input = dataset.kdd_test_2labels.drop(output_columns_2labels, axis = 1)\n",
    "    y_test = dataset.kdd_test_2labels.loc[:,output_columns_2labels]\n",
    "\n",
    "    ss = pp.StandardScaler()\n",
    "\n",
    "    x_train = ss.fit_transform(x_input)\n",
    "    x_test = ss.transform(x_test_input)\n",
    "\n",
    "    y_train = y_output.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "preprocess.x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:51.922801Z",
     "start_time": "2017-05-14T20:05:45.235199Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:52.425218Z",
     "start_time": "2017-05-14T20:05:51.924411Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 122\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 122\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 122\n",
    "    lam = 0.001\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            #loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            #loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            #loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:05:52.549607Z",
     "start_time": "2017-05-14T20:05:52.426827Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['epoch', 'no_of_features','hidden_layers','train_score', 'test_score'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    \n",
    "    def train(epochs, net, h,f):\n",
    "        batch_iterations = 200\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            Train.best_acc = 0\n",
    "            for epoch in range(1, (epochs+1)):\n",
    "                x_train, x_valid, y_train, y_valid, = ms.train_test_split(preprocess.x_train, \n",
    "                                                                          preprocess.y_train, \n",
    "                                                                          test_size=0.2)\n",
    "                batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                           batch_iterations)\n",
    "                                                                          \n",
    "                for i in batch_indices:\n",
    "                    _, train_loss = sess.run([net.train_op, \n",
    "                                                           net.regularized_loss, \n",
    "                                                           ], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_train[i,:], \n",
    "                                                                     net.y_: y_train[i,:], \n",
    "                                                                     net.keep_prob:1})\n",
    "                    \n",
    "                    #summary_writer_train.add_summary(summary_str, epoch)\n",
    "                    if(train_loss > 1e9):\n",
    "                        print(\"Step {} | Training Loss: {:.6f}\".format(epoch, train_loss))\n",
    "                    \n",
    "                valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], #net.summary_op\n",
    "                                                          feed_dict={net.x: x_valid, \n",
    "                                                                     net.y_: y_valid, \n",
    "                                                                     net.keep_prob:1})\n",
    "                \n",
    "                test_accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                  net.actual, net.y], #net.summary_op \n",
    "                                                                                  feed_dict={net.x: preprocess.x_test, \n",
    "                                                                                 net.y_: preprocess.y_test, \n",
    "                                                                                 net.keep_prob:1})\n",
    "                #summary_writer_valid.add_summary(summary_str, epoch)\n",
    "\n",
    "                if epoch % 1 == 0:\n",
    "                    print(\"Step {} | Training Loss: {:.6f} | Test Loss: {:.6f} | Test Accuracy: {:.6f}\"\n",
    "                          .format(epoch, train_loss, test_loss, test_accuracy))\n",
    "\n",
    "                curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1]})\n",
    "                Train.predictions.update({\"{}_{}_{}\".format(epochs,f,h):curr_pred})\n",
    "\n",
    "                if test_accuracy > Train.best_acc:\n",
    "                    Train.best_acc = test_accuracy\n",
    "                    Train.pred_value = pred_value\n",
    "                    Train.actual_value = actual_value\n",
    "                    Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "                    net.saver.save(sess, \"dataset/tf_vae_dense_trained_together_nsl_kdd_hidden layers_{}_features count_{}\".format(epochs,h,f))\n",
    "                Train.results.append(Train.result(epochs, f, h,valid_accuracy, test_accuracy))\n",
    "            print(\"Best Accuracy on Test data: {}\".format(Train.best_acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:50:18.851966Z",
     "start_time": "2017-05-14T20:05:52.551302Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:4\n",
      "Step 1 | Training Loss: 0.000039 | Test Loss: 0.000141 | Test Accuracy: 0.791164\n",
      "Step 2 | Training Loss: 0.000032 | Test Loss: 0.000306 | Test Accuracy: 0.801943\n",
      "Step 3 | Training Loss: 0.000059 | Test Loss: 0.145491 | Test Accuracy: 0.828158\n",
      "Step 4 | Training Loss: 0.000096 | Test Loss: 0.000229 | Test Accuracy: 0.817734\n",
      "Step 5 | Training Loss: 0.000059 | Test Loss: 0.000080 | Test Accuracy: 0.838272\n",
      "Step 6 | Training Loss: 0.000131 | Test Loss: 0.000249 | Test Accuracy: 0.825408\n",
      "Step 7 | Training Loss: 0.000036 | Test Loss: 0.000223 | Test Accuracy: 0.843994\n",
      "Step 8 | Training Loss: 0.000057 | Test Loss: 0.000244 | Test Accuracy: 0.838804\n",
      "Step 9 | Training Loss: 0.000058 | Test Loss: 0.000235 | Test Accuracy: 0.836897\n",
      "Step 10 | Training Loss: 0.000008 | Test Loss: 0.000218 | Test Accuracy: 0.836098\n",
      "Step 11 | Training Loss: 0.000043 | Test Loss: 0.000271 | Test Accuracy: 0.829755\n",
      "Step 12 | Training Loss: 0.000018 | Test Loss: 0.000385 | Test Accuracy: 0.838538\n",
      "Step 13 | Training Loss: 0.000023 | Test Loss: 0.000265 | Test Accuracy: 0.839913\n",
      "Step 14 | Training Loss: 0.000064 | Test Loss: 0.000215 | Test Accuracy: 0.839070\n",
      "Step 15 | Training Loss: 0.000030 | Test Loss: 0.000172 | Test Accuracy: 0.837740\n",
      "Step 16 | Training Loss: 0.000001 | Test Loss: 0.000194 | Test Accuracy: 0.838361\n",
      "Step 17 | Training Loss: 0.000017 | Test Loss: 0.000202 | Test Accuracy: 0.846966\n",
      "Step 18 | Training Loss: 0.000046 | Test Loss: 0.000171 | Test Accuracy: 0.851890\n",
      "Step 19 | Training Loss: 0.000014 | Test Loss: 0.000191 | Test Accuracy: 0.840135\n",
      "Step 20 | Training Loss: 0.000036 | Test Loss: 0.000262 | Test Accuracy: 0.844792\n",
      "Step 21 | Training Loss: 0.000031 | Test Loss: 0.000298 | Test Accuracy: 0.842575\n",
      "Step 22 | Training Loss: 0.000011 | Test Loss: 0.000215 | Test Accuracy: 0.857434\n",
      "Step 23 | Training Loss: 0.000020 | Test Loss: 0.000271 | Test Accuracy: 0.853664\n",
      "Step 24 | Training Loss: 0.000007 | Test Loss: 0.000194 | Test Accuracy: 0.852200\n",
      "Step 25 | Training Loss: 0.000038 | Test Loss: 0.000214 | Test Accuracy: 0.857035\n",
      "Step 26 | Training Loss: 0.000023 | Test Loss: 0.000228 | Test Accuracy: 0.859519\n",
      "Step 27 | Training Loss: 0.000030 | Test Loss: 0.000233 | Test Accuracy: 0.856059\n",
      "Step 28 | Training Loss: 0.000069 | Test Loss: 0.000306 | Test Accuracy: 0.849494\n",
      "Step 29 | Training Loss: 0.000003 | Test Loss: 0.000292 | Test Accuracy: 0.840224\n",
      "Step 30 | Training Loss: 0.000009 | Test Loss: 0.000192 | Test Accuracy: 0.866616\n",
      "Step 31 | Training Loss: 0.000011 | Test Loss: 0.000238 | Test Accuracy: 0.864931\n",
      "Step 32 | Training Loss: 0.000027 | Test Loss: 0.000236 | Test Accuracy: 0.870653\n",
      "Step 33 | Training Loss: 0.000015 | Test Loss: 0.000223 | Test Accuracy: 0.853841\n",
      "Step 34 | Training Loss: 0.000007 | Test Loss: 0.000233 | Test Accuracy: 0.847897\n",
      "Step 35 | Training Loss: 0.000034 | Test Loss: 0.000290 | Test Accuracy: 0.847631\n",
      "Step 36 | Training Loss: 0.000005 | Test Loss: 0.000246 | Test Accuracy: 0.838094\n",
      "Step 37 | Training Loss: 0.000036 | Test Loss: 0.000248 | Test Accuracy: 0.844615\n",
      "Step 38 | Training Loss: 0.000001 | Test Loss: 0.000266 | Test Accuracy: 0.860939\n",
      "Step 39 | Training Loss: 0.000019 | Test Loss: 0.000247 | Test Accuracy: 0.866173\n",
      "Step 40 | Training Loss: 0.000016 | Test Loss: 0.000263 | Test Accuracy: 0.864487\n",
      "Step 41 | Training Loss: 0.000001 | Test Loss: 0.000207 | Test Accuracy: 0.857745\n",
      "Step 42 | Training Loss: 0.000030 | Test Loss: 0.000296 | Test Accuracy: 0.857612\n",
      "Step 43 | Training Loss: 0.000012 | Test Loss: 0.000196 | Test Accuracy: 0.857168\n",
      "Step 44 | Training Loss: 0.000026 | Test Loss: 0.000264 | Test Accuracy: 0.865374\n",
      "Step 45 | Training Loss: 0.000057 | Test Loss: 0.000200 | Test Accuracy: 0.863112\n",
      "Step 46 | Training Loss: 0.000014 | Test Loss: 0.000197 | Test Accuracy: 0.861693\n",
      "Step 47 | Training Loss: 0.000048 | Test Loss: 0.000204 | Test Accuracy: 0.856325\n",
      "Step 48 | Training Loss: 0.000006 | Test Loss: 0.000242 | Test Accuracy: 0.852333\n",
      "Step 49 | Training Loss: 0.000035 | Test Loss: 0.000200 | Test Accuracy: 0.865064\n",
      "Step 50 | Training Loss: 0.000053 | Test Loss: 0.000241 | Test Accuracy: 0.851313\n",
      "Best Accuracy on Test data: 0.870652973651886\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:8\n",
      "Step 1 | Training Loss: 0.000112 | Test Loss: 0.000577 | Test Accuracy: 0.748758\n",
      "Step 2 | Training Loss: 0.000129 | Test Loss: 0.000734 | Test Accuracy: 0.753992\n",
      "Step 3 | Training Loss: 0.000033 | Test Loss: 868.871155 | Test Accuracy: 0.743346\n",
      "Step 4 | Training Loss: 0.000023 | Test Loss: 0.000153 | Test Accuracy: 0.790099\n",
      "Step 5 | Training Loss: 0.000060 | Test Loss: 0.000189 | Test Accuracy: 0.794624\n",
      "Step 6 | Training Loss: 0.000118 | Test Loss: 0.000223 | Test Accuracy: 0.826916\n",
      "Step 7 | Training Loss: 0.000019 | Test Loss: 0.000098 | Test Accuracy: 0.845059\n",
      "Step 8 | Training Loss: 0.000182 | Test Loss: 0.000355 | Test Accuracy: 0.812456\n",
      "Step 9 | Training Loss: 0.000015 | Test Loss: 0.000198 | Test Accuracy: 0.817734\n",
      "Step 10 | Training Loss: 0.000120 | Test Loss: 0.000135 | Test Accuracy: 0.826073\n",
      "Step 11 | Training Loss: 0.000054 | Test Loss: 0.000187 | Test Accuracy: 0.826207\n",
      "Step 12 | Training Loss: 0.000058 | Test Loss: 0.000173 | Test Accuracy: 0.830332\n",
      "Step 13 | Training Loss: 0.000024 | Test Loss: 0.000273 | Test Accuracy: 0.824477\n",
      "Step 14 | Training Loss: 0.000053 | Test Loss: 0.000150 | Test Accuracy: 0.844925\n",
      "Step 15 | Training Loss: 0.000010 | Test Loss: 0.000178 | Test Accuracy: 0.860983\n",
      "Step 16 | Training Loss: 0.000008 | Test Loss: 0.000164 | Test Accuracy: 0.878726\n",
      "Step 17 | Training Loss: 0.000058 | Test Loss: 0.000184 | Test Accuracy: 0.862491\n",
      "Step 18 | Training Loss: 0.000023 | Test Loss: 0.000171 | Test Accuracy: 0.868524\n",
      "Step 19 | Training Loss: 0.000015 | Test Loss: 0.000181 | Test Accuracy: 0.872871\n",
      "Step 20 | Training Loss: 0.000012 | Test Loss: 0.000174 | Test Accuracy: 0.881254\n",
      "Step 21 | Training Loss: 0.000021 | Test Loss: 0.000219 | Test Accuracy: 0.853620\n",
      "Step 22 | Training Loss: 0.000070 | Test Loss: 0.000178 | Test Accuracy: 0.868169\n",
      "Step 23 | Training Loss: 0.000028 | Test Loss: 0.000163 | Test Accuracy: 0.874778\n",
      "Step 24 | Training Loss: 0.000013 | Test Loss: 0.000167 | Test Accuracy: 0.886267\n",
      "Step 25 | Training Loss: 0.000016 | Test Loss: 0.000188 | Test Accuracy: 0.882630\n",
      "Step 26 | Training Loss: 0.000008 | Test Loss: 0.000173 | Test Accuracy: 0.887775\n",
      "Step 27 | Training Loss: 0.000011 | Test Loss: 0.000142 | Test Accuracy: 0.889372\n",
      "Step 28 | Training Loss: 0.000013 | Test Loss: 0.000145 | Test Accuracy: 0.883561\n",
      "Step 29 | Training Loss: 0.000015 | Test Loss: 0.000162 | Test Accuracy: 0.884892\n",
      "Step 30 | Training Loss: 0.000028 | Test Loss: 0.000212 | Test Accuracy: 0.860539\n",
      "Step 31 | Training Loss: 0.000045 | Test Loss: 0.000174 | Test Accuracy: 0.886666\n",
      "Step 32 | Training Loss: 0.000023 | Test Loss: 0.000201 | Test Accuracy: 0.865463\n",
      "Step 33 | Training Loss: 0.000040 | Test Loss: 0.000203 | Test Accuracy: 0.867326\n",
      "Step 34 | Training Loss: 0.000012 | Test Loss: 0.000162 | Test Accuracy: 0.876553\n",
      "Step 35 | Training Loss: 0.000057 | Test Loss: 0.000130 | Test Accuracy: 0.873181\n",
      "Step 36 | Training Loss: 0.000022 | Test Loss: 0.000229 | Test Accuracy: 0.861338\n",
      "Step 37 | Training Loss: 0.000020 | Test Loss: 0.000204 | Test Accuracy: 0.886311\n",
      "Step 38 | Training Loss: 0.000007 | Test Loss: 0.000127 | Test Accuracy: 0.888174\n",
      "Step 39 | Training Loss: 0.000044 | Test Loss: 0.000178 | Test Accuracy: 0.885779\n",
      "Step 40 | Training Loss: 0.000030 | Test Loss: 0.000192 | Test Accuracy: 0.883251\n",
      "Step 41 | Training Loss: 0.000021 | Test Loss: 0.000136 | Test Accuracy: 0.881831\n",
      "Step 42 | Training Loss: 0.000010 | Test Loss: 0.000156 | Test Accuracy: 0.888440\n",
      "Step 43 | Training Loss: 0.000074 | Test Loss: 0.000205 | Test Accuracy: 0.884271\n",
      "Step 44 | Training Loss: 0.000002 | Test Loss: 0.000132 | Test Accuracy: 0.883428\n",
      "Step 45 | Training Loss: 0.000011 | Test Loss: 0.000114 | Test Accuracy: 0.892300\n",
      "Step 46 | Training Loss: 0.000027 | Test Loss: 0.000128 | Test Accuracy: 0.888041\n",
      "Step 47 | Training Loss: 0.000006 | Test Loss: 0.000100 | Test Accuracy: 0.895227\n",
      "Step 48 | Training Loss: 0.000043 | Test Loss: 0.000128 | Test Accuracy: 0.907470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 49 | Training Loss: 0.000029 | Test Loss: 0.000115 | Test Accuracy: 0.905296\n",
      "Step 50 | Training Loss: 0.000006 | Test Loss: 0.000113 | Test Accuracy: 0.882984\n",
      "Best Accuracy on Test data: 0.9074698090553284\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:16\n",
      "Step 1 | Training Loss: 0.000108 | Test Loss: 0.000431 | Test Accuracy: 0.814363\n",
      "Step 2 | Training Loss: 0.000039 | Test Loss: 0.000364 | Test Accuracy: 0.810282\n",
      "Step 3 | Training Loss: 0.000228 | Test Loss: 0.000391 | Test Accuracy: 0.764461\n",
      "Step 4 | Training Loss: 0.000082 | Test Loss: 0.000249 | Test Accuracy: 0.798128\n",
      "Step 5 | Training Loss: 0.000012 | Test Loss: 0.000090 | Test Accuracy: 0.818666\n",
      "Step 6 | Training Loss: 0.000005 | Test Loss: 0.000137 | Test Accuracy: 0.857612\n",
      "Step 7 | Training Loss: 0.000057 | Test Loss: 0.000223 | Test Accuracy: 0.854995\n",
      "Step 8 | Training Loss: 0.000022 | Test Loss: 0.000125 | Test Accuracy: 0.865020\n",
      "Step 9 | Training Loss: 0.000027 | Test Loss: 0.000134 | Test Accuracy: 0.859918\n",
      "Step 10 | Training Loss: 0.000099 | Test Loss: 0.000083 | Test Accuracy: 0.853575\n",
      "Step 11 | Training Loss: 0.000026 | Test Loss: 0.000099 | Test Accuracy: 0.884892\n",
      "Step 12 | Training Loss: 0.000011 | Test Loss: 0.000063 | Test Accuracy: 0.896292\n",
      "Step 13 | Training Loss: 0.000024 | Test Loss: 0.000078 | Test Accuracy: 0.887198\n",
      "Step 14 | Training Loss: 0.000041 | Test Loss: 0.000079 | Test Accuracy: 0.891191\n",
      "Step 15 | Training Loss: 0.000009 | Test Loss: 0.000136 | Test Accuracy: 0.884626\n",
      "Step 16 | Training Loss: 0.000012 | Test Loss: 0.000153 | Test Accuracy: 0.882630\n",
      "Step 17 | Training Loss: 0.000023 | Test Loss: 0.000142 | Test Accuracy: 0.892078\n",
      "Step 18 | Training Loss: 0.000027 | Test Loss: 0.000084 | Test Accuracy: 0.889815\n",
      "Step 19 | Training Loss: 0.000020 | Test Loss: 0.000094 | Test Accuracy: 0.881432\n",
      "Step 20 | Training Loss: 0.000034 | Test Loss: 0.000060 | Test Accuracy: 0.882674\n",
      "Step 21 | Training Loss: 0.000026 | Test Loss: 0.000130 | Test Accuracy: 0.880367\n",
      "Step 22 | Training Loss: 0.000022 | Test Loss: 0.000068 | Test Accuracy: 0.886311\n",
      "Step 23 | Training Loss: 0.000034 | Test Loss: 0.000127 | Test Accuracy: 0.884448\n",
      "Step 24 | Training Loss: 0.000038 | Test Loss: 0.000148 | Test Accuracy: 0.884803\n",
      "Step 25 | Training Loss: 0.000006 | Test Loss: 0.000128 | Test Accuracy: 0.885557\n",
      "Step 26 | Training Loss: 0.000002 | Test Loss: 0.000123 | Test Accuracy: 0.878637\n",
      "Step 27 | Training Loss: 0.000023 | Test Loss: 0.000163 | Test Accuracy: 0.881210\n",
      "Step 28 | Training Loss: 0.000060 | Test Loss: 0.000098 | Test Accuracy: 0.879258\n",
      "Step 29 | Training Loss: 0.000007 | Test Loss: 0.000129 | Test Accuracy: 0.880456\n",
      "Step 30 | Training Loss: 0.000026 | Test Loss: 0.000171 | Test Accuracy: 0.882674\n",
      "Step 31 | Training Loss: 0.000002 | Test Loss: 0.000142 | Test Accuracy: 0.880722\n",
      "Step 32 | Training Loss: 0.000042 | Test Loss: 0.000133 | Test Accuracy: 0.873093\n",
      "Step 33 | Training Loss: 0.000024 | Test Loss: 0.000122 | Test Accuracy: 0.877129\n",
      "Step 34 | Training Loss: 0.000062 | Test Loss: 0.000151 | Test Accuracy: 0.878194\n",
      "Step 35 | Training Loss: 0.000054 | Test Loss: 0.000172 | Test Accuracy: 0.885335\n",
      "Step 36 | Training Loss: 0.000036 | Test Loss: 0.000148 | Test Accuracy: 0.892033\n",
      "Step 37 | Training Loss: 0.000024 | Test Loss: 0.000164 | Test Accuracy: 0.889283\n",
      "Step 38 | Training Loss: 0.000036 | Test Loss: 0.000139 | Test Accuracy: 0.882319\n",
      "Step 39 | Training Loss: 0.000014 | Test Loss: 0.000118 | Test Accuracy: 0.891457\n",
      "Step 40 | Training Loss: 0.000011 | Test Loss: 0.000097 | Test Accuracy: 0.889239\n",
      "Step 41 | Training Loss: 0.000018 | Test Loss: 0.000129 | Test Accuracy: 0.892388\n",
      "Step 42 | Training Loss: 0.000019 | Test Loss: 0.000120 | Test Accuracy: 0.890259\n",
      "Step 43 | Training Loss: 0.000024 | Test Loss: 0.000155 | Test Accuracy: 0.892300\n",
      "Step 44 | Training Loss: 0.000016 | Test Loss: 0.000159 | Test Accuracy: 0.904587\n",
      "Step 45 | Training Loss: 0.000020 | Test Loss: 0.000172 | Test Accuracy: 0.903123\n",
      "Step 46 | Training Loss: 0.000005 | Test Loss: 0.000117 | Test Accuracy: 0.901925\n",
      "Step 47 | Training Loss: 0.000014 | Test Loss: 0.000141 | Test Accuracy: 0.887553\n",
      "Step 48 | Training Loss: 0.000025 | Test Loss: 0.000128 | Test Accuracy: 0.891235\n",
      "Step 49 | Training Loss: 0.000004 | Test Loss: 0.000129 | Test Accuracy: 0.883517\n",
      "Step 50 | Training Loss: 0.000055 | Test Loss: 0.000156 | Test Accuracy: 0.887287\n",
      "Best Accuracy on Test data: 0.9045866131782532\n",
      "Current Layer Attributes - epochs:50 hidden layers:2 features count:32\n",
      "Step 1 | Training Loss: 0.000077 | Test Loss: 0.000381 | Test Accuracy: 0.809705\n",
      "Step 2 | Training Loss: 0.000078 | Test Loss: 0.000230 | Test Accuracy: 0.840578\n",
      "Step 3 | Training Loss: 0.000025 | Test Loss: 0.000169 | Test Accuracy: 0.850736\n",
      "Step 4 | Training Loss: 0.000187 | Test Loss: 0.000239 | Test Accuracy: 0.801278\n",
      "Step 5 | Training Loss: 0.000034 | Test Loss: 0.000232 | Test Accuracy: 0.876508\n",
      "Step 6 | Training Loss: 0.000023 | Test Loss: 0.000160 | Test Accuracy: 0.887731\n",
      "Step 7 | Training Loss: 0.000130 | Test Loss: 0.000184 | Test Accuracy: 0.888440\n",
      "Step 8 | Training Loss: 0.000051 | Test Loss: 0.000184 | Test Accuracy: 0.876996\n",
      "Step 9 | Training Loss: 0.000017 | Test Loss: 0.000195 | Test Accuracy: 0.886666\n",
      "Step 10 | Training Loss: 0.000053 | Test Loss: 0.000150 | Test Accuracy: 0.890924\n",
      "Step 11 | Training Loss: 0.000015 | Test Loss: 0.000102 | Test Accuracy: 0.890392\n",
      "Step 12 | Training Loss: 0.000102 | Test Loss: 0.000170 | Test Accuracy: 0.890969\n",
      "Step 13 | Training Loss: 0.000050 | Test Loss: 0.000107 | Test Accuracy: 0.891013\n",
      "Step 14 | Training Loss: 0.000059 | Test Loss: 0.000107 | Test Accuracy: 0.873802\n",
      "Step 15 | Training Loss: 0.000008 | Test Loss: 0.000098 | Test Accuracy: 0.865153\n",
      "Step 16 | Training Loss: 0.000057 | Test Loss: 0.000153 | Test Accuracy: 0.898643\n",
      "Step 17 | Training Loss: 0.000033 | Test Loss: 0.000079 | Test Accuracy: 0.876153\n",
      "Step 18 | Training Loss: 0.000047 | Test Loss: 0.000116 | Test Accuracy: 0.885823\n",
      "Step 19 | Training Loss: 0.000054 | Test Loss: 0.000113 | Test Accuracy: 0.887819\n",
      "Step 20 | Training Loss: 0.000038 | Test Loss: 0.000083 | Test Accuracy: 0.890392\n",
      "Step 21 | Training Loss: 0.000034 | Test Loss: 0.000145 | Test Accuracy: 0.882984\n",
      "Step 22 | Training Loss: 0.000010 | Test Loss: 0.000098 | Test Accuracy: 0.879214\n",
      "Step 23 | Training Loss: 0.000021 | Test Loss: 0.000123 | Test Accuracy: 0.881875\n",
      "Step 24 | Training Loss: 0.000008 | Test Loss: 0.000081 | Test Accuracy: 0.882630\n",
      "Step 25 | Training Loss: 0.000013 | Test Loss: 0.000133 | Test Accuracy: 0.890703\n",
      "Step 26 | Training Loss: 0.000014 | Test Loss: 0.000091 | Test Accuracy: 0.877617\n",
      "Step 27 | Training Loss: 0.000024 | Test Loss: 0.000073 | Test Accuracy: 0.881521\n",
      "Step 28 | Training Loss: 0.000006 | Test Loss: 0.000098 | Test Accuracy: 0.881343\n",
      "Step 29 | Training Loss: 0.000024 | Test Loss: 0.000122 | Test Accuracy: 0.877129\n",
      "Step 30 | Training Loss: 0.000023 | Test Loss: 0.000102 | Test Accuracy: 0.876331\n",
      "Step 31 | Training Loss: 0.000004 | Test Loss: 0.000095 | Test Accuracy: 0.877617\n",
      "Step 32 | Training Loss: 0.000013 | Test Loss: 0.000160 | Test Accuracy: 0.881875\n",
      "Step 33 | Training Loss: 0.000031 | Test Loss: 0.000105 | Test Accuracy: 0.880234\n",
      "Step 34 | Training Loss: 0.000051 | Test Loss: 0.000127 | Test Accuracy: 0.881476\n",
      "Step 35 | Training Loss: 0.000009 | Test Loss: 0.000118 | Test Accuracy: 0.884182\n",
      "Step 36 | Training Loss: 0.000051 | Test Loss: 0.000108 | Test Accuracy: 0.882142\n",
      "Step 37 | Training Loss: 0.000007 | Test Loss: 0.000114 | Test Accuracy: 0.891279\n",
      "Step 38 | Training Loss: 0.000013 | Test Loss: 0.000127 | Test Accuracy: 0.880988\n",
      "Step 39 | Training Loss: 0.000035 | Test Loss: 0.000142 | Test Accuracy: 0.883472\n",
      "Step 40 | Training Loss: 0.000013 | Test Loss: 0.000118 | Test Accuracy: 0.883295\n",
      "Step 41 | Training Loss: 0.000026 | Test Loss: 0.000155 | Test Accuracy: 0.885380\n",
      "Step 42 | Training Loss: 0.000054 | Test Loss: 0.000120 | Test Accuracy: 0.897312\n",
      "Step 43 | Training Loss: 0.000020 | Test Loss: 0.000145 | Test Accuracy: 0.883384\n",
      "Step 44 | Training Loss: 0.000031 | Test Loss: 0.000120 | Test Accuracy: 0.884138\n",
      "Step 45 | Training Loss: 0.000008 | Test Loss: 0.000126 | Test Accuracy: 0.881654\n",
      "Step 46 | Training Loss: 0.000020 | Test Loss: 0.000176 | Test Accuracy: 0.881077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 47 | Training Loss: 0.000022 | Test Loss: 0.000070 | Test Accuracy: 0.866350\n",
      "Step 48 | Training Loss: 0.000021 | Test Loss: 0.000080 | Test Accuracy: 0.884226\n",
      "Step 49 | Training Loss: 0.000029 | Test Loss: 0.000134 | Test Accuracy: 0.894384\n",
      "Step 50 | Training Loss: 0.000072 | Test Loss: 0.000033 | Test Accuracy: 0.891501\n",
      "Best Accuracy on Test data: 0.8986426591873169\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:4\n",
      "Step 1 | Training Loss: 0.000005 | Test Loss: 0.000269 | Test Accuracy: 0.697569\n",
      "Step 2 | Training Loss: 0.000027 | Test Loss: 0.000721 | Test Accuracy: 0.741838\n",
      "Step 3 | Training Loss: 0.000217 | Test Loss: 0.000516 | Test Accuracy: 0.707328\n",
      "Step 4 | Training Loss: 0.000101 | Test Loss: 0.000411 | Test Accuracy: 0.741040\n",
      "Step 5 | Training Loss: 0.000112 | Test Loss: 0.000360 | Test Accuracy: 0.729640\n",
      "Step 6 | Training Loss: 0.000048 | Test Loss: 0.000232 | Test Accuracy: 0.758783\n",
      "Step 7 | Training Loss: 0.000095 | Test Loss: 0.000213 | Test Accuracy: 0.773155\n",
      "Step 8 | Training Loss: 0.000072 | Test Loss: 0.000297 | Test Accuracy: 0.784688\n",
      "Step 9 | Training Loss: 0.000051 | Test Loss: 0.000296 | Test Accuracy: 0.781095\n",
      "Step 10 | Training Loss: 0.000021 | Test Loss: 0.000126 | Test Accuracy: 0.791297\n",
      "Step 11 | Training Loss: 0.000080 | Test Loss: 0.000172 | Test Accuracy: 0.818488\n",
      "Step 12 | Training Loss: 0.000025 | Test Loss: 0.000127 | Test Accuracy: 0.817645\n",
      "Step 13 | Training Loss: 0.000036 | Test Loss: 0.000202 | Test Accuracy: 0.816492\n",
      "Step 14 | Training Loss: 0.000010 | Test Loss: 0.000143 | Test Accuracy: 0.808508\n",
      "Step 15 | Training Loss: 0.000033 | Test Loss: 0.000180 | Test Accuracy: 0.807709\n",
      "Step 16 | Training Loss: 0.000065 | Test Loss: 0.000246 | Test Accuracy: 0.821593\n",
      "Step 17 | Training Loss: 0.000023 | Test Loss: 0.000222 | Test Accuracy: 0.819508\n",
      "Step 18 | Training Loss: 0.000062 | Test Loss: 0.000188 | Test Accuracy: 0.832993\n",
      "Step 19 | Training Loss: 0.000049 | Test Loss: 0.000218 | Test Accuracy: 0.833836\n",
      "Step 20 | Training Loss: 0.000012 | Test Loss: 0.000252 | Test Accuracy: 0.826295\n",
      "Step 21 | Training Loss: 0.000044 | Test Loss: 0.000226 | Test Accuracy: 0.843107\n",
      "Step 22 | Training Loss: 0.000009 | Test Loss: 0.000178 | Test Accuracy: 0.847055\n",
      "Step 23 | Training Loss: 0.000014 | Test Loss: 0.000191 | Test Accuracy: 0.847010\n",
      "Step 24 | Training Loss: 0.000060 | Test Loss: 0.000090 | Test Accuracy: 0.833126\n",
      "Step 25 | Training Loss: 0.000060 | Test Loss: 0.000111 | Test Accuracy: 0.833659\n",
      "Step 26 | Training Loss: 0.000033 | Test Loss: 0.000237 | Test Accuracy: 0.841776\n",
      "Step 27 | Training Loss: 0.000057 | Test Loss: 0.000118 | Test Accuracy: 0.842929\n",
      "Step 28 | Training Loss: 0.000055 | Test Loss: 0.000178 | Test Accuracy: 0.841865\n",
      "Step 29 | Training Loss: 0.000028 | Test Loss: 0.000160 | Test Accuracy: 0.837606\n",
      "Step 30 | Training Loss: 0.000069 | Test Loss: 0.000090 | Test Accuracy: 0.844260\n",
      "Step 31 | Training Loss: 0.000035 | Test Loss: 0.000176 | Test Accuracy: 0.836231\n",
      "Step 32 | Training Loss: 0.000015 | Test Loss: 0.000139 | Test Accuracy: 0.832905\n",
      "Step 33 | Training Loss: 0.000027 | Test Loss: 0.000185 | Test Accuracy: 0.838538\n",
      "Step 34 | Training Loss: 0.000076 | Test Loss: 0.000061 | Test Accuracy: 0.814629\n",
      "Step 35 | Training Loss: 0.000043 | Test Loss: 0.000134 | Test Accuracy: 0.817424\n",
      "Step 36 | Training Loss: 0.000019 | Test Loss: 0.000203 | Test Accuracy: 0.824255\n",
      "Step 37 | Training Loss: 0.000005 | Test Loss: 0.000153 | Test Accuracy: 0.820307\n",
      "Step 38 | Training Loss: 0.000032 | Test Loss: 0.000229 | Test Accuracy: 0.823811\n",
      "Step 39 | Training Loss: 0.000068 | Test Loss: 0.000114 | Test Accuracy: 0.822968\n",
      "Step 40 | Training Loss: 0.000074 | Test Loss: 0.000094 | Test Accuracy: 0.827981\n",
      "Step 41 | Training Loss: 0.000082 | Test Loss: 0.000116 | Test Accuracy: 0.826650\n",
      "Step 42 | Training Loss: 0.000028 | Test Loss: 0.000135 | Test Accuracy: 0.817424\n",
      "Step 43 | Training Loss: 0.000007 | Test Loss: 0.000204 | Test Accuracy: 0.818089\n",
      "Step 44 | Training Loss: 0.000019 | Test Loss: 0.000198 | Test Accuracy: 0.825053\n",
      "Step 45 | Training Loss: 0.000021 | Test Loss: 0.000234 | Test Accuracy: 0.815694\n",
      "Step 46 | Training Loss: 0.000014 | Test Loss: 0.000151 | Test Accuracy: 0.830908\n",
      "Step 47 | Training Loss: 0.000016 | Test Loss: 0.000165 | Test Accuracy: 0.813254\n",
      "Step 48 | Training Loss: 0.000019 | Test Loss: 0.000179 | Test Accuracy: 0.810504\n",
      "Step 49 | Training Loss: 0.000014 | Test Loss: 0.000195 | Test Accuracy: 0.798217\n",
      "Step 50 | Training Loss: 0.000068 | Test Loss: 0.000263 | Test Accuracy: 0.807976\n",
      "Best Accuracy on Test data: 0.847054660320282\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:8\n",
      "Step 1 | Training Loss: 0.000077 | Test Loss: 0.000483 | Test Accuracy: 0.671930\n",
      "Step 2 | Training Loss: 0.000204 | Test Loss: 0.002463 | Test Accuracy: 0.622871\n",
      "Step 3 | Training Loss: 0.000022 | Test Loss: 0.000522 | Test Accuracy: 0.752484\n",
      "Step 4 | Training Loss: 0.000095 | Test Loss: 0.000301 | Test Accuracy: 0.762243\n",
      "Step 5 | Training Loss: 0.000046 | Test Loss: 0.000303 | Test Accuracy: 0.809883\n",
      "Step 6 | Training Loss: 0.000055 | Test Loss: 0.000351 | Test Accuracy: 0.698944\n",
      "Step 7 | Training Loss: 0.000001 | Test Loss: 0.000432 | Test Accuracy: 0.698101\n",
      "Step 8 | Training Loss: 0.000033 | Test Loss: 0.000389 | Test Accuracy: 0.688165\n",
      "Step 9 | Training Loss: 0.000122 | Test Loss: 0.000275 | Test Accuracy: 0.658978\n",
      "Step 10 | Training Loss: 0.000022 | Test Loss: 0.000411 | Test Accuracy: 0.668604\n",
      "Step 11 | Training Loss: 0.000063 | Test Loss: 0.000402 | Test Accuracy: 0.683330\n",
      "Step 12 | Training Loss: 0.000015 | Test Loss: 0.000370 | Test Accuracy: 0.712651\n",
      "Step 13 | Training Loss: 0.000018 | Test Loss: 0.000434 | Test Accuracy: 0.686214\n",
      "Step 14 | Training Loss: 0.000058 | Test Loss: 0.000411 | Test Accuracy: 0.695263\n",
      "Step 15 | Training Loss: 0.000030 | Test Loss: 0.000400 | Test Accuracy: 0.623093\n",
      "Step 16 | Training Loss: 0.000034 | Test Loss: 0.000439 | Test Accuracy: 0.623137\n",
      "Step 17 | Training Loss: 0.000077 | Test Loss: 0.000386 | Test Accuracy: 0.681911\n",
      "Step 18 | Training Loss: 0.000003 | Test Loss: 0.000391 | Test Accuracy: 0.663369\n",
      "Step 19 | Training Loss: 0.000000 | Test Loss: 0.000353 | Test Accuracy: 0.685992\n",
      "Step 20 | Training Loss: 0.000023 | Test Loss: 0.000353 | Test Accuracy: 0.733233\n",
      "Step 21 | Training Loss: 0.000018 | Test Loss: 0.000365 | Test Accuracy: 0.724539\n",
      "Step 22 | Training Loss: 0.000007 | Test Loss: 0.000345 | Test Accuracy: 0.713893\n",
      "Step 23 | Training Loss: 0.000041 | Test Loss: 0.000350 | Test Accuracy: 0.714514\n",
      "Step 24 | Training Loss: 0.000012 | Test Loss: 0.000354 | Test Accuracy: 0.715711\n",
      "Step 25 | Training Loss: 0.000012 | Test Loss: 0.000322 | Test Accuracy: 0.730616\n",
      "Step 26 | Training Loss: 0.000026 | Test Loss: 0.000304 | Test Accuracy: 0.738556\n",
      "Step 27 | Training Loss: 0.000017 | Test Loss: 0.000287 | Test Accuracy: 0.730394\n",
      "Step 28 | Training Loss: 0.000024 | Test Loss: 0.000374 | Test Accuracy: 0.654631\n",
      "Step 29 | Training Loss: 0.000022 | Test Loss: 0.000304 | Test Accuracy: 0.716643\n",
      "Step 30 | Training Loss: 0.000009 | Test Loss: 0.000327 | Test Accuracy: 0.718417\n",
      "Step 31 | Training Loss: 0.000040 | Test Loss: 0.000260 | Test Accuracy: 0.748048\n",
      "Step 32 | Training Loss: 0.000020 | Test Loss: 0.000269 | Test Accuracy: 0.756875\n",
      "Step 33 | Training Loss: 0.000034 | Test Loss: 0.000248 | Test Accuracy: 0.759138\n",
      "Step 34 | Training Loss: 0.000021 | Test Loss: 0.000169 | Test Accuracy: 0.892078\n",
      "Step 35 | Training Loss: 0.000007 | Test Loss: 0.000109 | Test Accuracy: 0.890880\n",
      "Step 36 | Training Loss: 0.000054 | Test Loss: 0.000112 | Test Accuracy: 0.880811\n",
      "Step 37 | Training Loss: 0.000000 | Test Loss: 0.000126 | Test Accuracy: 0.867903\n",
      "Step 38 | Training Loss: 0.000038 | Test Loss: 0.000146 | Test Accuracy: 0.861338\n",
      "Step 39 | Training Loss: 0.000007 | Test Loss: 0.000146 | Test Accuracy: 0.857346\n",
      "Step 40 | Training Loss: 0.000018 | Test Loss: 0.000143 | Test Accuracy: 0.846833\n",
      "Step 41 | Training Loss: 0.000013 | Test Loss: 0.000157 | Test Accuracy: 0.837740\n",
      "Step 42 | Training Loss: 0.000008 | Test Loss: 0.000116 | Test Accuracy: 0.838139\n",
      "Step 43 | Training Loss: 0.000044 | Test Loss: 0.000148 | Test Accuracy: 0.837784\n",
      "Step 44 | Training Loss: 0.000008 | Test Loss: 0.000148 | Test Accuracy: 0.889239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 45 | Training Loss: 0.000009 | Test Loss: 0.000135 | Test Accuracy: 0.856370\n",
      "Step 46 | Training Loss: 0.000025 | Test Loss: 0.000172 | Test Accuracy: 0.835300\n",
      "Step 47 | Training Loss: 0.000043 | Test Loss: 0.000120 | Test Accuracy: 0.839514\n",
      "Step 48 | Training Loss: 0.000016 | Test Loss: 0.000140 | Test Accuracy: 0.846212\n",
      "Step 49 | Training Loss: 0.000015 | Test Loss: 0.000152 | Test Accuracy: 0.844349\n",
      "Step 50 | Training Loss: 0.000009 | Test Loss: 0.000130 | Test Accuracy: 0.848607\n",
      "Best Accuracy on Test data: 0.8920777440071106\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:16\n",
      "Step 1 | Training Loss: 0.000209 | Test Loss: 0.000420 | Test Accuracy: 0.792362\n",
      "Step 2 | Training Loss: 0.000095 | Test Loss: 0.000276 | Test Accuracy: 0.793559\n",
      "Step 3 | Training Loss: 0.000043 | Test Loss: 0.000713 | Test Accuracy: 0.862935\n",
      "Step 4 | Training Loss: 0.000021 | Test Loss: 0.000271 | Test Accuracy: 0.789700\n",
      "Step 5 | Training Loss: 0.000011 | Test Loss: 0.000134 | Test Accuracy: 0.815383\n",
      "Step 6 | Training Loss: 0.000006 | Test Loss: 0.000176 | Test Accuracy: 0.833126\n",
      "Step 7 | Training Loss: 0.000097 | Test Loss: 0.000253 | Test Accuracy: 0.828114\n",
      "Step 8 | Training Loss: 0.000003 | Test Loss: 0.000200 | Test Accuracy: 0.850160\n",
      "Step 9 | Training Loss: 0.000034 | Test Loss: 0.000173 | Test Accuracy: 0.844260\n",
      "Step 10 | Training Loss: 0.000006 | Test Loss: 0.000122 | Test Accuracy: 0.853043\n",
      "Step 11 | Training Loss: 0.000035 | Test Loss: 0.000202 | Test Accuracy: 0.861471\n",
      "Step 12 | Training Loss: 0.000006 | Test Loss: 0.000103 | Test Accuracy: 0.855837\n",
      "Step 13 | Training Loss: 0.000006 | Test Loss: 0.000150 | Test Accuracy: 0.880722\n",
      "Step 14 | Training Loss: 0.000053 | Test Loss: 0.000112 | Test Accuracy: 0.861072\n",
      "Step 15 | Training Loss: 0.000034 | Test Loss: 0.000159 | Test Accuracy: 0.829356\n",
      "Step 16 | Training Loss: 0.000116 | Test Loss: 0.000084 | Test Accuracy: 0.853575\n",
      "Step 17 | Training Loss: 0.000021 | Test Loss: 0.000134 | Test Accuracy: 0.830021\n",
      "Step 18 | Training Loss: 0.000029 | Test Loss: 0.000139 | Test Accuracy: 0.823412\n",
      "Step 19 | Training Loss: 0.000010 | Test Loss: 0.000158 | Test Accuracy: 0.826828\n",
      "Step 20 | Training Loss: 0.000030 | Test Loss: 0.000191 | Test Accuracy: 0.838449\n",
      "Step 21 | Training Loss: 0.000039 | Test Loss: 0.000188 | Test Accuracy: 0.844526\n",
      "Step 22 | Training Loss: 0.000004 | Test Loss: 0.000117 | Test Accuracy: 0.846123\n",
      "Step 23 | Training Loss: 0.000012 | Test Loss: 0.000128 | Test Accuracy: 0.843062\n",
      "Step 24 | Training Loss: 0.000006 | Test Loss: 0.000114 | Test Accuracy: 0.837385\n",
      "Step 25 | Training Loss: 0.000005 | Test Loss: 0.000121 | Test Accuracy: 0.828469\n",
      "Step 26 | Training Loss: 0.000028 | Test Loss: 0.000118 | Test Accuracy: 0.839159\n",
      "Step 27 | Training Loss: 0.000059 | Test Loss: 0.000156 | Test Accuracy: 0.825586\n",
      "Step 28 | Training Loss: 0.000018 | Test Loss: 0.000083 | Test Accuracy: 0.833393\n",
      "Step 29 | Training Loss: 0.000027 | Test Loss: 0.000067 | Test Accuracy: 0.823013\n",
      "Step 30 | Training Loss: 0.000026 | Test Loss: 0.000104 | Test Accuracy: 0.825763\n",
      "Step 31 | Training Loss: 0.000001 | Test Loss: 0.000136 | Test Accuracy: 0.829400\n",
      "Step 32 | Training Loss: 0.000018 | Test Loss: 0.000135 | Test Accuracy: 0.830953\n",
      "Step 33 | Training Loss: 0.000020 | Test Loss: 0.000133 | Test Accuracy: 0.827182\n",
      "Step 34 | Training Loss: 0.000001 | Test Loss: 0.000120 | Test Accuracy: 0.829489\n",
      "Step 35 | Training Loss: 0.000027 | Test Loss: 0.000130 | Test Accuracy: 0.830687\n",
      "Step 36 | Training Loss: 0.000005 | Test Loss: 0.000118 | Test Accuracy: 0.830997\n",
      "Step 37 | Training Loss: 0.000007 | Test Loss: 0.000141 | Test Accuracy: 0.832949\n",
      "Step 38 | Training Loss: 0.000014 | Test Loss: 0.000118 | Test Accuracy: 0.833747\n",
      "Step 39 | Training Loss: 0.000022 | Test Loss: 0.000106 | Test Accuracy: 0.833038\n",
      "Step 40 | Training Loss: 0.000002 | Test Loss: 0.000137 | Test Accuracy: 0.832550\n",
      "Step 41 | Training Loss: 0.000017 | Test Loss: 0.000131 | Test Accuracy: 0.830465\n",
      "Step 42 | Training Loss: 0.000017 | Test Loss: 0.000116 | Test Accuracy: 0.850559\n",
      "Step 43 | Training Loss: 0.000072 | Test Loss: 0.000123 | Test Accuracy: 0.834324\n",
      "Step 44 | Training Loss: 0.000010 | Test Loss: 0.000096 | Test Accuracy: 0.826029\n",
      "Step 45 | Training Loss: 0.000005 | Test Loss: 0.000109 | Test Accuracy: 0.830687\n",
      "Step 46 | Training Loss: 0.000046 | Test Loss: 0.000126 | Test Accuracy: 0.828513\n",
      "Step 47 | Training Loss: 0.000024 | Test Loss: 0.000182 | Test Accuracy: 0.832505\n",
      "Step 48 | Training Loss: 0.000005 | Test Loss: 0.000111 | Test Accuracy: 0.835167\n",
      "Step 49 | Training Loss: 0.000000 | Test Loss: 0.000115 | Test Accuracy: 0.834013\n",
      "Step 50 | Training Loss: 0.000014 | Test Loss: 0.000112 | Test Accuracy: 0.831396\n",
      "Best Accuracy on Test data: 0.880722165107727\n",
      "Current Layer Attributes - epochs:50 hidden layers:4 features count:32\n",
      "Step 1 | Training Loss: 0.000226 | Test Loss: 0.000089 | Test Accuracy: 0.775816\n",
      "Step 2 | Training Loss: 0.000256 | Test Loss: 0.000363 | Test Accuracy: 0.809218\n",
      "Step 3 | Training Loss: 0.000089 | Test Loss: 0.000241 | Test Accuracy: 0.792583\n",
      "Step 4 | Training Loss: 0.000007 | Test Loss: 0.000254 | Test Accuracy: 0.851890\n",
      "Step 5 | Training Loss: 0.000076 | Test Loss: 0.000257 | Test Accuracy: 0.847365\n",
      "Step 6 | Training Loss: 0.000105 | Test Loss: 0.000287 | Test Accuracy: 0.828868\n",
      "Step 7 | Training Loss: 0.000003 | Test Loss: 0.000259 | Test Accuracy: 0.786329\n",
      "Step 8 | Training Loss: 0.000022 | Test Loss: 0.000288 | Test Accuracy: 0.797418\n",
      "Step 9 | Training Loss: 0.000031 | Test Loss: 0.000242 | Test Accuracy: 0.808286\n",
      "Step 10 | Training Loss: 0.000038 | Test Loss: 0.000239 | Test Accuracy: 0.808020\n",
      "Step 11 | Training Loss: 0.000071 | Test Loss: 0.000329 | Test Accuracy: 0.812234\n",
      "Step 12 | Training Loss: 0.000051 | Test Loss: 0.000246 | Test Accuracy: 0.823856\n",
      "Step 13 | Training Loss: 0.000015 | Test Loss: 0.000206 | Test Accuracy: 0.822392\n",
      "Step 14 | Training Loss: 0.000022 | Test Loss: 0.000143 | Test Accuracy: 0.818045\n",
      "Step 15 | Training Loss: 0.000063 | Test Loss: 0.000077 | Test Accuracy: 0.814718\n",
      "Step 16 | Training Loss: 0.000115 | Test Loss: 0.000105 | Test Accuracy: 0.821904\n",
      "Step 17 | Training Loss: 0.000031 | Test Loss: 0.000132 | Test Accuracy: 0.810460\n",
      "Step 18 | Training Loss: 0.000029 | Test Loss: 0.000151 | Test Accuracy: 0.805048\n",
      "Step 19 | Training Loss: 0.000009 | Test Loss: 0.000224 | Test Accuracy: 0.801011\n",
      "Step 20 | Training Loss: 0.000116 | Test Loss: 0.000125 | Test Accuracy: 0.822525\n",
      "Step 21 | Training Loss: 0.000059 | Test Loss: 0.000210 | Test Accuracy: 0.813831\n",
      "Step 22 | Training Loss: 0.000013 | Test Loss: 0.000213 | Test Accuracy: 0.819021\n",
      "Step 23 | Training Loss: 0.000029 | Test Loss: 0.000162 | Test Accuracy: 0.824166\n",
      "Step 24 | Training Loss: 0.000013 | Test Loss: 0.000183 | Test Accuracy: 0.824344\n",
      "Step 25 | Training Loss: 0.000054 | Test Loss: 0.000167 | Test Accuracy: 0.826251\n",
      "Step 26 | Training Loss: 0.000078 | Test Loss: 0.000166 | Test Accuracy: 0.828424\n",
      "Step 27 | Training Loss: 0.000079 | Test Loss: 0.000103 | Test Accuracy: 0.829977\n",
      "Step 28 | Training Loss: 0.000025 | Test Loss: 0.000108 | Test Accuracy: 0.831219\n",
      "Step 29 | Training Loss: 0.000035 | Test Loss: 0.000159 | Test Accuracy: 0.830376\n",
      "Step 30 | Training Loss: 0.000014 | Test Loss: 0.000154 | Test Accuracy: 0.831973\n",
      "Step 31 | Training Loss: 0.000018 | Test Loss: 0.000123 | Test Accuracy: 0.830598\n",
      "Step 32 | Training Loss: 0.000039 | Test Loss: 0.000113 | Test Accuracy: 0.831529\n",
      "Step 33 | Training Loss: 0.000054 | Test Loss: 0.000170 | Test Accuracy: 0.830465\n",
      "Step 34 | Training Loss: 0.000021 | Test Loss: 0.000190 | Test Accuracy: 0.830066\n",
      "Step 35 | Training Loss: 0.000055 | Test Loss: 0.000140 | Test Accuracy: 0.830731\n",
      "Step 36 | Training Loss: 0.000037 | Test Loss: 0.000236 | Test Accuracy: 0.829090\n",
      "Step 37 | Training Loss: 0.000001 | Test Loss: 0.000131 | Test Accuracy: 0.830908\n",
      "Step 38 | Training Loss: 0.000006 | Test Loss: 0.000151 | Test Accuracy: 0.831086\n",
      "Step 39 | Training Loss: 0.000064 | Test Loss: 0.000140 | Test Accuracy: 0.823235\n",
      "Step 40 | Training Loss: 0.000059 | Test Loss: 0.000193 | Test Accuracy: 0.832017\n",
      "Step 41 | Training Loss: 0.000001 | Test Loss: 0.000091 | Test Accuracy: 0.829711\n",
      "Step 42 | Training Loss: 0.000065 | Test Loss: 0.000085 | Test Accuracy: 0.829933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 43 | Training Loss: 0.000023 | Test Loss: 0.000093 | Test Accuracy: 0.831042\n",
      "Step 44 | Training Loss: 0.000016 | Test Loss: 0.000112 | Test Accuracy: 0.824565\n",
      "Step 45 | Training Loss: 0.000005 | Test Loss: 0.000120 | Test Accuracy: 0.832683\n",
      "Step 46 | Training Loss: 0.000020 | Test Loss: 0.000238 | Test Accuracy: 0.820706\n",
      "Step 47 | Training Loss: 0.000010 | Test Loss: 0.000205 | Test Accuracy: 0.826872\n",
      "Step 48 | Training Loss: 0.000062 | Test Loss: 0.000124 | Test Accuracy: 0.851446\n",
      "Step 49 | Training Loss: 0.000028 | Test Loss: 0.000114 | Test Accuracy: 0.853487\n",
      "Step 50 | Training Loss: 0.000012 | Test Loss: 0.000061 | Test Accuracy: 0.856104\n",
      "Best Accuracy on Test data: 0.8561035990715027\n",
      "Current Layer Attributes - epochs:50 hidden layers:6 features count:4\n",
      "Step 1 | Training Loss: 0.000086 | Test Loss: 0.000304 | Test Accuracy: 0.724494\n",
      "Step 2 | Training Loss: 0.000265 | Test Loss: 0.000363 | Test Accuracy: 0.777502\n",
      "Step 3 | Training Loss: 0.000039 | Test Loss: 0.000144 | Test Accuracy: 0.810149\n",
      "Step 4 | Training Loss: 0.000014 | Test Loss: 0.000233 | Test Accuracy: 0.827936\n",
      "Step 5 | Training Loss: 0.000019 | Test Loss: 0.000211 | Test Accuracy: 0.820795\n",
      "Step 6 | Training Loss: 0.000043 | Test Loss: 0.000217 | Test Accuracy: 0.829533\n",
      "Step 7 | Training Loss: 0.000020 | Test Loss: 0.000279 | Test Accuracy: 0.810504\n",
      "Step 8 | Training Loss: 0.000060 | Test Loss: 0.000235 | Test Accuracy: 0.839425\n",
      "Step 9 | Training Loss: 0.000058 | Test Loss: 0.000474 | Test Accuracy: 0.811968\n",
      "Step 10 | Training Loss: 0.000205 | Test Loss: 0.000335 | Test Accuracy: 0.817024\n",
      "Step 11 | Training Loss: 0.000083 | Test Loss: 0.000264 | Test Accuracy: 0.817912\n",
      "Step 12 | Training Loss: 0.000012 | Test Loss: 0.000318 | Test Accuracy: 0.835832\n",
      "Step 13 | Training Loss: 0.000010 | Test Loss: 0.000297 | Test Accuracy: 0.788414\n",
      "Step 14 | Training Loss: 0.000033 | Test Loss: 0.000267 | Test Accuracy: 0.802475\n",
      "Step 15 | Training Loss: 0.000044 | Test Loss: 0.000206 | Test Accuracy: 0.779853\n",
      "Step 16 | Training Loss: 0.000043 | Test Loss: 0.000256 | Test Accuracy: 0.814540\n",
      "Step 17 | Training Loss: 0.000059 | Test Loss: 0.000279 | Test Accuracy: 0.761489\n",
      "Step 18 | Training Loss: 0.000028 | Test Loss: 0.000291 | Test Accuracy: 0.755500\n",
      "Step 19 | Training Loss: 0.000096 | Test Loss: 0.000351 | Test Accuracy: 0.778744\n",
      "Step 20 | Training Loss: 0.000001 | Test Loss: 0.000288 | Test Accuracy: 0.759892\n",
      "Step 21 | Training Loss: 0.000035 | Test Loss: 0.000293 | Test Accuracy: 0.727910\n",
      "Step 22 | Training Loss: 0.000026 | Test Loss: 0.000283 | Test Accuracy: 0.754303\n",
      "Step 23 | Training Loss: 0.000041 | Test Loss: 0.000300 | Test Accuracy: 0.738955\n",
      "Step 24 | Training Loss: 0.000015 | Test Loss: 0.000209 | Test Accuracy: 0.764549\n",
      "Step 25 | Training Loss: 0.000055 | Test Loss: 0.000205 | Test Accuracy: 0.759404\n",
      "Step 26 | Training Loss: 0.000057 | Test Loss: 0.000290 | Test Accuracy: 0.766723\n",
      "Step 27 | Training Loss: 0.000006 | Test Loss: 0.000250 | Test Accuracy: 0.774042\n",
      "Step 28 | Training Loss: 0.000001 | Test Loss: 0.000184 | Test Accuracy: 0.766900\n",
      "Step 29 | Training Loss: 0.000009 | Test Loss: 0.000237 | Test Accuracy: 0.744899\n",
      "Step 30 | Training Loss: 0.000027 | Test Loss: 0.000203 | Test Accuracy: 0.753194\n",
      "Step 31 | Training Loss: 0.000016 | Test Loss: 0.000233 | Test Accuracy: 0.757275\n",
      "Step 32 | Training Loss: 0.000082 | Test Loss: 0.000278 | Test Accuracy: 0.774929\n",
      "Step 33 | Training Loss: 0.000015 | Test Loss: 0.000206 | Test Accuracy: 0.766191\n",
      "Step 34 | Training Loss: 0.000011 | Test Loss: 0.000236 | Test Accuracy: 0.769961\n",
      "Step 35 | Training Loss: 0.000022 | Test Loss: 0.000266 | Test Accuracy: 0.767787\n",
      "Step 36 | Training Loss: 0.000100 | Test Loss: 0.000170 | Test Accuracy: 0.769118\n",
      "Step 37 | Training Loss: 0.000027 | Test Loss: 0.000223 | Test Accuracy: 0.755057\n",
      "Step 38 | Training Loss: 0.000058 | Test Loss: 0.000255 | Test Accuracy: 0.743213\n",
      "Step 39 | Training Loss: 0.000028 | Test Loss: 0.000165 | Test Accuracy: 0.750266\n",
      "Step 40 | Training Loss: 0.000010 | Test Loss: 0.000281 | Test Accuracy: 0.731458\n",
      "Step 41 | Training Loss: 0.000004 | Test Loss: 0.000194 | Test Accuracy: 0.745520\n",
      "Step 42 | Training Loss: 0.000000 | Test Loss: 0.000224 | Test Accuracy: 0.740507\n",
      "Step 43 | Training Loss: 0.000006 | Test Loss: 0.000251 | Test Accuracy: 0.655696\n",
      "Step 44 | Training Loss: 0.000064 | Test Loss: 0.000231 | Test Accuracy: 0.744855\n",
      "Step 45 | Training Loss: 0.000012 | Test Loss: 0.000282 | Test Accuracy: 0.719393\n",
      "Step 46 | Training Loss: 0.000003 | Test Loss: 0.000268 | Test Accuracy: 0.740374\n",
      "Step 47 | Training Loss: 0.000035 | Test Loss: 0.000272 | Test Accuracy: 0.729152\n",
      "Step 48 | Training Loss: 0.000042 | Test Loss: 0.000267 | Test Accuracy: 0.704045\n",
      "Step 49 | Training Loss: 0.000109 | Test Loss: 0.000291 | Test Accuracy: 0.725337\n",
      "Step 50 | Training Loss: 0.000067 | Test Loss: 0.000256 | Test Accuracy: 0.701118\n",
      "Best Accuracy on Test data: 0.8394251465797424\n",
      "Current Layer Attributes - epochs:50 hidden layers:6 features count:8\n",
      "Step 1 | Training Loss: 0.000244 | Test Loss: 0.000248 | Test Accuracy: 0.693843\n",
      "Step 2 | Training Loss: 0.000380 | Test Loss: 0.000881 | Test Accuracy: 0.545955\n",
      "Step 3 | Training Loss: 0.000120 | Test Loss: 0.000230 | Test Accuracy: 0.803007\n",
      "Step 4 | Training Loss: 0.000075 | Test Loss: 0.000029 | Test Accuracy: 0.838804\n",
      "Step 5 | Training Loss: 0.000057 | Test Loss: 0.000232 | Test Accuracy: 0.815738\n",
      "Step 6 | Training Loss: 0.000080 | Test Loss: 0.000197 | Test Accuracy: 0.822525\n",
      "Step 7 | Training Loss: 0.000002 | Test Loss: 0.000307 | Test Accuracy: 0.764727\n",
      "Step 8 | Training Loss: 0.000039 | Test Loss: 0.000288 | Test Accuracy: 0.842575\n",
      "Step 9 | Training Loss: 0.000014 | Test Loss: 0.000216 | Test Accuracy: 0.815117\n",
      "Step 10 | Training Loss: 0.000008 | Test Loss: 0.000251 | Test Accuracy: 0.825142\n",
      "Step 11 | Training Loss: 0.000003 | Test Loss: 0.000288 | Test Accuracy: 0.811081\n",
      "Step 12 | Training Loss: 0.000000 | Test Loss: 0.000325 | Test Accuracy: 0.804338\n",
      "Step 13 | Training Loss: 0.000071 | Test Loss: 0.000304 | Test Accuracy: 0.764150\n",
      "Step 14 | Training Loss: 0.000062 | Test Loss: 0.000389 | Test Accuracy: 0.711941\n",
      "Step 15 | Training Loss: 0.000076 | Test Loss: 0.000262 | Test Accuracy: 0.780740\n",
      "Step 16 | Training Loss: 0.000046 | Test Loss: 3443604224.000000 | Test Accuracy: 0.777191\n",
      "Step 17 | Training Loss: 284652881974277439488.000000\n",
      "Step 17 | Training Loss: 0.000936 | Test Loss: 0.001160 | Test Accuracy: 0.438831\n",
      "Step 18 | Training Loss: 0.001140 | Test Loss: 0.000131 | Test Accuracy: 0.436169\n",
      "Step 19 | Training Loss: 0.000767 | Test Loss: 0.000925 | Test Accuracy: 0.438254\n",
      "Step 20 | Training Loss: 0.000575 | Test Loss: 0.000970 | Test Accuracy: 0.441936\n",
      "Step 21 | Training Loss: 0.000882 | Test Loss: 0.003396 | Test Accuracy: 0.434351\n",
      "Step 22 | Training Loss: 0.000753 | Test Loss: 0.001025 | Test Accuracy: 0.430713\n",
      "Step 23 | Training Loss: 0.000638 | Test Loss: 0.001037 | Test Accuracy: 0.437234\n",
      "Step 24 | Training Loss: 0.000907 | Test Loss: 0.001190 | Test Accuracy: 0.433463\n",
      "Step 25 | Training Loss: 0.001287 | Test Loss: 0.001278 | Test Accuracy: 0.430846\n",
      "Step 26 | Training Loss: 0.000995 | Test Loss: 0.001195 | Test Accuracy: 0.443666\n",
      "Step 27 | Training Loss: 0.000655 | Test Loss: 0.001103 | Test Accuracy: 0.430758\n",
      "Step 28 | Training Loss: 0.000588 | Test Loss: 0.001001 | Test Accuracy: 0.437855\n",
      "Step 29 | Training Loss: 0.000622 | Test Loss: 0.000972 | Test Accuracy: 0.434528\n",
      "Step 30 | Training Loss: 0.000661 | Test Loss: 0.000949 | Test Accuracy: 0.430758\n",
      "Step 31 | Training Loss: 0.000760 | Test Loss: 0.002175 | Test Accuracy: 0.430758\n",
      "Step 32 | Training Loss: 0.000685 | Test Loss: 0.001072 | Test Accuracy: 0.430891\n",
      "Step 33 | Training Loss: 0.000561 | Test Loss: 0.001301 | Test Accuracy: 0.430758\n",
      "Step 34 | Training Loss: 0.000681 | Test Loss: 0.001065 | Test Accuracy: 0.430758\n",
      "Step 35 | Training Loss: 0.001030 | Test Loss: 0.001177 | Test Accuracy: 0.437944\n",
      "Step 36 | Training Loss: 0.000719 | Test Loss: 0.001067 | Test Accuracy: 0.431911\n",
      "Step 37 | Training Loss: 0.000726 | Test Loss: 0.001029 | Test Accuracy: 0.432532\n",
      "Step 38 | Training Loss: 0.000788 | Test Loss: 0.001020 | Test Accuracy: 0.430758\n",
      "Step 39 | Training Loss: 0.000743 | Test Loss: 0.001026 | Test Accuracy: 0.431334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40 | Training Loss: 0.000686 | Test Loss: 0.001032 | Test Accuracy: 0.430891\n",
      "Step 41 | Training Loss: 0.000691 | Test Loss: 0.000962 | Test Accuracy: 0.442734\n",
      "Step 42 | Training Loss: 0.000646 | Test Loss: 0.001010 | Test Accuracy: 0.430846\n",
      "Step 43 | Training Loss: 0.000676 | Test Loss: 0.001021 | Test Accuracy: 0.432266\n",
      "Step 44 | Training Loss: 0.000653 | Test Loss: 0.001086 | Test Accuracy: 0.430403\n",
      "Step 45 | Training Loss: 0.000583 | Test Loss: 0.001072 | Test Accuracy: 0.430891\n",
      "Step 46 | Training Loss: 0.000740 | Test Loss: 0.001058 | Test Accuracy: 0.430758\n",
      "Step 47 | Training Loss: 0.000640 | Test Loss: 0.001021 | Test Accuracy: 0.436347\n",
      "Step 48 | Training Loss: 0.000670 | Test Loss: 0.001033 | Test Accuracy: 0.430802\n",
      "Step 49 | Training Loss: 0.000703 | Test Loss: 0.001072 | Test Accuracy: 0.433996\n",
      "Step 50 | Training Loss: 0.000710 | Test Loss: 0.001109 | Test Accuracy: 0.433996\n",
      "Best Accuracy on Test data: 0.8425745368003845\n",
      "Current Layer Attributes - epochs:50 hidden layers:6 features count:16\n",
      "Step 1 | Training Loss: 14738451993035014144.000000\n",
      "Step 1 | Training Loss: 0.000456 | Test Loss: 0.000310 | Test Accuracy: 0.702759\n",
      "Step 2 | Training Loss: 0.000029 | Test Loss: 0.000171 | Test Accuracy: 0.837252\n",
      "Step 3 | Training Loss: 0.000005 | Test Loss: 0.000201 | Test Accuracy: 0.804249\n",
      "Step 4 | Training Loss: 0.000054 | Test Loss: 0.000268 | Test Accuracy: 0.801322\n",
      "Step 5 | Training Loss: 0.000004 | Test Loss: 0.000176 | Test Accuracy: 0.831707\n",
      "Step 6 | Training Loss: 0.000024 | Test Loss: 0.000240 | Test Accuracy: 0.796265\n",
      "Step 7 | Training Loss: 0.000007 | Test Loss: 0.000205 | Test Accuracy: 0.821859\n",
      "Step 8 | Training Loss: 0.000022 | Test Loss: 0.000258 | Test Accuracy: 0.829800\n",
      "Step 9 | Training Loss: 0.000066 | Test Loss: 0.000233 | Test Accuracy: 0.820839\n",
      "Step 10 | Training Loss: 0.000048 | Test Loss: 0.000233 | Test Accuracy: 0.825674\n",
      "Step 11 | Training Loss: 0.000037 | Test Loss: 0.000222 | Test Accuracy: 0.814008\n",
      "Step 12 | Training Loss: 0.000011 | Test Loss: 0.000163 | Test Accuracy: 0.843683\n",
      "Step 13 | Training Loss: 0.000081 | Test Loss: 0.000129 | Test Accuracy: 0.858055\n",
      "Step 14 | Training Loss: 0.000009 | Test Loss: 0.000160 | Test Accuracy: 0.846966\n",
      "Step 15 | Training Loss: 0.000048 | Test Loss: 0.000211 | Test Accuracy: 0.817912\n",
      "Step 16 | Training Loss: 0.000037 | Test Loss: 0.000133 | Test Accuracy: 0.854107\n",
      "Step 17 | Training Loss: 0.000045 | Test Loss: 0.000235 | Test Accuracy: 0.821993\n",
      "Step 18 | Training Loss: 0.000077 | Test Loss: 0.000290 | Test Accuracy: 0.832905\n",
      "Step 19 | Training Loss: 0.000041 | Test Loss: 0.000185 | Test Accuracy: 0.831574\n",
      "Step 20 | Training Loss: 0.000048 | Test Loss: 0.000223 | Test Accuracy: 0.811702\n",
      "Step 21 | Training Loss: 0.000012 | Test Loss: 0.000156 | Test Accuracy: 0.847232\n",
      "Step 22 | Training Loss: 0.000038 | Test Loss: 0.000157 | Test Accuracy: 0.819686\n",
      "Step 23 | Training Loss: 0.000002 | Test Loss: 0.000174 | Test Accuracy: 0.837429\n",
      "Step 24 | Training Loss: 0.000012 | Test Loss: 0.000139 | Test Accuracy: 0.823545\n",
      "Step 25 | Training Loss: 0.000027 | Test Loss: 0.000172 | Test Accuracy: 0.845325\n",
      "Step 26 | Training Loss: 0.000061 | Test Loss: 0.000124 | Test Accuracy: 0.848563\n",
      "Step 27 | Training Loss: 0.000015 | Test Loss: 0.000131 | Test Accuracy: 0.851845\n",
      "Step 28 | Training Loss: 0.000024 | Test Loss: 0.000197 | Test Accuracy: 0.810282\n",
      "Step 29 | Training Loss: 0.000049 | Test Loss: 0.000248 | Test Accuracy: 0.834235\n",
      "Step 30 | Training Loss: 0.000044 | Test Loss: 0.000199 | Test Accuracy: 0.831973\n",
      "Step 31 | Training Loss: 0.000011 | Test Loss: 0.000239 | Test Accuracy: 0.831441\n",
      "Step 32 | Training Loss: 0.000034 | Test Loss: 0.000226 | Test Accuracy: 0.889505\n",
      "Step 33 | Training Loss: 0.000021 | Test Loss: 0.000237 | Test Accuracy: 0.861072\n",
      "Step 34 | Training Loss: 0.000015 | Test Loss: 0.000104 | Test Accuracy: 0.874024\n",
      "Step 35 | Training Loss: 0.000059 | Test Loss: 0.000071 | Test Accuracy: 0.871318\n",
      "Step 36 | Training Loss: 0.000042 | Test Loss: 0.000150 | Test Accuracy: 0.857390\n",
      "Step 37 | Training Loss: 0.000024 | Test Loss: 0.000118 | Test Accuracy: 0.869633\n",
      "Step 38 | Training Loss: 0.000007 | Test Loss: 0.000151 | Test Accuracy: 0.842974\n",
      "Step 39 | Training Loss: 0.000017 | Test Loss: 0.000233 | Test Accuracy: 0.789257\n",
      "Step 40 | Training Loss: 0.000040 | Test Loss: 0.000171 | Test Accuracy: 0.875843\n",
      "Step 41 | Training Loss: 0.000051 | Test Loss: 0.000161 | Test Accuracy: 0.883295\n",
      "Step 42 | Training Loss: 0.000014 | Test Loss: 0.000161 | Test Accuracy: 0.862624\n",
      "Step 43 | Training Loss: 0.000054 | Test Loss: 0.000172 | Test Accuracy: 0.876375\n",
      "Step 44 | Training Loss: 0.000032 | Test Loss: 0.000116 | Test Accuracy: 0.878194\n",
      "Step 45 | Training Loss: 0.000005 | Test Loss: 0.000150 | Test Accuracy: 0.859918\n",
      "Step 46 | Training Loss: 0.000048 | Test Loss: 0.000174 | Test Accuracy: 0.853841\n",
      "Step 47 | Training Loss: 0.000003 | Test Loss: 0.000171 | Test Accuracy: 0.855616\n",
      "Step 48 | Training Loss: 0.000007 | Test Loss: 0.000117 | Test Accuracy: 0.856104\n",
      "Step 49 | Training Loss: 0.000036 | Test Loss: 0.000152 | Test Accuracy: 0.860051\n",
      "Step 50 | Training Loss: 0.000014 | Test Loss: 0.000147 | Test Accuracy: 0.855616\n",
      "Best Accuracy on Test data: 0.8895049691200256\n",
      "Current Layer Attributes - epochs:50 hidden layers:6 features count:32\n",
      "Step 1 | Training Loss: 0.000269 | Test Loss: 0.000390 | Test Accuracy: 0.848918\n",
      "Step 2 | Training Loss: 0.000003 | Test Loss: 0.000018 | Test Accuracy: 0.872072\n",
      "Step 3 | Training Loss: 0.000092 | Test Loss: 0.000108 | Test Accuracy: 0.850958\n",
      "Step 4 | Training Loss: 0.000184 | Test Loss: 0.000361 | Test Accuracy: 0.866439\n",
      "Step 5 | Training Loss: 383188325105664.000000\n",
      "Step 5 | Training Loss: 0.000335 | Test Loss: 0.001253 | Test Accuracy: 0.460078\n",
      "Step 6 | Training Loss: 0.000711 | Test Loss: 0.001143 | Test Accuracy: 0.461409\n",
      "Step 7 | Training Loss: 0.000729 | Test Loss: 0.001014 | Test Accuracy: 0.443799\n",
      "Step 8 | Training Loss: 0.000882 | Test Loss: 0.001389 | Test Accuracy: 0.442956\n",
      "Step 9 | Training Loss: 0.001701 | Test Loss: 0.003881 | Test Accuracy: 0.449521\n",
      "Step 10 | Training Loss: 0.000552 | Test Loss: 0.001580 | Test Accuracy: 0.455332\n",
      "Step 11 | Training Loss: 0.000743 | Test Loss: 0.000992 | Test Accuracy: 0.446194\n",
      "Step 12 | Training Loss: 0.000640 | Test Loss: 0.001074 | Test Accuracy: 0.443311\n",
      "Step 13 | Training Loss: 0.000902 | Test Loss: 0.000949 | Test Accuracy: 0.442113\n",
      "Step 14 | Training Loss: 0.000537 | Test Loss: 0.001054 | Test Accuracy: 0.439230\n",
      "Step 15 | Training Loss: 0.000735 | Test Loss: 0.001003 | Test Accuracy: 0.450319\n",
      "Step 16 | Training Loss: 0.001427 | Test Loss: 0.000931 | Test Accuracy: 0.465800\n",
      "Step 17 | Training Loss: 0.000985 | Test Loss: 0.000885 | Test Accuracy: 0.438432\n",
      "Step 18 | Training Loss: 0.000714 | Test Loss: 0.002025 | Test Accuracy: 0.452803\n",
      "Step 19 | Training Loss: 0.000689 | Test Loss: 0.000891 | Test Accuracy: 0.433153\n",
      "Step 20 | Training Loss: 0.000764 | Test Loss: 0.000824 | Test Accuracy: 0.443178\n",
      "Step 21 | Training Loss: 0.000727 | Test Loss: 0.000842 | Test Accuracy: 0.438476\n",
      "Step 22 | Training Loss: 0.000743 | Test Loss: 0.000793 | Test Accuracy: 0.439629\n",
      "Step 23 | Training Loss: 0.000693 | Test Loss: 0.000793 | Test Accuracy: 0.439186\n",
      "Step 24 | Training Loss: 0.000759 | Test Loss: 0.000981 | Test Accuracy: 0.441625\n",
      "Step 25 | Training Loss: 0.000766 | Test Loss: 0.000800 | Test Accuracy: 0.445751\n",
      "Step 26 | Training Loss: 0.000708 | Test Loss: 0.000814 | Test Accuracy: 0.447214\n",
      "Step 27 | Training Loss: 0.001127 | Test Loss: 0.001121 | Test Accuracy: 0.445529\n",
      "Step 28 | Training Loss: 0.003240 | Test Loss: 0.005057 | Test Accuracy: 0.454622\n",
      "Step 29 | Training Loss: 0.000644 | Test Loss: 0.001095 | Test Accuracy: 0.443267\n",
      "Step 30 | Training Loss: 0.000486 | Test Loss: 0.004673 | Test Accuracy: 0.448811\n",
      "Step 31 | Training Loss: 0.000375 | Test Loss: 0.002173 | Test Accuracy: 0.438476\n",
      "Step 32 | Training Loss: 0.000982 | Test Loss: 0.002301 | Test Accuracy: 0.441137\n",
      "Step 33 | Training Loss: 0.000628 | Test Loss: 0.001082 | Test Accuracy: 0.445839\n",
      "Step 34 | Training Loss: 0.000874 | Test Loss: 0.000918 | Test Accuracy: 0.454533\n",
      "Step 35 | Training Loss: 0.000714 | Test Loss: 0.001043 | Test Accuracy: 0.440339\n",
      "Step 36 | Training Loss: 0.000816 | Test Loss: 0.001170 | Test Accuracy: 0.433685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 37 | Training Loss: 0.000783 | Test Loss: 0.001175 | Test Accuracy: 0.440073\n",
      "Step 38 | Training Loss: 0.000931 | Test Loss: 0.001122 | Test Accuracy: 0.435504\n",
      "Step 39 | Training Loss: 0.000850 | Test Loss: 0.001149 | Test Accuracy: 0.434839\n",
      "Step 40 | Training Loss: 0.000612 | Test Loss: 0.001278 | Test Accuracy: 0.468950\n",
      "Step 41 | Training Loss: 0.000599 | Test Loss: 0.001097 | Test Accuracy: 0.457106\n",
      "Step 42 | Training Loss: 0.000746 | Test Loss: 0.001111 | Test Accuracy: 0.452715\n",
      "Step 43 | Training Loss: 0.000733 | Test Loss: 0.001115 | Test Accuracy: 0.437367\n",
      "Step 44 | Training Loss: 0.000735 | Test Loss: 0.001209 | Test Accuracy: 0.447880\n",
      "Step 45 | Training Loss: 0.000784 | Test Loss: 0.001193 | Test Accuracy: 0.439319\n",
      "Step 46 | Training Loss: 0.000651 | Test Loss: 0.001075 | Test Accuracy: 0.439629\n",
      "Step 47 | Training Loss: 0.000635 | Test Loss: 0.001081 | Test Accuracy: 0.440339\n",
      "Step 48 | Training Loss: 0.000695 | Test Loss: 0.001065 | Test Accuracy: 0.439141\n",
      "Step 49 | Training Loss: 0.000674 | Test Loss: 0.001013 | Test Accuracy: 0.447658\n",
      "Step 50 | Training Loss: 0.000835 | Test Loss: 0.001031 | Test Accuracy: 0.447791\n",
      "Best Accuracy on Test data: 0.8720723986625671\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "    features_arr = [4, 8, 16, 32]\n",
    "    hidden_layers_arr = [2, 4, 6]\n",
    "\n",
    "    epochs = [50]\n",
    "    \n",
    "    for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "        print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "        n = network(2,h,f)\n",
    "        n.build_layers()\n",
    "        Train.train(e, n, h,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:50:18.858732Z",
     "start_time": "2017-05-14T20:50:18.853668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(Train.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:50:18.883312Z",
     "start_time": "2017-05-14T20:50:18.860747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.913118</td>\n",
       "      <td>0.907470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.914031</td>\n",
       "      <td>0.905296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891248</td>\n",
       "      <td>0.904587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.881484</td>\n",
       "      <td>0.903123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.888272</td>\n",
       "      <td>0.901925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.888391</td>\n",
       "      <td>0.898643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.889780</td>\n",
       "      <td>0.897312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.894781</td>\n",
       "      <td>0.896292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.915340</td>\n",
       "      <td>0.895227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.851201</td>\n",
       "      <td>0.894384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.897361</td>\n",
       "      <td>0.892388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.896567</td>\n",
       "      <td>0.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.901965</td>\n",
       "      <td>0.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.877753</td>\n",
       "      <td>0.892078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.897599</td>\n",
       "      <td>0.892078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.887240</td>\n",
       "      <td>0.892033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.846755</td>\n",
       "      <td>0.891501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.530978</td>\n",
       "      <td>0.431911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.539671</td>\n",
       "      <td>0.431334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.534154</td>\n",
       "      <td>0.430891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.534312</td>\n",
       "      <td>0.430891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.533677</td>\n",
       "      <td>0.430891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.534511</td>\n",
       "      <td>0.430846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.536416</td>\n",
       "      <td>0.430846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.539155</td>\n",
       "      <td>0.430802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.534868</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.531217</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.538162</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.539829</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.532447</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.527525</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.538043</td>\n",
       "      <td>0.430758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.532368</td>\n",
       "      <td>0.430713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.539710</td>\n",
       "      <td>0.430403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  no_of_features  hidden_layers  train_score  test_score\n",
       "97      50               8              2     0.913118    0.907470\n",
       "98      50               8              2     0.914031    0.905296\n",
       "143     50              16              2     0.891248    0.904587\n",
       "144     50              16              2     0.881484    0.903123\n",
       "145     50              16              2     0.888272    0.901925\n",
       "165     50              32              2     0.888391    0.898643\n",
       "191     50              32              2     0.889780    0.897312\n",
       "111     50              16              2     0.894781    0.896292\n",
       "96      50               8              2     0.915340    0.895227\n",
       "198     50              32              2     0.851201    0.894384\n",
       "140     50              16              2     0.897361    0.892388\n",
       "142     50              16              2     0.896567    0.892300\n",
       "94      50               8              2     0.901965    0.892300\n",
       "283     50               8              4     0.877753    0.892078\n",
       "116     50              16              2     0.897599    0.892078\n",
       "135     50              16              2     0.887240    0.892033\n",
       "199     50              32              2     0.846755    0.891501\n",
       "..     ...             ...            ...          ...         ...\n",
       "485     50               8              6     0.530978    0.431911\n",
       "488     50               8              6     0.539671    0.431334\n",
       "494     50               8              6     0.534154    0.430891\n",
       "481     50               8              6     0.534312    0.430891\n",
       "489     50               8              6     0.533677    0.430891\n",
       "474     50               8              6     0.534511    0.430846\n",
       "491     50               8              6     0.536416    0.430846\n",
       "497     50               8              6     0.539155    0.430802\n",
       "495     50               8              6     0.534868    0.430758\n",
       "487     50               8              6     0.531217    0.430758\n",
       "476     50               8              6     0.538162    0.430758\n",
       "479     50               8              6     0.539829    0.430758\n",
       "480     50               8              6     0.532447    0.430758\n",
       "483     50               8              6     0.527525    0.430758\n",
       "482     50               8              6     0.538043    0.430758\n",
       "471     50               8              6     0.532368    0.430713\n",
       "493     50               8              6     0.539710    0.430403\n",
       "\n",
       "[600 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by = 'test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:50:18.911119Z",
     "start_time": "2017-05-14T20:50:18.884942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:50:18.987155Z",
     "start_time": "2017-05-14T20:50:18.912977Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j].round(4),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, preprocess.output_columns_2labels, normalize = True,\n",
    "                         title = Train.best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-14T20:50:19.335049Z",
     "start_time": "2017-05-14T20:50:18.989182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.8446  0.1554]\n",
      " [ 0.0916  0.9084]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGeCAYAAAAXNE8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDm5mZpY7/w/arISvMMYExwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac6e3fe438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = Train.actual_value, pred_value = Train.pred_value)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
