{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T21:35:34.006875Z",
     "start_time": "2017-07-17T21:35:33.335646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4416"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import os\n",
    "pd.set_option(\"display.max_rows\",15)\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import preprocessing as pp\n",
    "os.system(\"taskset -p 0xff %d\" % os.getpid())\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T21:35:34.254787Z",
     "start_time": "2017-07-17T21:35:34.008561Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_variables = [\"service\",\"flag\", \"session\"] # 'ids_detection', 'malware_detection', 'ashula_detection',\n",
    "dummy_variables_2labels = category_variables\n",
    "special_variables = ['ids_detection', 'malware_detection', 'ashula_detection']\n",
    "attack_types = {1:'Normal', -1:'Attack', -2:'Attack'}\n",
    "col_names = ['duration', 'service', 'source_bytes', 'destination_bytes',\n",
    "             'count', 'same_srv_rate', 'serror_srv_rate', 'srv_serror_rate',\n",
    "             'dst_host_count', 'dst_host_srv_count', 'dst_host_same_src_port_rate',\n",
    "             'dst_host_serror_rate', 'dst_host_serror_srv_rate', 'flag',\n",
    "             'ids_detection', 'malware_detection', 'ashula_detection', 'label',\n",
    "             'source_ip_address', 'source_port_number', 'destination_ip_address',\n",
    "             'destination_port_number', 'start_time', 'session'\n",
    "            ]\n",
    "\n",
    "\n",
    "class dataset:\n",
    "    def read_data(train_file, test_file):\n",
    "\n",
    "        kdd_train = pd.read_csv(\"{}.txt\".format(train_file),names = col_names, sep='\\t')\n",
    "        kdd_train_label = kdd_train.label.copy()\n",
    "        kdd_train = kdd_train.drop(['start_time', 'source_port_number', 'source_ip_address', 'destination_port_number', 'destination_ip_address', 'label'], axis = 1)\n",
    "        \n",
    "        kdd_train_label = kdd_train_label.map(lambda x: attack_types[x])\n",
    "        \n",
    "        kdd_train.loc[:,special_variables] = kdd_train.loc[:,special_variables].copy().applymap(lambda x: \n",
    "                                                                                                0 if x == '0' \n",
    "                                                                                                or x == 0 else 1)\n",
    "        \n",
    "        kdd_train['session'] = kdd_train['session'].astype(\"category\", categories = ['icmp', 'tcp', 'udp'])\n",
    "        kdd_train['flag'] = kdd_train['flag'].astype(\"category\", categories = ['OTH', 'REJ', 'RSTO', 'RSTOS0', 'RSTR', 'RSTRH', \n",
    "                                                                               'S0', 'S1', 'S2', 'S3', 'SF', 'SH', 'SHR'])\n",
    "        kdd_train['service'] = kdd_train['service'].astype(\"category\", categories = ['dhcp', 'dns', 'http', 'other', 'rdp', \n",
    "                                                                                     'sip', 'smtp', 'smtp,ssl', 'snmp', 'ssh', 'ssl'])\n",
    "        \n",
    "\n",
    "        kdd_train_label = kdd_train_label.astype(\"category\", categories = ['Normal', 'Attack'])\n",
    "        \n",
    "        dataset.kdd_train = kdd_train\n",
    "        dataset.kdd_train_label = kdd_train_label\n",
    "        \n",
    "   \n",
    "\n",
    "class pre_processing:\n",
    "    kdd_train = []\n",
    "    def process(train_file, test_file):\n",
    "        pre_processing.kdd_train = pd.get_dummies(dataset.kdd_train, columns = dummy_variables_2labels, prefix=dummy_variables_2labels)\n",
    "        #preprocessing.kdd_test = pd.get_dummies(dataset.kdd_test, columns = dummy_variables_2labels, prefix=dummy_variables_2labels)\n",
    "\n",
    "        pre_processing.kdd_train_label = pd.get_dummies(dataset.kdd_train_label, prefix='is')\n",
    "        #preprocessing.kdd_test_label = pd.get_dummies(dataset.kdd_test_label, prefix='is')\n",
    "            \n",
    "        output_columns_2labels = ['label']\n",
    "\n",
    "        x_input = pre_processing.kdd_train\n",
    "        y_output = pre_processing.kdd_train_label\n",
    "\n",
    "        ss = pp.StandardScaler()\n",
    "\n",
    "        x_train = ss.fit_transform(x_input)\n",
    "        y_train = y_output.values\n",
    "         \n",
    "            \n",
    "        x_train = pd.DataFrame(x_train)\n",
    "        x_train.columns = pre_processing.kdd_train.columns\n",
    "        \n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        y_train.columns = pre_processing.kdd_train_label.columns\n",
    "        \n",
    "        x_train.to_csv(\"{}_x.csv\".format(train_file), index = False)\n",
    "        #preprocessing.kdd_test.to_csv(\"{}.csv\".format(test_file))\n",
    "        \n",
    "        y_train.to_csv(\"{}_y.csv\".format(train_file), index = False)\n",
    "        #preprocessing.kdd_test.to_csv(\"{}_label.csv\".format(test_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T21:35:34.269890Z",
     "start_time": "2017-07-17T21:35:34.256602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame()\n",
    "a.aggregate(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/Kyoto2016/2014/01/20140122', 'dataset/Kyoto2016/2014/01/20140110', 'dataset/Kyoto2016/2014/01/20140120', 'dataset/Kyoto2016/2014/01/20140109', 'dataset/Kyoto2016/2014/01/20140115', 'dataset/Kyoto2016/2014/01/20140127', 'dataset/Kyoto2016/2014/01/20140104', 'dataset/Kyoto2016/2014/01/20140124', 'dataset/Kyoto2016/2014/01/20140103', 'dataset/Kyoto2016/2014/01/20140113', 'dataset/Kyoto2016/2014/01/20140114', 'dataset/Kyoto2016/2014/01/20140108', 'dataset/Kyoto2016/2014/01/20140129', 'dataset/Kyoto2016/2014/01/20140106', 'dataset/Kyoto2016/2014/01/20140119', 'dataset/Kyoto2016/2014/01/20140118', 'dataset/Kyoto2016/2014/01/20140101', 'dataset/Kyoto2016/2014/01/20140102', 'dataset/Kyoto2016/2014/01/20140125', 'dataset/Kyoto2016/2014/01/20140116', 'dataset/Kyoto2016/2014/01/20140131', 'dataset/Kyoto2016/2014/01/20140111', 'dataset/Kyoto2016/2014/01/20140126', 'dataset/Kyoto2016/2014/01/20140130', 'dataset/Kyoto2016/2014/01/20140121', 'dataset/Kyoto2016/2014/01/20140112', 'dataset/Kyoto2016/2014/01/20140105', 'dataset/Kyoto2016/2014/01/20140117', 'dataset/Kyoto2016/2014/01/20140107', 'dataset/Kyoto2016/2014/01/20140128', 'dataset/Kyoto2016/2014/01/20140123']\n",
      "--------------------------------------------------------\n",
      "['dataset/Kyoto2016/2015/12/20151202', 'dataset/Kyoto2016/2015/12/20151224', 'dataset/Kyoto2016/2015/12/20151208', 'dataset/Kyoto2016/2015/12/20151209', 'dataset/Kyoto2016/2015/12/20151223', 'dataset/Kyoto2016/2015/12/20151214', 'dataset/Kyoto2016/2015/12/20151228', 'dataset/Kyoto2016/2015/12/20151212', 'dataset/Kyoto2016/2015/12/20151226', 'dataset/Kyoto2016/2015/12/20151210', 'dataset/Kyoto2016/2015/12/20151213', 'dataset/Kyoto2016/2015/12/20151219', 'dataset/Kyoto2016/2015/12/20151218', 'dataset/Kyoto2016/2015/12/20151211', 'dataset/Kyoto2016/2015/12/20151230', 'dataset/Kyoto2016/2015/12/20151220', 'dataset/Kyoto2016/2015/12/20151216', 'dataset/Kyoto2016/2015/12/20151215', 'dataset/Kyoto2016/2015/12/20151229', 'dataset/Kyoto2016/2015/12/20151221', 'dataset/Kyoto2016/2015/12/20151227', 'dataset/Kyoto2016/2015/12/20151204', 'dataset/Kyoto2016/2015/12/20151225', 'dataset/Kyoto2016/2015/12/20151201', 'dataset/Kyoto2016/2015/12/20151203', 'dataset/Kyoto2016/2015/12/20151205', 'dataset/Kyoto2016/2015/12/20151222', 'dataset/Kyoto2016/2015/12/20151217', 'dataset/Kyoto2016/2015/12/20151207', 'dataset/Kyoto2016/2015/12/20151206', 'dataset/Kyoto2016/2015/12/20151231']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_paths = []\n",
    "for path, subdirs, files in os.walk(\"dataset/Kyoto2016/2014/01/\"):\n",
    "    for name in files:\n",
    "        if name.endswith(\"txt\"):\n",
    "            fullname = os.path.join(path, name)\n",
    "            train_paths.append(fullname.replace(\".txt\",\"\"))\n",
    "            #print (fullname.replace(\".txt\",\"\"))\n",
    "\n",
    "test_paths = []\n",
    "for path, subdirs, files in os.walk(\"dataset/Kyoto2016/2015/12\"):\n",
    "    for name in files:\n",
    "        if name.endswith(\"txt\"):\n",
    "            fullname = os.path.join(path, name)\n",
    "            test_paths.append(fullname.replace(\".txt\",\"\"))\n",
    "            #print (fullname.replace(\".txt\",\"\"))\n",
    "print(train_paths)\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.170Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Kyoto2016/2014/01/20140122\n",
      "Skipping dataset/Kyoto2016/2014/01/20140122\n",
      "dataset/Kyoto2016/2014/01/20140110\n",
      "Skipping dataset/Kyoto2016/2014/01/20140110\n",
      "dataset/Kyoto2016/2014/01/20140120\n",
      "Skipping dataset/Kyoto2016/2014/01/20140120\n",
      "dataset/Kyoto2016/2014/01/20140109\n",
      "Skipping dataset/Kyoto2016/2014/01/20140109\n",
      "dataset/Kyoto2016/2014/01/20140115\n",
      "Skipping dataset/Kyoto2016/2014/01/20140115\n",
      "dataset/Kyoto2016/2014/01/20140127\n",
      "Skipping dataset/Kyoto2016/2014/01/20140127\n",
      "dataset/Kyoto2016/2014/01/20140104\n",
      "Skipping dataset/Kyoto2016/2014/01/20140104\n",
      "dataset/Kyoto2016/2014/01/20140124\n",
      "Skipping dataset/Kyoto2016/2014/01/20140124\n",
      "dataset/Kyoto2016/2014/01/20140103\n",
      "Skipping dataset/Kyoto2016/2014/01/20140103\n",
      "dataset/Kyoto2016/2014/01/20140113\n",
      "Skipping dataset/Kyoto2016/2014/01/20140113\n",
      "dataset/Kyoto2016/2014/01/20140114\n",
      "Skipping dataset/Kyoto2016/2014/01/20140114\n",
      "dataset/Kyoto2016/2014/01/20140108\n",
      "Skipping dataset/Kyoto2016/2014/01/20140108\n",
      "dataset/Kyoto2016/2014/01/20140129\n",
      "Skipping dataset/Kyoto2016/2014/01/20140129\n",
      "dataset/Kyoto2016/2014/01/20140106\n",
      "Skipping dataset/Kyoto2016/2014/01/20140106\n",
      "dataset/Kyoto2016/2014/01/20140119\n",
      "Skipping dataset/Kyoto2016/2014/01/20140119\n",
      "dataset/Kyoto2016/2014/01/20140118\n",
      "Skipping dataset/Kyoto2016/2014/01/20140118\n",
      "dataset/Kyoto2016/2014/01/20140101\n",
      "Skipping dataset/Kyoto2016/2014/01/20140101\n",
      "dataset/Kyoto2016/2014/01/20140102\n",
      "Skipping dataset/Kyoto2016/2014/01/20140102\n",
      "dataset/Kyoto2016/2014/01/20140125\n",
      "Skipping dataset/Kyoto2016/2014/01/20140125\n",
      "dataset/Kyoto2016/2014/01/20140116\n",
      "Skipping dataset/Kyoto2016/2014/01/20140116\n",
      "dataset/Kyoto2016/2014/01/20140131\n",
      "Skipping dataset/Kyoto2016/2014/01/20140131\n",
      "dataset/Kyoto2016/2014/01/20140111\n",
      "Skipping dataset/Kyoto2016/2014/01/20140111\n",
      "dataset/Kyoto2016/2014/01/20140126\n",
      "Skipping dataset/Kyoto2016/2014/01/20140126\n",
      "dataset/Kyoto2016/2014/01/20140130\n",
      "Skipping dataset/Kyoto2016/2014/01/20140130\n",
      "dataset/Kyoto2016/2014/01/20140121\n",
      "Skipping dataset/Kyoto2016/2014/01/20140121\n",
      "dataset/Kyoto2016/2014/01/20140112\n",
      "Skipping dataset/Kyoto2016/2014/01/20140112\n",
      "dataset/Kyoto2016/2014/01/20140105\n",
      "Skipping dataset/Kyoto2016/2014/01/20140105\n",
      "dataset/Kyoto2016/2014/01/20140117\n",
      "Skipping dataset/Kyoto2016/2014/01/20140117\n",
      "dataset/Kyoto2016/2014/01/20140107\n",
      "Skipping dataset/Kyoto2016/2014/01/20140107\n",
      "dataset/Kyoto2016/2014/01/20140128\n",
      "Skipping dataset/Kyoto2016/2014/01/20140128\n",
      "dataset/Kyoto2016/2014/01/20140123\n",
      "Skipping dataset/Kyoto2016/2014/01/20140123\n",
      "dataset/Kyoto2016/2015/12/20151202\n",
      "dataset/Kyoto2016/2015/12/20151224\n",
      "dataset/Kyoto2016/2015/12/20151208\n",
      "dataset/Kyoto2016/2015/12/20151209\n",
      "dataset/Kyoto2016/2015/12/20151223\n",
      "dataset/Kyoto2016/2015/12/20151214\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def run(paths):\n",
    "    for path in paths:\n",
    "        #print(train_path)\n",
    "        #print(test_path)\n",
    "        print(path)\n",
    "        if not (os.path.exists(\"{}_x.csv\".format(path)) and os.path.exists(\"{}_y.csv\".format(path))):\n",
    "            dataset.read_data(path, path)\n",
    "            pre_processing.process(path, path)\n",
    "        else:\n",
    "            print(\"Skipping {}\".format(path))\n",
    "            \n",
    "with Pool() as p:\n",
    "    p.map(run, [train_paths])\n",
    "    p.map(run, [test_paths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.173Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.176Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.kdd_train_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.179Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.182Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.kdd_train.info(memory_usage = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.185Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.kdd_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.188Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    #print(\"Length of Categories for {} are {}\".format(cv , len(dataset.kdd_train[cv].cat.categories)))\n",
    "    #print(\"Categories for {} are {} \\n\".format(cv ,dataset.kdd_train[cv].cat.categories))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T14:55:01.066892Z",
     "start_time": "2017-06-30T14:55:00.640246Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.192Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing.kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.195Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing.kdd_train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.198Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = dataset.kdd_train.isin([0])\n",
    "a.sum().sum() / a.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.201Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from pandas.plotting import andrews_curves\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn import preprocessing as ps\n",
    "from pandas.plotting import radviz\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "df_train = preprocessing.kdd_train\n",
    "df_train = pd.concat([df_train, dataset.kdd_train_label], axis = 1)\n",
    "\n",
    "df_test = preprocessing.kdd_test\n",
    "df_test = pd.concat([df_test, dataset.kdd_test_label], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.205Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "#np.set_printoptions(suppress=True)\n",
    "sample = df_train.sample(frac = 0.01) # 10% of total data\n",
    "sample.to_pickle(\"dataset/tsne_sample.pkl\")\n",
    "sample = pd.read_pickle(\"dataset/tsne_sample.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.208Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.211Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tsne = sample.iloc[:, :-1]\n",
    "y_tsne = sample.iloc[:, -1]\n",
    "\n",
    "\n",
    "from sklearn.decomposition import SparsePCA\n",
    "pca_analysis = SparsePCA(n_components=10)\n",
    "x_tsne_pca = pca_analysis.fit_transform(x_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.214Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(x_tsne_pca).to_pickle(\"dataset/tsne_pca_df.pkl\")\n",
    "x_tsne_pca = pd.read_pickle(\"dataset/tsne_pca_df.pkl\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.218Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,3))\n",
    "x_tsne_pca_df = pd.DataFrame(x_tsne_pca)\n",
    "\n",
    "x_tsne_pca_df['label'] = y_tsne.values\n",
    "\n",
    "andrews_curves(x_tsne_pca_df, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = model.fit_transform(x_tsne_pca) \n",
    "df1 = model.fit_transform(df)\n",
    "df2 = model.fit_transform(df1) \n",
    "df3 = model.fit_transform(df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.231Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df).to_pickle(\"dataset/tsne_df.pkl\")\n",
    "pd.DataFrame(df1).to_pickle(\"dataset/tsne_df1.pkl\")\n",
    "pd.DataFrame(df2).to_pickle(\"dataset/tsne_df2.pkl\")\n",
    "pd.DataFrame(df3).to_pickle(\"dataset/tsne_df3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.235Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"dataset/tsne_df.pkl\").values\n",
    "df1 = pd.read_pickle(\"dataset/tsne_df1.pkl\").values\n",
    "df2 = pd.read_pickle(\"dataset/tsne_df2.pkl\").values\n",
    "df3 = pd.read_pickle(\"dataset/tsne_df3.pkl\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.238Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#plt.figure(figsize=(15,8))\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10,5))\n",
    "\n",
    "ax1.scatter(x = df[y_tsne=='Normal', 0], y = df[y_tsne=='Normal',1], label = 'Normal')\n",
    "ax1.scatter(x = df[y_tsne=='Attack',0], y = df[y_tsne=='Attack',1], label = 'Attack')\n",
    "ax1.title.set_text(\"After 1000 epochs\")\n",
    "\n",
    "ax2.scatter(x = df1[y_tsne=='Normal',0], y = df1[y_tsne=='Normal',1], label = 'Normal')\n",
    "ax2.scatter(x = df1[y_tsne=='Attack',0], y = df1[y_tsne=='Attack',1], label = 'Attack')\n",
    "ax2.title.set_text(\"After 2000 epochs\")\n",
    "\n",
    "ax3.scatter(x = df2[y_tsne=='Normal',0], y = df2[y_tsne=='Normal',1], label = 'Normal')\n",
    "ax3.scatter(x = df2[y_tsne=='Attack',0], y = df2[y_tsne=='Attack',1], label = 'Attack')\n",
    "ax3.title.set_text(\"After 3000 epochs\")\n",
    "\n",
    "ax4.scatter(x = df3[y_tsne=='Normal',0], y = df3[y_tsne=='Normal',1], label = 'Normal')\n",
    "ax4.scatter(x = df3[y_tsne=='Attack',0], y = df3[y_tsne=='Attack',1], label = 'Attack')\n",
    "ax4.title.set_text(\"After 4000 epochs\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.18)\n",
    "ax1.legend(loc=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing.kdd_train.to_pickle(\"dataset/kdd_train.pkl\")\n",
    "preprocessing.kdd_train_label.to_pickle(\"dataset/kdd_train_label.pkl\")\n",
    "\n",
    "preprocessing.kdd_test.to_pickle(\"dataset/kdd_test.pkl\")\n",
    "preprocessing.kdd_test_label.to_pickle(\"dataset/kdd_test_label.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.244Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing.kdd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-07-17T21:35:35.247Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing.kdd_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/a442376f3fb1d15375cdf44746cfb2fb"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "With Difficulty level",
    "public": false
   },
   "id": "a442376f3fb1d15375cdf44746cfb2fb"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
