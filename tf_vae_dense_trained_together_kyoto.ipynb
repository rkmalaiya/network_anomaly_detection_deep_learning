{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:40:02.271029Z",
     "start_time": "2017-07-21T21:39:59.068757Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "pd.set_option(\"display.max_rows\",100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:40:02.306487Z",
     "start_time": "2017-07-21T21:40:02.273050Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:40:02.400849Z",
     "start_time": "2017-07-21T21:40:02.308554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'20140115': {'x': 'dataset/Kyoto2016/2014/01/20140115_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140115_y.csv'}, '20140107': {'y': 'dataset/Kyoto2016/2014/01/20140107_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140107_x.csv'}, '20140109': {'x': 'dataset/Kyoto2016/2014/01/20140109_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140109_y.csv'}, '20140104': {'y': 'dataset/Kyoto2016/2014/01/20140104_y.csv', 'x': 'dataset/Kyoto2016/2014/01/20140104_x.csv'}, '20140119': {'x': 'dataset/Kyoto2016/2014/01/20140119_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140119_y.csv'}, '20140120': {'x': 'dataset/Kyoto2016/2014/01/20140120_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140120_y.csv'}, '20140123': {'x': 'dataset/Kyoto2016/2014/01/20140123_x.csv', 'y': 'dataset/Kyoto2016/2014/01/20140123_y.csv'}}\n",
      "----------------------------------------------------------------------------------------\n",
      "{'20151224': {'y': 'dataset/Kyoto2016/2015/12/20151224_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151224_x.csv'}, '20151204': {'y': 'dataset/Kyoto2016/2015/12/20151204_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151204_x.csv'}, '20151216': {'x': 'dataset/Kyoto2016/2015/12/20151216_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151216_y.csv'}, '20151222': {'x': 'dataset/Kyoto2016/2015/12/20151222_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151222_y.csv'}, '20151214': {'x': 'dataset/Kyoto2016/2015/12/20151214_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151214_y.csv'}, '20151202': {'x': 'dataset/Kyoto2016/2015/12/20151202_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151202_y.csv'}, '20151227': {'y': 'dataset/Kyoto2016/2015/12/20151227_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151227_x.csv'}, '20151203': {'y': 'dataset/Kyoto2016/2015/12/20151203_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151203_x.csv'}, '20151223': {'y': 'dataset/Kyoto2016/2015/12/20151223_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151223_x.csv'}, '20151205': {'x': 'dataset/Kyoto2016/2015/12/20151205_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151205_y.csv'}, '20151229': {'x': 'dataset/Kyoto2016/2015/12/20151229_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151229_y.csv'}, '20151208': {'x': 'dataset/Kyoto2016/2015/12/20151208_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151208_y.csv'}, '20151219': {'y': 'dataset/Kyoto2016/2015/12/20151219_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151219_x.csv'}, '20151206': {'y': 'dataset/Kyoto2016/2015/12/20151206_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151206_x.csv'}, '20151225': {'x': 'dataset/Kyoto2016/2015/12/20151225_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151225_y.csv'}, '20151210': {'x': 'dataset/Kyoto2016/2015/12/20151210_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151210_y.csv'}, '20151217': {'x': 'dataset/Kyoto2016/2015/12/20151217_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151217_y.csv'}, '20151207': {'x': 'dataset/Kyoto2016/2015/12/20151207_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151207_y.csv'}, '20151215': {'x': 'dataset/Kyoto2016/2015/12/20151215_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151215_y.csv'}, '20151213': {'y': 'dataset/Kyoto2016/2015/12/20151213_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151213_x.csv'}, '20151209': {'x': 'dataset/Kyoto2016/2015/12/20151209_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151209_y.csv'}, '20151228': {'y': 'dataset/Kyoto2016/2015/12/20151228_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151228_x.csv'}, '20151226': {'y': 'dataset/Kyoto2016/2015/12/20151226_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151226_x.csv'}, '20151218': {'x': 'dataset/Kyoto2016/2015/12/20151218_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151218_y.csv'}, '20151231': {'x': 'dataset/Kyoto2016/2015/12/20151231_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151231_y.csv'}, '20151212': {'x': 'dataset/Kyoto2016/2015/12/20151212_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151212_y.csv'}, '20151211': {'x': 'dataset/Kyoto2016/2015/12/20151211_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151211_y.csv'}, '20151221': {'y': 'dataset/Kyoto2016/2015/12/20151221_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151221_x.csv'}, '20151201': {'x': 'dataset/Kyoto2016/2015/12/20151201_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151201_y.csv'}, '20151220': {'x': 'dataset/Kyoto2016/2015/12/20151220_x.csv', 'y': 'dataset/Kyoto2016/2015/12/20151220_y.csv'}, '20151230': {'y': 'dataset/Kyoto2016/2015/12/20151230_y.csv', 'x': 'dataset/Kyoto2016/2015/12/20151230_x.csv'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class preprocess:\n",
    "    \n",
    "    paths = {}\n",
    "\n",
    "    def get_files(folder_path):\n",
    "        paths = {}\n",
    "        for path, subdirs, files in os.walk(folder_path):\n",
    "            for name in files:\n",
    "                if name.endswith(\"csv\"):\n",
    "                    key = name.split(\"_\")[0]\n",
    "\n",
    "                    if paths.get(key) is None:\n",
    "                        paths[key] = {}\n",
    "\n",
    "                    if name.endswith(\"_x.csv\"):\n",
    "                        x = os.path.join(path, name)\n",
    "                        paths[key]['x'] = x\n",
    "                    elif name.endswith(\"_y.csv\"):\n",
    "                        y = os.path.join(path, name)\n",
    "                        paths[key]['y'] = y\n",
    "        preprocess.paths = paths\n",
    "        return paths\n",
    "\n",
    "    def get_data(paths):\n",
    "        for key, value in paths.items():\n",
    "            x = pd.read_csv(value['x'])\n",
    "            y = pd.read_csv(value['y'])\n",
    "            #print(x.shape)\n",
    "            #print(x.values.shape)\n",
    "            #print(y.sum())\n",
    "            yield key, x.values, y.values\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "train_paths = preprocess.get_files(\"dataset/Kyoto2016/2014/01\")\n",
    "test_paths = preprocess.get_files(\"dataset/Kyoto2016/2015/12\")\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/11\"))\n",
    "#test_paths.update(preprocess.get_files(\"dataset/Kyoto2016/2015/10\"))\n",
    "\n",
    "paths = {}\n",
    "keys = train_paths.keys()\n",
    "for key in list(keys)[0:7]:\n",
    "    paths.update({key: train_paths[key]})\n",
    "train_paths = paths\n",
    "print(train_paths)\n",
    "print(\"----------------------------------------------------------------------------------------\")\n",
    "#test_paths = test_paths.popitem()\n",
    "#test_paths = {test_paths[0]: test_paths[1]}\n",
    "print(test_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:40:07.961188Z",
     "start_time": "2017-07-21T21:40:02.402424Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:40:08.516232Z",
     "start_time": "2017-07-21T21:40:07.963277Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network(object):\n",
    "    \n",
    "    input_dim = 42\n",
    "    classes = 2\n",
    "    hidden_encoder_dim = 42\n",
    "    hidden_layers = 1\n",
    "    latent_dim = 10\n",
    "\n",
    "    hidden_decoder_dim = 42\n",
    "    lam = 0.001\n",
    "    \n",
    "    def __init__(self, classes, hidden_layers, num_of_features):\n",
    "        self.classes = classes\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = num_of_features\n",
    "            \n",
    "    def build_layers(self):\n",
    "        tf.reset_default_graph()\n",
    "        #learning_rate = tf.Variable(initial_value=0.001)\n",
    "\n",
    "        input_dim = self.input_dim\n",
    "        classes = self.classes\n",
    "        hidden_encoder_dim = self.hidden_encoder_dim\n",
    "        hidden_layers = self.hidden_layers\n",
    "        latent_dim = self.latent_dim\n",
    "        hidden_decoder_dim = self.hidden_decoder_dim\n",
    "        lam = self.lam\n",
    "        \n",
    "        with tf.variable_scope(\"Input\"):\n",
    "            self.x = tf.placeholder(\"float\", shape=[None, input_dim])\n",
    "            self.y_ = tf.placeholder(\"float\", shape=[None, classes])\n",
    "            self.keep_prob = tf.placeholder(\"float\")\n",
    "            self.lr = tf.placeholder(\"float\")\n",
    "        \n",
    "        with tf.variable_scope(\"Layer_Encoder\"):\n",
    "\n",
    "            hidden_encoder = tf.layers.dense(self.x, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_encoder = tf.layers.dense(hidden_encoder, hidden_encoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_encoder = tf.nn.dropout(hidden_encoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Mean\"):\n",
    "            mu_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Variance\"):\n",
    "            logvar_encoder = tf.layers.dense(hidden_encoder, latent_dim, activation = None, kernel_regularizer=tf.nn.l2_loss)\n",
    "\n",
    "        with tf.variable_scope(\"Sampling_Distribution\"):\n",
    "            # Sample epsilon\n",
    "            epsilon = tf.random_normal(tf.shape(logvar_encoder), mean=0, stddev=1, name='epsilon')\n",
    "\n",
    "            # Sample latent variable\n",
    "            std_encoder = tf.exp(0.5 * logvar_encoder)\n",
    "            z = mu_encoder + tf.multiply(std_encoder, epsilon)\n",
    "            \n",
    "            #tf.summary.histogram(\"Sample_Distribution\", z)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Decoder\"):\n",
    "            hidden_decoder = tf.layers.dense(z, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "            hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "            for h in range(hidden_layers - 1):\n",
    "                hidden_decoder = tf.layers.dense(hidden_decoder, hidden_decoder_dim, activation = tf.nn.relu, kernel_regularizer=tf.nn.l2_loss)\n",
    "                hidden_decoder = tf.nn.dropout(hidden_decoder, self.keep_prob)\n",
    "                \n",
    "        with tf.variable_scope(\"Layer_Reconstruction\"):\n",
    "            x_hat = tf.layers.dense(hidden_decoder, input_dim, activation = None)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer_Dense_Hidden\"):\n",
    "            hidden_output = tf.layers.dense(z,latent_dim, activation=tf.nn.relu)\n",
    "\n",
    "        with tf.variable_scope(\"Layer_Dense_Softmax\"):\n",
    "            self.y = tf.layers.dense(z, classes, activation=tf.nn.softmax)\n",
    "\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            \n",
    "            BCE = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=x_hat, labels=self.x), reduction_indices=1)\n",
    "            KLD = -0.5 * tf.reduce_mean(1 + logvar_encoder - tf.pow(mu_encoder, 2) - tf.exp(logvar_encoder), reduction_indices=1)\n",
    "            softmax_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y_, logits = self.y))\n",
    "\n",
    "            loss = tf.reduce_mean((BCE + KLD + softmax_loss) * lam)\n",
    "\n",
    "            loss = tf.clip_by_value(loss, -1e-2, 1e-2)\n",
    "            loss = tf.where(tf.is_nan(loss), 1e-2, loss)\n",
    "            loss = tf.where(tf.equal(loss, -1e-2), tf.random_normal(loss.shape), loss)\n",
    "            loss = tf.where(tf.equal(loss, 1e-2), tf.random_normal(loss.shape), loss)\n",
    "            \n",
    "            self.regularized_loss = tf.abs(loss, name = \"Regularized_loss\")\n",
    "            correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.y, 1))\n",
    "            self.tf_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = \"Accuracy\")\n",
    "\n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            learning_rate=self.lr #1e-2\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(self.regularized_loss))\n",
    "            gradients = [\n",
    "                None if gradient is None else tf.clip_by_value(gradient, -1, 1)\n",
    "                for gradient in gradients]\n",
    "            self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "            #self.train_op = optimizer.minimize(self.regularized_loss)\n",
    "            \n",
    "        # add op for merging summary\n",
    "        #self.summary_op = tf.summary.merge_all()\n",
    "        self.pred = tf.argmax(self.y, axis = 1)\n",
    "        self.actual = tf.argmax(self.y_, axis = 1)\n",
    "\n",
    "        # add Saver ops\n",
    "        self.saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:40:08.768711Z",
     "start_time": "2017-07-21T21:40:08.517988Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "\n",
    "class Train:    \n",
    "    \n",
    "    result = namedtuple(\"score\", ['key', 'no_of_features','hidden_layers','train_score', 'test_score', 'quality_score', 'time_taken'])\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_acc_global = 0\n",
    "\n",
    "    def train(epochs, net, h,f, lrs):\n",
    "        batch_iterations = 1000\n",
    "        train_loss = None\n",
    "        Train.best_acc = 0\n",
    "        os.makedirs(\"dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}\".format(h,f),\n",
    "                    exist_ok = True)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            #summary_writer_train = tf.summary.FileWriter('./logs/kdd/VAE/training', graph=sess.graph)\n",
    "            #summary_writer_valid = tf.summary.FileWriter('./logs/kdd/VAE/validation')\n",
    "\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            Train.best_acc = 0\n",
    "            for lr in lrs:\n",
    "                for epoch in range(1, (epochs+1)):\n",
    "                    \n",
    "                    for key, x_train, y_train in preprocess.get_data(train_paths):\n",
    "                        x_train, x_valid, y_train, y_valid, = ms.train_test_split(x_train, \n",
    "                                                                                  y_train, \n",
    "                                                                                  test_size=0.1)\n",
    "                    \n",
    "                        batch_indices = np.array_split(np.arange(x_train.shape[0]), \n",
    "                                                   batch_iterations)\n",
    "\n",
    "                        for i in batch_indices:\n",
    "\n",
    "                            def train_batch():\n",
    "                                nonlocal train_loss\n",
    "                                _, train_loss = sess.run([net.train_op, \n",
    "                                                                       net.regularized_loss, \n",
    "                                                                       ], #net.summary_op\n",
    "                                                                      feed_dict={net.x: x_train[i,:], \n",
    "                                                                                 net.y_: y_train[i,:], \n",
    "                                                                                 net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                            train_batch()\n",
    "                            count = 10\n",
    "\n",
    "                            while((train_loss > 1e9 or np.isnan(train_loss)) and epoch > 1 and count > 1):\n",
    "                                print(\"Step {} | High Training Loss: {:.6f} ... Restoring Net\".format(epoch, train_loss))\n",
    "                                net.saver.restore(sess, \n",
    "                                                  tf.train.latest_checkpoint('dataset/tf_vae_dense_trained_together_nsl_kdd/hidden layers_{}_features count_{}'\n",
    "                                                                             .format(h,f)))\n",
    "                                train_batch()\n",
    "                                count -= 1\n",
    "\n",
    "                        valid_loss, valid_accuracy = sess.run([net.regularized_loss, net.tf_accuracy], #net.summary_op\n",
    "                                                                  feed_dict={net.x: x_valid, \n",
    "                                                                             net.y_: y_valid, \n",
    "                                                                             net.keep_prob:1, net.lr:lr})\n",
    "                        print(\"Train Loss:{}, valid loss: {}\".format(train_loss, valid_loss))\n",
    "                        \n",
    "                    end_time = time.perf_counter()    \n",
    "                    for key, x_test, y_test in preprocess.get_data(test_paths):\n",
    "                        test_accuracy, test_loss, pred_value, actual_value, y_pred = sess.run([net.tf_accuracy, net.regularized_loss, net.pred, \n",
    "                                                                                          net.actual, net.y], #net.summary_op \n",
    "                                                                                          feed_dict={net.x: x_test, \n",
    "                                                                                         net.y_: y_test, \n",
    "                                                                                         net.keep_prob:1, net.lr:lr})\n",
    "\n",
    "                        \n",
    "                        quality_score = me.matthews_corrcoef(actual_value, pred_value)\n",
    "                        recall = me.recall_score(actual_value, pred_value)\n",
    "                        prec = me.precision_score(actual_value, pred_value)\n",
    "                        test_accuracy = me.roc_auc_score(actual_value, pred_value)\n",
    "                        \n",
    "                        print(\"Key {} | Test Accuracy: {:.6f}, quality_score: {}, recall: {}, prec: {}\".format(key, test_accuracy, quality_score, recall, prec))\n",
    "\n",
    "                        if test_accuracy > Train.best_acc_global:\n",
    "                            Train.best_acc_global = test_accuracy\n",
    "                            Train.pred_value = pred_value\n",
    "                            Train.actual_value = actual_value\n",
    "\n",
    "                            Train.best_parameters = \"Hidden Layers:{}, Features Count:{}\".format(h, f)\n",
    "\n",
    "                        curr_pred = pd.DataFrame({\"Attack_prob\":y_pred[:,-2], \"Normal_prob\":y_pred[:, -1], \"Prediction\":pred_value, \"Actual\":actual_value})\n",
    "                        Train.predictions.update({\"{}_{}_{}\".format(key,f,h):\n",
    "                                                  (curr_pred, \n",
    "                                                   Train.result(key, f, h,valid_accuracy, test_accuracy, quality_score, end_time - start_time))})\n",
    "                        #Train.results.append(Train.result(epochs, f, h,valid_accuracy, test_accuracy))\n",
    "                print(\"Best Accuracy on Test data: {}\".format(Train.best_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T21:40:08.836014Z",
     "start_time": "2017-07-21T21:40:08.770547Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "df_results = []\n",
    "past_scores = []\n",
    "\n",
    "class Hyperparameters:\n",
    "#    features_arr = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "#    hidden_layers_arr = [2, 4, 6, 10]\n",
    "\n",
    "    def start_training():\n",
    "        global df_results\n",
    "        global past_scores\n",
    "        \n",
    "        features_arr = [1, 4, 8, 16, 42]\n",
    "        hidden_layers_arr = [1, 3]\n",
    "\n",
    "        Train.predictions = {}\n",
    "        Train.results = []\n",
    "\n",
    "        epochs = [1]\n",
    "        lrs = [1e-4]\n",
    "\n",
    "        for e, h, f in itertools.product(epochs, hidden_layers_arr, features_arr):\n",
    "            print(\"Current Layer Attributes - epochs:{} hidden layers:{} features count:{}\".format(e,h,f))\n",
    "            n = network(2,h,f)\n",
    "            n.build_layers()\n",
    "            Train.train(e, n, h,f, lrs)\n",
    "\n",
    "        dict1 = {}\n",
    "        dict2 = []\n",
    "        for k, (v1, v2) in Train.predictions.items():\n",
    "            dict1.update({k: v1})\n",
    "            dict2.append(v2)\n",
    "            \n",
    "        Train.predictions = dict1\n",
    "        Train.results = dict2\n",
    "        df_results = pd.DataFrame(Train.results)\n",
    "        #temp = df_results.set_index(['no_of_features', 'hidden_layers'])\n",
    "\n",
    "        if not os.path.isfile('dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl'):\n",
    "            past_scores = df_results\n",
    "        else:\n",
    "            past_scores = pd.read_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl\")\n",
    "\n",
    "        past_scores.append(df_results).to_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:01:21.996849Z",
     "start_time": "2017-07-21T21:40:08.837700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:1\n",
      "Train Loss:0.5553274750709534, valid loss: 0.7469290494918823\n",
      "Train Loss:0.3605841398239136, valid loss: 0.5322356820106506\n",
      "Train Loss:1.1689424514770508, valid loss: 0.06889316439628601\n",
      "Train Loss:1.397558331489563, valid loss: 0.10114549845457077\n",
      "Train Loss:0.46864446997642517, valid loss: 0.5548378229141235\n",
      "Train Loss:0.3129993677139282, valid loss: 1.4913982152938843\n",
      "Train Loss:0.07916267961263657, valid loss: 0.7590144276618958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:516: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(var_yt * var_yp)\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151231 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:4\n",
      "Train Loss:0.7179460525512695, valid loss: 0.8342313170433044\n",
      "Train Loss:0.8894203305244446, valid loss: 0.11675956845283508\n",
      "Train Loss:0.35545992851257324, valid loss: 0.6929122805595398\n",
      "Train Loss:0.1202688217163086, valid loss: 0.02917063608765602\n",
      "Train Loss:0.409315288066864, valid loss: 1.0511770248413086\n",
      "Train Loss:0.8168465495109558, valid loss: 1.4766793251037598\n",
      "Train Loss:1.5182558298110962, valid loss: 0.13637816905975342\n",
      "Key 20151224 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151231 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:8\n",
      "Train Loss:2.229818820953369, valid loss: 0.3867243528366089\n",
      "Train Loss:0.8759057521820068, valid loss: 0.1396448165178299\n",
      "Train Loss:1.0186829566955566, valid loss: 0.04392430931329727\n",
      "Train Loss:0.3899076282978058, valid loss: 0.2900722026824951\n",
      "Train Loss:0.3402332663536072, valid loss: 0.40888771414756775\n",
      "Train Loss:0.1681937575340271, valid loss: 0.061399683356285095\n",
      "Train Loss:0.41258490085601807, valid loss: 0.7547692060470581\n",
      "Key 20151224 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151231 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:16\n",
      "Train Loss:0.0008439552038908005, valid loss: 1.6987950801849365\n",
      "Train Loss:1.4702444076538086, valid loss: 0.706442654132843\n",
      "Train Loss:1.2255284786224365, valid loss: 0.4302367866039276\n",
      "Train Loss:1.1860840320587158, valid loss: 0.6480539441108704\n",
      "Train Loss:1.8465873003005981, valid loss: 0.7901051044464111\n",
      "Train Loss:0.9938177466392517, valid loss: 2.545039653778076\n",
      "Train Loss:1.291789174079895, valid loss: 1.8101918697357178\n",
      "Key 20151224 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151231 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:1 features count:42\n",
      "Train Loss:0.0009592110873199999, valid loss: 1.1721551418304443\n",
      "Train Loss:0.00014533563808072358, valid loss: 0.909801721572876\n",
      "Train Loss:0.2792789340019226, valid loss: 2.125187397003174\n",
      "Train Loss:0.6595825552940369, valid loss: 0.8982923626899719\n",
      "Train Loss:1.4063137769699097, valid loss: 0.9614118337631226\n",
      "Train Loss:0.20707237720489502, valid loss: 0.5564367175102234\n",
      "Train Loss:0.8775281310081482, valid loss: 0.11994365602731705\n",
      "Key 20151224 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151204 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151216 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151222 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151214 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151202 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151227 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151203 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151223 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151205 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151229 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151208 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151219 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151206 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151225 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151210 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151217 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151207 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151215 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151213 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151209 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151228 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151226 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151218 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151231 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151212 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151211 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151221 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151201 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151220 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Key 20151230 | Test Accuracy: 0.500000, quality_score: 0.0, recall: 0.0, prec: 0.0\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:1\n",
      "Train Loss:0.0006147497915662825, valid loss: 0.0006313721532933414\n",
      "Train Loss:0.0003093514824286103, valid loss: 3.4155742469010875e-05\n",
      "Train Loss:3.806069798883982e-05, valid loss: 2.9994951546541415e-05\n",
      "Train Loss:0.00019989268912468106, valid loss: 2.025818139372859e-05\n",
      "Train Loss:3.9435581129509956e-05, valid loss: 3.4795313695212826e-05\n",
      "Train Loss:8.131017239065841e-05, valid loss: 3.5199882404413074e-05\n",
      "Train Loss:0.00020338546892162412, valid loss: 3.619916469688178e-06\n",
      "Key 20151224 | Test Accuracy: 0.524735, quality_score: 0.02966313961789622, recall: 0.9516413797063172, prec: 0.9837617533151645\n",
      "Key 20151204 | Test Accuracy: 0.506563, quality_score: 0.00647063475154868, recall: 0.9431252789844422, prec: 0.9869323489871716\n",
      "Key 20151216 | Test Accuracy: 0.509601, quality_score: 0.014160743885768034, recall: 0.960744329462593, prec: 0.9792517472934082\n",
      "Key 20151222 | Test Accuracy: 0.508004, quality_score: 0.010068707457481153, recall: 0.9659675709224443, prec: 0.9869586224077667\n",
      "Key 20151214 | Test Accuracy: 0.506306, quality_score: 0.007349155365164071, recall: 0.9545082436576645, prec: 0.9851676735837492\n",
      "Key 20151202 | Test Accuracy: 0.523998, quality_score: 0.04357997702331073, recall: 0.9452256250161253, prec: 0.9557757612386787\n",
      "Key 20151227 | Test Accuracy: 0.506795, quality_score: 0.006853013200795831, recall: 0.951969344858344, prec: 0.9883638356983567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151203 | Test Accuracy: 0.532402, quality_score: 0.08229109627853344, recall: 0.9502745779389147, prec: 0.9120842656955919\n",
      "Key 20151223 | Test Accuracy: 0.511634, quality_score: 0.019646034472757513, recall: 0.9512514782751154, prec: 0.9660341062701466\n",
      "Key 20151205 | Test Accuracy: 0.508981, quality_score: 0.009380479359955748, recall: 0.94881644091546, prec: 0.9867642315598855\n",
      "Key 20151229 | Test Accuracy: 0.501850, quality_score: 0.002794905590171601, recall: 0.9512898420330683, prec: 0.972863919873898\n",
      "Key 20151208 | Test Accuracy: 0.524908, quality_score: 0.03041583521569516, recall: 0.9547929900724614, prec: 0.9842057464759072\n",
      "Key 20151219 | Test Accuracy: 0.534907, quality_score: 0.15354415793729836, recall: 0.9752526431644598, prec: 0.7045514782063947\n",
      "Key 20151206 | Test Accuracy: 0.514711, quality_score: 0.016528047620681892, recall: 0.9521632440806687, prec: 0.9857335127860027\n",
      "Key 20151225 | Test Accuracy: 0.518584, quality_score: 0.0198025536492406, recall: 0.9486788819629174, prec: 0.9863926271466315\n",
      "Key 20151210 | Test Accuracy: 0.509594, quality_score: 0.010795461695505434, recall: 0.9568880950799041, prec: 0.9869555052121545\n",
      "Key 20151217 | Test Accuracy: 0.531926, quality_score: 0.055620713622564405, recall: 0.957332899658481, prec: 0.9685605786380419\n",
      "Key 20151207 | Test Accuracy: 0.526046, quality_score: 0.044064614791055855, recall: 0.9510677315045899, prec: 0.9661111294820498\n",
      "Key 20151215 | Test Accuracy: 0.527331, quality_score: 0.036176183804751544, recall: 0.9560474968236591, prec: 0.981885938193332\n",
      "Key 20151213 | Test Accuracy: 0.505940, quality_score: 0.0058404879766061, recall: 0.9599564662613526, prec: 0.9907121942150681\n",
      "Key 20151209 | Test Accuracy: 0.504229, quality_score: 0.0055189282740884275, recall: 0.9574925634521034, prec: 0.9824490317238707\n",
      "Key 20151228 | Test Accuracy: 0.501185, quality_score: 0.001191265576441405, recall: 0.9535610019178753, prec: 0.9887133311650147\n",
      "Key 20151226 | Test Accuracy: 0.515142, quality_score: 0.042673968068922784, recall: 0.956463951975862, prec: 0.9052655426513724\n",
      "Key 20151218 | Test Accuracy: 0.514728, quality_score: 0.019886868282557146, recall: 0.9528851984885944, prec: 0.9794714728394099\n",
      "Key 20151231 | Test Accuracy: 0.506236, quality_score: 0.01536513531119751, recall: 0.9531514962920488, prec: 0.9262897509692458\n",
      "Key 20151212 | Test Accuracy: 0.503363, quality_score: 0.0047119111131576664, recall: 0.9546037814410356, prec: 0.9783415769566483\n",
      "Key 20151211 | Test Accuracy: 0.497328, quality_score: -0.002518792553538323, recall: 0.9530874145700898, prec: 0.9899225671260563\n",
      "Key 20151221 | Test Accuracy: 0.514443, quality_score: 0.017716655078272796, recall: 0.9642223269048673, prec: 0.9871058566786987\n",
      "Key 20151201 | Test Accuracy: 0.531318, quality_score: 0.04599006503509318, recall: 0.943768931918799, prec: 0.9715155638880595\n",
      "Key 20151220 | Test Accuracy: 0.523345, quality_score: 0.05454209255158789, recall: 0.9584783321521879, prec: 0.9411175157085735\n",
      "Key 20151230 | Test Accuracy: 0.515855, quality_score: 0.03073105024773189, recall: 0.947021033575166, prec: 0.9505356074749416\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:4\n",
      "Train Loss:0.00047737668501213193, valid loss: 0.0003357290697749704\n",
      "Train Loss:0.00016139641229528934, valid loss: 2.184224285883829e-05\n",
      "Train Loss:3.8486461562570184e-05, valid loss: 1.611335277557373\n",
      "Train Loss:2.8133288651588373e-05, valid loss: 1.0892948012042325e-05\n",
      "Train Loss:0.0001793166738934815, valid loss: 1.0909365300904028e-05\n",
      "Train Loss:5.8016965340357274e-05, valid loss: 9.871519068838097e-06\n",
      "Train Loss:0.00015570528921671212, valid loss: 2.3572241843794473e-05\n",
      "Key 20151224 | Test Accuracy: 0.637182, quality_score: 0.11481637015469817, recall: 0.8970626934799355, prec: 0.98805815981829\n",
      "Key 20151204 | Test Accuracy: 0.565525, quality_score: 0.04426311382346021, recall: 0.8696983883318272, prec: 0.9887243637083765\n",
      "Key 20151216 | Test Accuracy: 0.629298, quality_score: 0.11469950027435201, recall: 0.8858766532064699, prec: 0.9849219439032533\n",
      "Key 20151222 | Test Accuracy: 0.538652, quality_score: 0.02534941703592326, recall: 0.8592671131914453, prec: 0.987920130143183\n",
      "Key 20151214 | Test Accuracy: 0.558927, quality_score: 0.043784508860743074, recall: 0.8796006057084201, prec: 0.9869593359731953\n",
      "Key 20151202 | Test Accuracy: 0.617052, quality_score: 0.15352201337292062, recall: 0.8942138170059427, prec: 0.9652657865909133\n",
      "Key 20151227 | Test Accuracy: 0.563356, quality_score: 0.043074773610692174, recall: 0.8875951473358746, prec: 0.9898651511974913\n",
      "Key 20151203 | Test Accuracy: 0.666780, quality_score: 0.2966399002050799, recall: 0.9088640584347949, prec: 0.9385446989253218\n",
      "Key 20151223 | Test Accuracy: 0.607664, quality_score: 0.1312069687477877, recall: 0.9069467411393085, prec: 0.9732504862049642\n",
      "Key 20151205 | Test Accuracy: 0.556827, quality_score: 0.03937114867050179, recall: 0.8745184493227464, prec: 0.9882445835989985\n",
      "Key 20151229 | Test Accuracy: 0.566031, quality_score: 0.06608500458124034, recall: 0.8833131757105562, prec: 0.9767385915125419\n",
      "Key 20151208 | Test Accuracy: 0.634836, quality_score: 0.11145976910753867, recall: 0.8971238867430651, prec: 0.9882969370890038\n",
      "Key 20151219 | Test Accuracy: 0.642419, quality_score: 0.3779458538183198, recall: 0.9467631657939499, prec: 0.7600010709217959\n",
      "Key 20151206 | Test Accuracy: 0.617541, quality_score: 0.09044652780033711, recall: 0.8934059921888119, prec: 0.989115050616514\n",
      "Key 20151225 | Test Accuracy: 0.635159, quality_score: 0.10535771745427328, recall: 0.9014968039067773, prec: 0.990047636177333\n",
      "Key 20151210 | Test Accuracy: 0.571109, quality_score: 0.05154616720199963, recall: 0.8892480136319757, prec: 0.9887966137786874\n",
      "Key 20151217 | Test Accuracy: 0.618587, quality_score: 0.1303626831530126, recall: 0.8854480403317613, prec: 0.9751686290805014\n",
      "Key 20151207 | Test Accuracy: 0.605981, quality_score: 0.12393215488449895, recall: 0.8936880140628052, prec: 0.9724708354980399\n",
      "Key 20151215 | Test Accuracy: 0.604592, quality_score: 0.08742368934610378, recall: 0.8811517576973633, prec: 0.9852975495915987\n",
      "Key 20151213 | Test Accuracy: 0.489596, quality_score: -0.0053554694353034876, recall: 0.8305361655282845, prec: 0.9903635880866501\n",
      "Key 20151209 | Test Accuracy: 0.571498, quality_score: 0.05852020836812925, recall: 0.8848397440024725, prec: 0.9851139985759139\n",
      "Key 20151228 | Test Accuracy: 0.539593, quality_score: 0.023603881934698798, recall: 0.8531710981418347, prec: 0.9897248795809512\n",
      "Key 20151226 | Test Accuracy: 0.553507, quality_score: 0.08817735627114429, recall: 0.8573700737306579, prec: 0.9135913923642808\n",
      "Key 20151218 | Test Accuracy: 0.609465, quality_score: 0.10283426163624984, recall: 0.8997061155945328, prec: 0.983898840078816\n",
      "Key 20151231 | Test Accuracy: 0.567640, quality_score: 0.10917314182348581, recall: 0.8895411650763793, prec: 0.9360065339999779\n",
      "Key 20151212 | Test Accuracy: 0.537017, quality_score: 0.03021416824008395, recall: 0.850775025077124, prec: 0.9800511910420037\n",
      "Key 20151211 | Test Accuracy: 0.515251, quality_score: 0.008781385171592234, recall: 0.8612797325931895, prec: 0.9903296763632995\n",
      "Key 20151221 | Test Accuracy: 0.556696, quality_score: 0.03819256365384672, recall: 0.8681617251699904, prec: 0.9884282835287235\n",
      "Key 20151201 | Test Accuracy: 0.674635, quality_score: 0.19211688896389706, recall: 0.9010570965272263, prec: 0.9811318800807746\n",
      "Key 20151220 | Test Accuracy: 0.606680, quality_score: 0.15677060187287198, recall: 0.8909890946784993, prec: 0.9523621090238953\n",
      "Key 20151230 | Test Accuracy: 0.641017, quality_score: 0.19971329616572428, recall: 0.9059651562281323, prec: 0.9642457295245417\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:8\n",
      "Train Loss:0.0005302959471009672, valid loss: 0.0006388192996382713\n",
      "Train Loss:4.2156942072324455e-05, valid loss: 2.115070128638763e-05\n",
      "Train Loss:0.0001629730686545372, valid loss: 2.9815542802680284e-05\n",
      "Train Loss:7.844240462873131e-05, valid loss: 1.4857962131500244\n",
      "Train Loss:4.405386061989702e-05, valid loss: 7.642445780220442e-06\n",
      "Train Loss:0.0001525464904261753, valid loss: 9.004123057820834e-06\n",
      "Train Loss:4.8873393097892404e-05, valid loss: 0.00567634915933013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151224 | Test Accuracy: 0.551728, quality_score: 0.061561467065997674, recall: 0.9517657810249712, prec: 0.9847183513442603\n",
      "Key 20151204 | Test Accuracy: 0.521809, quality_score: 0.022389156091791367, recall: 0.9482132293968856, prec: 0.9873521520613687\n",
      "Key 20151216 | Test Accuracy: 0.564811, quality_score: 0.08433062817988249, recall: 0.9511399337596974, prec: 0.9816687327696264\n",
      "Key 20151222 | Test Accuracy: 0.538118, quality_score: 0.036239387273905944, recall: 0.9393090811022488, prec: 0.9878046885319782\n",
      "Key 20151214 | Test Accuracy: 0.530038, quality_score: 0.032745142841877704, recall: 0.9483154264772626, prec: 0.9859106483747635\n",
      "Key 20151202 | Test Accuracy: 0.529277, quality_score: 0.05918823563574908, recall: 0.9572830396381056, prec: 0.9562386172296484\n",
      "Key 20151227 | Test Accuracy: 0.529267, quality_score: 0.029383033709959858, recall: 0.9520373429543972, prec: 0.9889149257948181\n",
      "Key 20151203 | Test Accuracy: 0.564232, quality_score: 0.16759089410954642, recall: 0.9593473953877998, prec: 0.9177744139010088\n",
      "Key 20151223 | Test Accuracy: 0.536051, quality_score: 0.06530278811010161, recall: 0.959744840755853, prec: 0.9677412693905996\n",
      "Key 20151205 | Test Accuracy: 0.523260, quality_score: 0.02377013868738061, recall: 0.9467650630546474, prec: 0.9871665004363546\n",
      "Key 20151229 | Test Accuracy: 0.518176, quality_score: 0.027791991476035435, recall: 0.9533916429445862, prec: 0.973772220548358\n",
      "Key 20151208 | Test Accuracy: 0.561490, quality_score: 0.07532221658785308, recall: 0.9563124461327289, prec: 0.9854607176221366\n",
      "Key 20151219 | Test Accuracy: 0.551074, quality_score: 0.20214652587858584, recall: 0.9736784177700697, prec: 0.7121029548791851\n",
      "Key 20151206 | Test Accuracy: 0.558537, quality_score: 0.0665111430305214, recall: 0.9545791871380825, prec: 0.9870666718945169\n",
      "Key 20151225 | Test Accuracy: 0.560866, quality_score: 0.06774503114362503, recall: 0.9544308571882321, prec: 0.9876285800855574\n",
      "Key 20151210 | Test Accuracy: 0.525785, quality_score: 0.027429364210463814, recall: 0.9518894434261282, prec: 0.9874040200108649\n",
      "Key 20151217 | Test Accuracy: 0.540561, quality_score: 0.06451180403450076, recall: 0.9484176288827452, prec: 0.96917554384857\n",
      "Key 20151207 | Test Accuracy: 0.530285, quality_score: 0.055334998811827386, recall: 0.9589617830248053, prec: 0.9664018545957561\n",
      "Key 20151215 | Test Accuracy: 0.519727, quality_score: 0.024367509596536847, recall: 0.9486988902029563, prec: 0.981591977602752\n",
      "Key 20151213 | Test Accuracy: 0.492878, quality_score: -0.004816188484792927, recall: 0.9103730391053066, prec: 0.9904511738686628\n",
      "Key 20151209 | Test Accuracy: 0.527634, quality_score: 0.03471592578629666, recall: 0.9547669023183143, prec: 0.9833031254144379\n",
      "Key 20151228 | Test Accuracy: 0.511539, quality_score: 0.010080150488061693, recall: 0.9377199571848782, prec: 0.9889608900935306\n",
      "Key 20151226 | Test Accuracy: 0.514512, quality_score: 0.03536905846037795, recall: 0.9395540885712756, prec: 0.9051972642144933\n",
      "Key 20151218 | Test Accuracy: 0.542365, quality_score: 0.057463518557067085, recall: 0.9545062275504969, prec: 0.980673376467769\n",
      "Key 20151231 | Test Accuracy: 0.517682, quality_score: 0.04642369526997017, recall: 0.9608226372081802, prec: 0.9279337079258739\n",
      "Key 20151212 | Test Accuracy: 0.528470, quality_score: 0.03294347234147396, recall: 0.9328311599825879, prec: 0.9794951490268244\n",
      "Key 20151211 | Test Accuracy: 0.510529, quality_score: 0.008622032626679479, recall: 0.937037037037037, prec: 0.9902012110508389\n",
      "Key 20151221 | Test Accuracy: 0.544288, quality_score: 0.04308096037061404, recall: 0.9422865544659044, prec: 0.9879468784610319\n",
      "Key 20151201 | Test Accuracy: 0.557309, quality_score: 0.09323491967562954, recall: 0.9566842453148702, prec: 0.9731021306440301\n",
      "Key 20151220 | Test Accuracy: 0.542080, quality_score: 0.09465605727306604, recall: 0.9571085560761997, prec: 0.9434076360072565\n",
      "Key 20151230 | Test Accuracy: 0.544628, quality_score: 0.09378784424481253, recall: 0.9585332930028035, prec: 0.9534454498506859\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:16\n",
      "Train Loss:0.00015506165800616145, valid loss: 1.0114257335662842\n",
      "Train Loss:0.00016132670862134546, valid loss: 6.423066952265799e-05\n",
      "Train Loss:3.64723673556e-05, valid loss: 4.059827915625647e-05\n",
      "Train Loss:1.753913784341421e-05, valid loss: 0.0022539079654961824\n",
      "Train Loss:5.089385012979619e-05, valid loss: 1.5116494068934117e-05\n",
      "Train Loss:0.00012069763761246577, valid loss: 0.5907931923866272\n",
      "Train Loss:1.2854337683165795e-06, valid loss: 0.0005238843732513487\n",
      "Key 20151224 | Test Accuracy: 0.544222, quality_score: 0.06120179570746064, recall: 0.9650575834565385, prec: 0.9844305181367381\n",
      "Key 20151204 | Test Accuracy: 0.539933, quality_score: 0.04595110276528582, recall: 0.959866160049065, prec: 0.9878394024276377\n",
      "Key 20151216 | Test Accuracy: 0.543297, quality_score: 0.05921465463138259, recall: 0.9553751899142641, prec: 0.9807187121884791\n",
      "Key 20151222 | Test Accuracy: 0.521326, quality_score: 0.021013020254333003, recall: 0.9433950414322289, prec: 0.9873335988277006\n",
      "Key 20151214 | Test Accuracy: 0.539596, quality_score: 0.044140934420185046, recall: 0.9510300860631922, prec: 0.9862061685169872\n",
      "Key 20151202 | Test Accuracy: 0.540360, quality_score: 0.09238242696482231, recall: 0.9686868426257987, prec: 0.9572274770755255\n",
      "Key 20151227 | Test Accuracy: 0.543662, quality_score: 0.04667599202859302, recall: 0.9583891651033771, prec: 0.9892611630643463\n",
      "Key 20151203 | Test Accuracy: 0.568054, quality_score: 0.2024948614894319, recall: 0.9727771154025149, prec: 0.9182984385198574\n",
      "Key 20151223 | Test Accuracy: 0.547760, quality_score: 0.09351505567585289, recall: 0.966968550496854, prec: 0.9685403975139991\n",
      "Key 20151205 | Test Accuracy: 0.538540, quality_score: 0.04487565054450099, recall: 0.9601083605791686, prec: 0.9875816742255362\n",
      "Key 20151229 | Test Accuracy: 0.542137, quality_score: 0.06045916416242059, recall: 0.9478495258042154, prec: 0.9751224341928069\n",
      "Key 20151208 | Test Accuracy: 0.537806, quality_score: 0.05158619321097919, recall: 0.9647524499632905, prec: 0.9846356338615513\n",
      "Key 20151219 | Test Accuracy: 0.580883, quality_score: 0.30143817236086884, recall: 0.9842377347163392, prec: 0.7259845309086616\n",
      "Key 20151206 | Test Accuracy: 0.533056, quality_score: 0.042697936321913876, recall: 0.9649146565406512, prec: 0.9862796775336641\n",
      "Key 20151225 | Test Accuracy: 0.539244, quality_score: 0.05172326270843049, recall: 0.9678752928097828, prec: 0.9869784813457763\n",
      "Key 20151210 | Test Accuracy: 0.545036, quality_score: 0.050570480487702976, recall: 0.9576462955653088, prec: 0.9879286848959383\n",
      "Key 20151217 | Test Accuracy: 0.555049, quality_score: 0.09027605736392494, recall: 0.9527988290778988, prec: 0.9701572119795064\n",
      "Key 20151207 | Test Accuracy: 0.546450, quality_score: 0.09190424700922159, recall: 0.9667310488508866, prec: 0.9675447027329348\n",
      "Key 20151215 | Test Accuracy: 0.553748, quality_score: 0.06873125071392218, recall: 0.9536785140216687, prec: 0.982934818706035\n",
      "Key 20151213 | Test Accuracy: 0.490859, quality_score: -0.005893432041254552, recall: 0.9002727113512973, prec: 0.9904077728692732\n",
      "Key 20151209 | Test Accuracy: 0.537446, quality_score: 0.04658292269591145, recall: 0.9541487996360062, prec: 0.9836623432943478\n",
      "Key 20151228 | Test Accuracy: 0.522921, quality_score: 0.017951462816612033, recall: 0.9212928185612909, prec: 0.9892424511020504\n",
      "Key 20151226 | Test Accuracy: 0.534268, quality_score: 0.07510301734082958, recall: 0.9270441281609539, prec: 0.9090243699062078\n",
      "Key 20151218 | Test Accuracy: 0.551848, quality_score: 0.0838463789345743, recall: 0.9694278583757056, prec: 0.9810520228485106\n",
      "Key 20151231 | Test Accuracy: 0.539595, quality_score: 0.07933152348201232, recall: 0.931582094144549, prec: 0.9312922934958895\n",
      "Key 20151212 | Test Accuracy: 0.516944, quality_score: 0.01909291328760481, recall: 0.9282851032420463, prec: 0.9789706475268156\n",
      "Key 20151211 | Test Accuracy: 0.530720, quality_score: 0.025938422447658867, recall: 0.9414510400811771, prec: 0.9906260893049489\n",
      "Key 20151221 | Test Accuracy: 0.531379, quality_score: 0.032303921607208484, recall: 0.9486246127931709, prec: 0.9875810550406452\n",
      "Key 20151201 | Test Accuracy: 0.541415, quality_score: 0.07971983908898975, recall: 0.9695662718207454, prec: 0.9720809157612194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 20151220 | Test Accuracy: 0.554048, quality_score: 0.11715319372617232, recall: 0.9546704604351551, prec: 0.9448908338088035\n",
      "Key 20151230 | Test Accuracy: 0.559695, quality_score: 0.1439598152259568, recall: 0.9715624619079651, prec: 0.954904666610462\n",
      "Best Accuracy on Test data: 0\n",
      "Current Layer Attributes - epochs:1 hidden layers:3 features count:42\n",
      "Train Loss:4.987735519534908e-05, valid loss: 0.46129077672958374\n",
      "Train Loss:2.93846387648955e-05, valid loss: 1.8720847947406583e-05\n",
      "Train Loss:2.342178595426958e-05, valid loss: 0.3281831741333008\n",
      "Train Loss:5.826502274430823e-06, valid loss: 0.1163952648639679\n",
      "Train Loss:7.309024658752605e-05, valid loss: 4.446404636837542e-05\n",
      "Train Loss:8.986554166767746e-05, valid loss: 0.19591183960437775\n",
      "Train Loss:1.3886680790164974e-05, valid loss: 7.323551471927203e-06\n",
      "Key 20151224 | Test Accuracy: 0.505219, quality_score: 0.01257729708177539, recall: 0.9884546006956905, prec: 0.98306423598719\n",
      "Key 20151204 | Test Accuracy: 0.510353, quality_score: 0.019323241871612926, recall: 0.9850301031017626, prec: 0.9870252658220483\n",
      "Key 20151216 | Test Accuracy: 0.508282, quality_score: 0.026691382405609837, recall: 0.9923093126336122, prec: 0.9791834351975517\n",
      "Key 20151222 | Test Accuracy: 0.503649, quality_score: 0.007181515338446548, recall: 0.9864010669306902, prec: 0.9868385551609052\n",
      "Key 20151214 | Test Accuracy: 0.516737, quality_score: 0.03464747713203621, recall: 0.9864903269043973, prec: 0.9854745762711864\n",
      "Key 20151202 | Test Accuracy: 0.504837, quality_score: 0.0162722222076767, recall: 0.9845326246807193, prec: 0.9539557014766175\n",
      "Key 20151227 | Test Accuracy: 0.510398, quality_score: 0.019625741358970245, recall: 0.9869723647737864, prec: 0.9884431269653694\n",
      "Key 20151203 | Test Accuracy: 0.523247, quality_score: 0.11140628929014712, recall: 0.9893360045031668, prec: 0.9102635206224665\n",
      "Key 20151223 | Test Accuracy: 0.506005, quality_score: 0.0212410421249473, recall: 0.9895663214371649, prec: 0.9656197389258292\n",
      "Key 20151205 | Test Accuracy: 0.507745, quality_score: 0.015690934046773432, recall: 0.9870677253619804, prec: 0.9867211516552804\n",
      "Key 20151229 | Test Accuracy: 0.513300, quality_score: 0.031801901973454276, recall: 0.981828640540421, prec: 0.9734792813740759\n",
      "Key 20151208 | Test Accuracy: 0.505122, quality_score: 0.011954208653947754, recall: 0.9879975739777189, prec: 0.9835206040114904\n",
      "Key 20151219 | Test Accuracy: 0.519605, quality_score: 0.12261883248063582, recall: 0.9897808758296368, prec: 0.6974556872182713\n",
      "Key 20151206 | Test Accuracy: 0.507432, quality_score: 0.016808695317770003, recall: 0.9887490723537027, prec: 0.9855033339632319\n",
      "Key 20151225 | Test Accuracy: 0.503563, quality_score: 0.008496838649178795, recall: 0.9901883908365426, prec: 0.9859460474493855\n",
      "Key 20151210 | Test Accuracy: 0.514350, quality_score: 0.028903460926850963, recall: 0.987263013495187, prec: 0.9870739846357036\n",
      "Key 20151217 | Test Accuracy: 0.517096, quality_score: 0.05297890466070634, recall: 0.98743210278094, prec: 0.9675153531625762\n",
      "Key 20151207 | Test Accuracy: 0.505736, quality_score: 0.02174253135124454, recall: 0.9907116039844615, prec: 0.9646173838628224\n",
      "Key 20151215 | Test Accuracy: 0.510424, quality_score: 0.02520716300784813, recall: 0.9873532200957112, prec: 0.981206153756586\n",
      "Key 20151213 | Test Accuracy: 0.507008, quality_score: 0.006684035129489273, recall: 0.9573469438815082, prec: 0.9907332848291522\n",
      "Key 20151209 | Test Accuracy: 0.516899, quality_score: 0.03970793737287469, recall: 0.9878353958217976, prec: 0.9828907975040894\n",
      "Key 20151228 | Test Accuracy: 0.511225, quality_score: 0.013977573207765596, recall: 0.9705136989128371, prec: 0.9889443431936071\n",
      "Key 20151226 | Test Accuracy: 0.508822, quality_score: 0.029411395431671306, recall: 0.9689739123861836, prec: 0.9040755335748856\n",
      "Key 20151218 | Test Accuracy: 0.505886, quality_score: 0.015435152403887181, recall: 0.9880463684284182, prec: 0.9790774671081142\n",
      "Key 20151231 | Test Accuracy: 0.518054, quality_score: 0.06656835627455546, recall: 0.9819479942798602, prec: 0.9279313808416155\n",
      "Key 20151212 | Test Accuracy: 0.515602, quality_score: 0.02611411219909863, recall: 0.9692336809432783, prec: 0.9788785161058483\n",
      "Key 20151211 | Test Accuracy: 0.514091, quality_score: 0.018345217421789614, recall: 0.9762945056256902, prec: 0.9902646643034664\n",
      "Key 20151221 | Test Accuracy: 0.501046, quality_score: 0.0019767767807422902, recall: 0.9851303061015707, prec: 0.986740763335494\n",
      "Key 20151201 | Test Accuracy: 0.507748, quality_score: 0.02422280965504554, recall: 0.9882402172207349, prec: 0.9700154963499561\n",
      "Key 20151220 | Test Accuracy: 0.522085, quality_score: 0.07623820762162418, recall: 0.9828963095376609, prec: 0.94089772278531\n",
      "Key 20151230 | Test Accuracy: 0.502251, quality_score: 0.00992750089945093, recall: 0.9901626614537952, prec: 0.9491299512288005\n",
      "Best Accuracy on Test data: 0\n"
     ]
    }
   ],
   "source": [
    "#%%timeit -r 1\n",
    "\n",
    "Hyperparameters.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:02:10.600500Z",
     "start_time": "2017-07-21T22:01:21.998604Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Panel(Train.predictions).to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")\n",
    "df_results.to_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:02:10.677343Z",
     "start_time": "2017-07-21T22:02:10.602178Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "    \n",
    "    #print(cm)\n",
    "\n",
    "    label = [[\"\\n True Positive\", \"\\n False Negative \\n Type II Error\"],\n",
    "             [\"\\n False Positive \\n Type I Error\", \"\\n True Negative\"]\n",
    "            ]\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j, i, \"{} {}\".format(cm[i, j].round(4), label[i][j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot(actual_value, pred_value):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm_2labels = confusion_matrix(y_pred = pred_value, y_true = actual_value)\n",
    "    plt.figure(figsize=[6,6])\n",
    "    plot_confusion_matrix(cm_2labels, ['Normal', 'Attack'], normalize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:02:10.700312Z",
     "start_time": "2017-07-21T22:02:10.678823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "past_scores = pd.read_pickle(\"dataset/scores/tf_vae_dense_trained_together_nsl_kdd_scores-all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:02:10.743979Z",
     "start_time": "2017-07-21T22:02:10.701705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20151206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20151225</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20151210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20151217</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20151207</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20151215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20151213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20151209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20151226</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20151218</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20151231</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20151212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20151221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20151201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20151220</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20151230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20151224</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20151204</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20151216</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20151222</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>20151214</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20151202</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20151227</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20151223</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20151205</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20151229</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20151208</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20151219</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20151206</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>20151225</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20151210</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20151217</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20151207</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20151215</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>20151219</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.580883</td>\n",
       "      <td>0.301438</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>20151206</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.533056</td>\n",
       "      <td>0.042698</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>20151225</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.539244</td>\n",
       "      <td>0.051723</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>20151210</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.545036</td>\n",
       "      <td>0.050570</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>20151217</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.555049</td>\n",
       "      <td>0.090276</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>20151207</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.546450</td>\n",
       "      <td>0.091904</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>20151215</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.553748</td>\n",
       "      <td>0.068731</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>20151213</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.490859</td>\n",
       "      <td>-0.005893</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>20151209</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.537446</td>\n",
       "      <td>0.046583</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>20151228</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.522921</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>20151226</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.534268</td>\n",
       "      <td>0.075103</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>20151218</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.551848</td>\n",
       "      <td>0.083846</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>20151231</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.539595</td>\n",
       "      <td>0.079332</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>20151212</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.516944</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>20151211</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.530720</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>20151221</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.531379</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>20151201</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.541415</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>20151220</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.554048</td>\n",
       "      <td>0.117153</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.559695</td>\n",
       "      <td>0.143960</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>20151224</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.505219</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>20151204</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.510353</td>\n",
       "      <td>0.019323</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>20151216</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.508282</td>\n",
       "      <td>0.026691</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>20151222</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.503649</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>20151214</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.516737</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>20151202</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.504837</td>\n",
       "      <td>0.016272</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.510398</td>\n",
       "      <td>0.019626</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>20151203</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.523247</td>\n",
       "      <td>0.111406</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>20151223</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.506005</td>\n",
       "      <td>0.021241</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>20151205</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.507745</td>\n",
       "      <td>0.015691</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>20151229</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.513300</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>20151208</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.505122</td>\n",
       "      <td>0.011954</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>20151219</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.519605</td>\n",
       "      <td>0.122619</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>20151206</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.507432</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>20151225</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.503563</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>20151210</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.514350</td>\n",
       "      <td>0.028903</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>20151217</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.517096</td>\n",
       "      <td>0.052979</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>20151207</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.505736</td>\n",
       "      <td>0.021743</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>20151215</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.510424</td>\n",
       "      <td>0.025207</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>20151213</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.507008</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>20151209</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.516899</td>\n",
       "      <td>0.039708</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.511225</td>\n",
       "      <td>0.013978</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>20151226</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.508822</td>\n",
       "      <td>0.029411</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>20151218</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.505886</td>\n",
       "      <td>0.015435</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>20151231</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.518054</td>\n",
       "      <td>0.066568</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>20151212</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.515602</td>\n",
       "      <td>0.026114</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>20151211</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.514091</td>\n",
       "      <td>0.018345</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>20151221</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.501046</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>20151201</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.507748</td>\n",
       "      <td>0.024223</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>20151220</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.522085</td>\n",
       "      <td>0.076238</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>20151230</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.502251</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "0    20151224               1              1     0.004345    0.500000   \n",
       "1    20151204               1              1     0.004345    0.500000   \n",
       "2    20151216               1              1     0.004345    0.500000   \n",
       "3    20151222               1              1     0.004345    0.500000   \n",
       "4    20151214               1              1     0.004345    0.500000   \n",
       "5    20151202               1              1     0.004345    0.500000   \n",
       "6    20151227               1              1     0.004345    0.500000   \n",
       "7    20151203               1              1     0.004345    0.500000   \n",
       "8    20151223               1              1     0.004345    0.500000   \n",
       "9    20151205               1              1     0.004345    0.500000   \n",
       "10   20151229               1              1     0.004345    0.500000   \n",
       "11   20151208               1              1     0.004345    0.500000   \n",
       "12   20151219               1              1     0.004345    0.500000   \n",
       "13   20151206               1              1     0.004345    0.500000   \n",
       "14   20151225               1              1     0.004345    0.500000   \n",
       "15   20151210               1              1     0.004345    0.500000   \n",
       "16   20151217               1              1     0.004345    0.500000   \n",
       "17   20151207               1              1     0.004345    0.500000   \n",
       "18   20151215               1              1     0.004345    0.500000   \n",
       "19   20151213               1              1     0.004345    0.500000   \n",
       "20   20151209               1              1     0.004345    0.500000   \n",
       "21   20151228               1              1     0.004345    0.500000   \n",
       "22   20151226               1              1     0.004345    0.500000   \n",
       "23   20151218               1              1     0.004345    0.500000   \n",
       "24   20151231               1              1     0.004345    0.500000   \n",
       "25   20151212               1              1     0.004345    0.500000   \n",
       "26   20151211               1              1     0.004345    0.500000   \n",
       "27   20151221               1              1     0.004345    0.500000   \n",
       "28   20151201               1              1     0.004345    0.500000   \n",
       "29   20151220               1              1     0.004345    0.500000   \n",
       "30   20151230               1              1     0.004345    0.500000   \n",
       "31   20151224               4              1     0.002897    0.500000   \n",
       "32   20151204               4              1     0.002897    0.500000   \n",
       "33   20151216               4              1     0.002897    0.500000   \n",
       "34   20151222               4              1     0.002897    0.500000   \n",
       "35   20151214               4              1     0.002897    0.500000   \n",
       "36   20151202               4              1     0.002897    0.500000   \n",
       "37   20151227               4              1     0.002897    0.500000   \n",
       "38   20151203               4              1     0.002897    0.500000   \n",
       "39   20151223               4              1     0.002897    0.500000   \n",
       "40   20151205               4              1     0.002897    0.500000   \n",
       "41   20151229               4              1     0.002897    0.500000   \n",
       "42   20151208               4              1     0.002897    0.500000   \n",
       "43   20151219               4              1     0.002897    0.500000   \n",
       "44   20151206               4              1     0.002897    0.500000   \n",
       "45   20151225               4              1     0.002897    0.500000   \n",
       "46   20151210               4              1     0.002897    0.500000   \n",
       "47   20151217               4              1     0.002897    0.500000   \n",
       "48   20151207               4              1     0.002897    0.500000   \n",
       "49   20151215               4              1     0.002897    0.500000   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "260  20151219              16              3     0.971171    0.580883   \n",
       "261  20151206              16              3     0.971171    0.533056   \n",
       "262  20151225              16              3     0.971171    0.539244   \n",
       "263  20151210              16              3     0.971171    0.545036   \n",
       "264  20151217              16              3     0.971171    0.555049   \n",
       "265  20151207              16              3     0.971171    0.546450   \n",
       "266  20151215              16              3     0.971171    0.553748   \n",
       "267  20151213              16              3     0.971171    0.490859   \n",
       "268  20151209              16              3     0.971171    0.537446   \n",
       "269  20151228              16              3     0.971171    0.522921   \n",
       "270  20151226              16              3     0.971171    0.534268   \n",
       "271  20151218              16              3     0.971171    0.551848   \n",
       "272  20151231              16              3     0.971171    0.539595   \n",
       "273  20151212              16              3     0.971171    0.516944   \n",
       "274  20151211              16              3     0.971171    0.530720   \n",
       "275  20151221              16              3     0.971171    0.531379   \n",
       "276  20151201              16              3     0.971171    0.541415   \n",
       "277  20151220              16              3     0.971171    0.554048   \n",
       "278  20151230              16              3     0.971171    0.559695   \n",
       "279  20151224              42              3     0.989413    0.505219   \n",
       "280  20151204              42              3     0.989413    0.510353   \n",
       "281  20151216              42              3     0.989413    0.508282   \n",
       "282  20151222              42              3     0.989413    0.503649   \n",
       "283  20151214              42              3     0.989413    0.516737   \n",
       "284  20151202              42              3     0.989413    0.504837   \n",
       "285  20151227              42              3     0.989413    0.510398   \n",
       "286  20151203              42              3     0.989413    0.523247   \n",
       "287  20151223              42              3     0.989413    0.506005   \n",
       "288  20151205              42              3     0.989413    0.507745   \n",
       "289  20151229              42              3     0.989413    0.513300   \n",
       "290  20151208              42              3     0.989413    0.505122   \n",
       "291  20151219              42              3     0.989413    0.519605   \n",
       "292  20151206              42              3     0.989413    0.507432   \n",
       "293  20151225              42              3     0.989413    0.503563   \n",
       "294  20151210              42              3     0.989413    0.514350   \n",
       "295  20151217              42              3     0.989413    0.517096   \n",
       "296  20151207              42              3     0.989413    0.505736   \n",
       "297  20151215              42              3     0.989413    0.510424   \n",
       "298  20151213              42              3     0.989413    0.507008   \n",
       "299  20151209              42              3     0.989413    0.516899   \n",
       "300  20151228              42              3     0.989413    0.511225   \n",
       "301  20151226              42              3     0.989413    0.508822   \n",
       "302  20151218              42              3     0.989413    0.505886   \n",
       "303  20151231              42              3     0.989413    0.518054   \n",
       "304  20151212              42              3     0.989413    0.515602   \n",
       "305  20151211              42              3     0.989413    0.514091   \n",
       "306  20151221              42              3     0.989413    0.501046   \n",
       "307  20151201              42              3     0.989413    0.507748   \n",
       "308  20151220              42              3     0.989413    0.522085   \n",
       "309  20151230              42              3     0.989413    0.502251   \n",
       "\n",
       "     quality_score  time_taken  \n",
       "0         0.000000   44.018526  \n",
       "1         0.000000   44.018526  \n",
       "2         0.000000   44.018526  \n",
       "3         0.000000   44.018526  \n",
       "4         0.000000   44.018526  \n",
       "5         0.000000   44.018526  \n",
       "6         0.000000   44.018526  \n",
       "7         0.000000   44.018526  \n",
       "8         0.000000   44.018526  \n",
       "9         0.000000   44.018526  \n",
       "10        0.000000   44.018526  \n",
       "11        0.000000   44.018526  \n",
       "12        0.000000   44.018526  \n",
       "13        0.000000   44.018526  \n",
       "14        0.000000   44.018526  \n",
       "15        0.000000   44.018526  \n",
       "16        0.000000   44.018526  \n",
       "17        0.000000   44.018526  \n",
       "18        0.000000   44.018526  \n",
       "19        0.000000   44.018526  \n",
       "20        0.000000   44.018526  \n",
       "21        0.000000   44.018526  \n",
       "22        0.000000   44.018526  \n",
       "23        0.000000   44.018526  \n",
       "24        0.000000   44.018526  \n",
       "25        0.000000   44.018526  \n",
       "26        0.000000   44.018526  \n",
       "27        0.000000   44.018526  \n",
       "28        0.000000   44.018526  \n",
       "29        0.000000   44.018526  \n",
       "30        0.000000   44.018526  \n",
       "31        0.000000   35.370012  \n",
       "32        0.000000   35.370012  \n",
       "33        0.000000   35.370012  \n",
       "34        0.000000   35.370012  \n",
       "35        0.000000   35.370012  \n",
       "36        0.000000   35.370012  \n",
       "37        0.000000   35.370012  \n",
       "38        0.000000   35.370012  \n",
       "39        0.000000   35.370012  \n",
       "40        0.000000   35.370012  \n",
       "41        0.000000   35.370012  \n",
       "42        0.000000   35.370012  \n",
       "43        0.000000   35.370012  \n",
       "44        0.000000   35.370012  \n",
       "45        0.000000   35.370012  \n",
       "46        0.000000   35.370012  \n",
       "47        0.000000   35.370012  \n",
       "48        0.000000   35.370012  \n",
       "49        0.000000   35.370012  \n",
       "..             ...         ...  \n",
       "260       0.301438   46.347040  \n",
       "261       0.042698   46.347040  \n",
       "262       0.051723   46.347040  \n",
       "263       0.050570   46.347040  \n",
       "264       0.090276   46.347040  \n",
       "265       0.091904   46.347040  \n",
       "266       0.068731   46.347040  \n",
       "267      -0.005893   46.347040  \n",
       "268       0.046583   46.347040  \n",
       "269       0.017951   46.347040  \n",
       "270       0.075103   46.347040  \n",
       "271       0.083846   46.347040  \n",
       "272       0.079332   46.347040  \n",
       "273       0.019093   46.347040  \n",
       "274       0.025938   46.347040  \n",
       "275       0.032304   46.347040  \n",
       "276       0.079720   46.347040  \n",
       "277       0.117153   46.347040  \n",
       "278       0.143960   46.347040  \n",
       "279       0.012577   53.214450  \n",
       "280       0.019323   53.214450  \n",
       "281       0.026691   53.214450  \n",
       "282       0.007182   53.214450  \n",
       "283       0.034647   53.214450  \n",
       "284       0.016272   53.214450  \n",
       "285       0.019626   53.214450  \n",
       "286       0.111406   53.214450  \n",
       "287       0.021241   53.214450  \n",
       "288       0.015691   53.214450  \n",
       "289       0.031802   53.214450  \n",
       "290       0.011954   53.214450  \n",
       "291       0.122619   53.214450  \n",
       "292       0.016809   53.214450  \n",
       "293       0.008497   53.214450  \n",
       "294       0.028903   53.214450  \n",
       "295       0.052979   53.214450  \n",
       "296       0.021743   53.214450  \n",
       "297       0.025207   53.214450  \n",
       "298       0.006684   53.214450  \n",
       "299       0.039708   53.214450  \n",
       "300       0.013978   53.214450  \n",
       "301       0.029411   53.214450  \n",
       "302       0.015435   53.214450  \n",
       "303       0.066568   53.214450  \n",
       "304       0.026114   53.214450  \n",
       "305       0.018345   53.214450  \n",
       "306       0.001977   53.214450  \n",
       "307       0.024223   53.214450  \n",
       "308       0.076238   53.214450  \n",
       "309       0.009928   53.214450  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:02:10.785185Z",
     "start_time": "2017-07-21T22:02:10.745307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>20151219</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.642419</td>\n",
       "      <td>0.377946</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>20151219</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.642419</td>\n",
       "      <td>0.377946</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>20151219</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.580883</td>\n",
       "      <td>0.301438</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>20151219</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.580883</td>\n",
       "      <td>0.301438</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>0.296640</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>20151203</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>0.296640</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>20151203</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.568054</td>\n",
       "      <td>0.202495</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>20151203</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.568054</td>\n",
       "      <td>0.202495</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.551074</td>\n",
       "      <td>0.202147</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>20151219</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.551074</td>\n",
       "      <td>0.202147</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>20151230</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.641017</td>\n",
       "      <td>0.199713</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>20151230</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.641017</td>\n",
       "      <td>0.199713</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>20151201</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.674635</td>\n",
       "      <td>0.192117</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>20151201</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.674635</td>\n",
       "      <td>0.192117</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.564232</td>\n",
       "      <td>0.167591</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>20151203</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.564232</td>\n",
       "      <td>0.167591</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>20151220</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.606680</td>\n",
       "      <td>0.156771</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>20151220</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.606680</td>\n",
       "      <td>0.156771</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.534907</td>\n",
       "      <td>0.153544</td>\n",
       "      <td>43.386331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>20151219</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.534907</td>\n",
       "      <td>0.153544</td>\n",
       "      <td>43.386331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>20151202</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.617052</td>\n",
       "      <td>0.153522</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>20151202</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.617052</td>\n",
       "      <td>0.153522</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.559695</td>\n",
       "      <td>0.143960</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.559695</td>\n",
       "      <td>0.143960</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>20151223</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.607664</td>\n",
       "      <td>0.131207</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>20151223</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.607664</td>\n",
       "      <td>0.131207</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>20151217</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.618587</td>\n",
       "      <td>0.130363</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>20151217</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.618587</td>\n",
       "      <td>0.130363</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>20151207</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.605981</td>\n",
       "      <td>0.123932</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>20151207</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.605981</td>\n",
       "      <td>0.123932</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>20151219</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.519605</td>\n",
       "      <td>0.122619</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>20151219</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.519605</td>\n",
       "      <td>0.122619</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>20151220</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.554048</td>\n",
       "      <td>0.117153</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>20151220</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.554048</td>\n",
       "      <td>0.117153</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>20151224</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.637182</td>\n",
       "      <td>0.114816</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>20151224</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.637182</td>\n",
       "      <td>0.114816</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>20151216</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.629298</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>20151216</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.629298</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>20151208</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.634836</td>\n",
       "      <td>0.111460</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>20151208</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.634836</td>\n",
       "      <td>0.111460</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>20151203</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.523247</td>\n",
       "      <td>0.111406</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>20151203</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.523247</td>\n",
       "      <td>0.111406</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>20151231</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.567640</td>\n",
       "      <td>0.109173</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>20151231</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.567640</td>\n",
       "      <td>0.109173</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>20151225</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.635159</td>\n",
       "      <td>0.105358</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>20151225</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.635159</td>\n",
       "      <td>0.105358</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>20151218</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>0.102834</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>20151218</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.609465</td>\n",
       "      <td>0.102834</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>20151220</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.542080</td>\n",
       "      <td>0.094656</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>20151220</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.542080</td>\n",
       "      <td>0.094656</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>20151219</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>20151208</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>20151229</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>20151205</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>20151223</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>20151203</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>20151227</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>20151202</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>20151214</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>20151222</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>20151216</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>20151204</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>20151224</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>20151230</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.997144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>20151220</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.997144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>20151201</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.997144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>20151221</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.997144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>20151211</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.997144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>20151209</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>20151228</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>20151226</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20151208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20151229</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20151205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20151223</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20151203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20151227</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20151202</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20151214</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20151222</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>20151218</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20151224</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>20151230</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>20151220</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>20151201</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>20151221</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>20151211</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>20151212</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>20151231</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.497328</td>\n",
       "      <td>-0.002519</td>\n",
       "      <td>43.386331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>20151211</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.497328</td>\n",
       "      <td>-0.002519</td>\n",
       "      <td>43.386331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>20151213</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.492878</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>20151213</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.492878</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>20151213</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.489596</td>\n",
       "      <td>-0.005355</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>20151213</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.489596</td>\n",
       "      <td>-0.005355</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>20151213</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.490859</td>\n",
       "      <td>-0.005893</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>20151213</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.490859</td>\n",
       "      <td>-0.005893</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          key  no_of_features  hidden_layers  train_score  test_score  \\\n",
       "198  20151219               4              3     0.892065    0.642419   \n",
       "198  20151219               4              3     0.892065    0.642419   \n",
       "260  20151219              16              3     0.971171    0.580883   \n",
       "260  20151219              16              3     0.971171    0.580883   \n",
       "193  20151203               4              3     0.892065    0.666780   \n",
       "193  20151203               4              3     0.892065    0.666780   \n",
       "255  20151203              16              3     0.971171    0.568054   \n",
       "255  20151203              16              3     0.971171    0.568054   \n",
       "229  20151219               8              3     0.951516    0.551074   \n",
       "229  20151219               8              3     0.951516    0.551074   \n",
       "216  20151230               4              3     0.892065    0.641017   \n",
       "216  20151230               4              3     0.892065    0.641017   \n",
       "214  20151201               4              3     0.892065    0.674635   \n",
       "214  20151201               4              3     0.892065    0.674635   \n",
       "224  20151203               8              3     0.951516    0.564232   \n",
       "224  20151203               8              3     0.951516    0.564232   \n",
       "215  20151220               4              3     0.892065    0.606680   \n",
       "215  20151220               4              3     0.892065    0.606680   \n",
       "167  20151219               1              3     0.940550    0.534907   \n",
       "167  20151219               1              3     0.940550    0.534907   \n",
       "191  20151202               4              3     0.892065    0.617052   \n",
       "191  20151202               4              3     0.892065    0.617052   \n",
       "278  20151230              16              3     0.971171    0.559695   \n",
       "278  20151230              16              3     0.971171    0.559695   \n",
       "194  20151223               4              3     0.892065    0.607664   \n",
       "194  20151223               4              3     0.892065    0.607664   \n",
       "202  20151217               4              3     0.892065    0.618587   \n",
       "202  20151217               4              3     0.892065    0.618587   \n",
       "203  20151207               4              3     0.892065    0.605981   \n",
       "203  20151207               4              3     0.892065    0.605981   \n",
       "291  20151219              42              3     0.989413    0.519605   \n",
       "291  20151219              42              3     0.989413    0.519605   \n",
       "277  20151220              16              3     0.971171    0.554048   \n",
       "277  20151220              16              3     0.971171    0.554048   \n",
       "186  20151224               4              3     0.892065    0.637182   \n",
       "186  20151224               4              3     0.892065    0.637182   \n",
       "188  20151216               4              3     0.892065    0.629298   \n",
       "188  20151216               4              3     0.892065    0.629298   \n",
       "197  20151208               4              3     0.892065    0.634836   \n",
       "197  20151208               4              3     0.892065    0.634836   \n",
       "286  20151203              42              3     0.989413    0.523247   \n",
       "286  20151203              42              3     0.989413    0.523247   \n",
       "210  20151231               4              3     0.892065    0.567640   \n",
       "210  20151231               4              3     0.892065    0.567640   \n",
       "200  20151225               4              3     0.892065    0.635159   \n",
       "200  20151225               4              3     0.892065    0.635159   \n",
       "209  20151218               4              3     0.892065    0.609465   \n",
       "209  20151218               4              3     0.892065    0.609465   \n",
       "246  20151220               8              3     0.951516    0.542080   \n",
       "246  20151220               8              3     0.951516    0.542080   \n",
       "..        ...             ...            ...          ...         ...   \n",
       "136  20151219              42              1     0.004035    0.500000   \n",
       "135  20151208              42              1     0.004035    0.500000   \n",
       "134  20151229              42              1     0.004035    0.500000   \n",
       "133  20151205              42              1     0.004035    0.500000   \n",
       "132  20151223              42              1     0.004035    0.500000   \n",
       "131  20151203              42              1     0.004035    0.500000   \n",
       "130  20151227              42              1     0.004035    0.500000   \n",
       "129  20151202              42              1     0.004035    0.500000   \n",
       "128  20151214              42              1     0.004035    0.500000   \n",
       "127  20151222              42              1     0.004035    0.500000   \n",
       "126  20151216              42              1     0.004035    0.500000   \n",
       "125  20151204              42              1     0.004035    0.500000   \n",
       "124  20151224              42              1     0.004035    0.500000   \n",
       "123  20151230              16              1     0.003793    0.500000   \n",
       "122  20151220              16              1     0.003793    0.500000   \n",
       "121  20151201              16              1     0.003793    0.500000   \n",
       "120  20151221              16              1     0.003793    0.500000   \n",
       "119  20151211              16              1     0.003793    0.500000   \n",
       "144  20151209              42              1     0.004035    0.500000   \n",
       "145  20151228              42              1     0.004035    0.500000   \n",
       "146  20151226              42              1     0.004035    0.500000   \n",
       "1    20151204               1              1     0.004345    0.500000   \n",
       "11   20151208               1              1     0.004345    0.500000   \n",
       "10   20151229               1              1     0.004345    0.500000   \n",
       "9    20151205               1              1     0.004345    0.500000   \n",
       "8    20151223               1              1     0.004345    0.500000   \n",
       "7    20151203               1              1     0.004345    0.500000   \n",
       "6    20151227               1              1     0.004345    0.500000   \n",
       "5    20151202               1              1     0.004345    0.500000   \n",
       "4    20151214               1              1     0.004345    0.500000   \n",
       "3    20151222               1              1     0.004345    0.500000   \n",
       "2    20151216               1              1     0.004345    0.500000   \n",
       "1    20151204               1              1     0.004345    0.500000   \n",
       "147  20151218              42              1     0.004035    0.500000   \n",
       "0    20151224               1              1     0.004345    0.500000   \n",
       "154  20151230              42              1     0.004035    0.500000   \n",
       "153  20151220              42              1     0.004035    0.500000   \n",
       "152  20151201              42              1     0.004035    0.500000   \n",
       "151  20151221              42              1     0.004035    0.500000   \n",
       "150  20151211              42              1     0.004035    0.500000   \n",
       "149  20151212              42              1     0.004035    0.500000   \n",
       "148  20151231              42              1     0.004035    0.500000   \n",
       "181  20151211               1              3     0.940550    0.497328   \n",
       "181  20151211               1              3     0.940550    0.497328   \n",
       "236  20151213               8              3     0.951516    0.492878   \n",
       "236  20151213               8              3     0.951516    0.492878   \n",
       "205  20151213               4              3     0.892065    0.489596   \n",
       "205  20151213               4              3     0.892065    0.489596   \n",
       "267  20151213              16              3     0.971171    0.490859   \n",
       "267  20151213              16              3     0.971171    0.490859   \n",
       "\n",
       "     quality_score  time_taken  \n",
       "198       0.377946   45.412066  \n",
       "198       0.377946   45.412066  \n",
       "260       0.301438   46.347040  \n",
       "260       0.301438   46.347040  \n",
       "193       0.296640   45.412066  \n",
       "193       0.296640   45.412066  \n",
       "255       0.202495   46.347040  \n",
       "255       0.202495   46.347040  \n",
       "229       0.202147   47.684825  \n",
       "229       0.202147   47.684825  \n",
       "216       0.199713   45.412066  \n",
       "216       0.199713   45.412066  \n",
       "214       0.192117   45.412066  \n",
       "214       0.192117   45.412066  \n",
       "224       0.167591   47.684825  \n",
       "224       0.167591   47.684825  \n",
       "215       0.156771   45.412066  \n",
       "215       0.156771   45.412066  \n",
       "167       0.153544   43.386331  \n",
       "167       0.153544   43.386331  \n",
       "191       0.153522   45.412066  \n",
       "191       0.153522   45.412066  \n",
       "278       0.143960   46.347040  \n",
       "278       0.143960   46.347040  \n",
       "194       0.131207   45.412066  \n",
       "194       0.131207   45.412066  \n",
       "202       0.130363   45.412066  \n",
       "202       0.130363   45.412066  \n",
       "203       0.123932   45.412066  \n",
       "203       0.123932   45.412066  \n",
       "291       0.122619   53.214450  \n",
       "291       0.122619   53.214450  \n",
       "277       0.117153   46.347040  \n",
       "277       0.117153   46.347040  \n",
       "186       0.114816   45.412066  \n",
       "186       0.114816   45.412066  \n",
       "188       0.114700   45.412066  \n",
       "188       0.114700   45.412066  \n",
       "197       0.111460   45.412066  \n",
       "197       0.111460   45.412066  \n",
       "286       0.111406   53.214450  \n",
       "286       0.111406   53.214450  \n",
       "210       0.109173   45.412066  \n",
       "210       0.109173   45.412066  \n",
       "200       0.105358   45.412066  \n",
       "200       0.105358   45.412066  \n",
       "209       0.102834   45.412066  \n",
       "209       0.102834   45.412066  \n",
       "246       0.094656   47.684825  \n",
       "246       0.094656   47.684825  \n",
       "..             ...         ...  \n",
       "136       0.000000   37.039608  \n",
       "135       0.000000   37.039608  \n",
       "134       0.000000   37.039608  \n",
       "133       0.000000   37.039608  \n",
       "132       0.000000   37.039608  \n",
       "131       0.000000   37.039608  \n",
       "130       0.000000   37.039608  \n",
       "129       0.000000   37.039608  \n",
       "128       0.000000   37.039608  \n",
       "127       0.000000   37.039608  \n",
       "126       0.000000   37.039608  \n",
       "125       0.000000   37.039608  \n",
       "124       0.000000   37.039608  \n",
       "123       0.000000   33.997144  \n",
       "122       0.000000   33.997144  \n",
       "121       0.000000   33.997144  \n",
       "120       0.000000   33.997144  \n",
       "119       0.000000   33.997144  \n",
       "144       0.000000   37.039608  \n",
       "145       0.000000   37.039608  \n",
       "146       0.000000   37.039608  \n",
       "1         0.000000   44.018526  \n",
       "11        0.000000   44.018526  \n",
       "10        0.000000   44.018526  \n",
       "9         0.000000   44.018526  \n",
       "8         0.000000   44.018526  \n",
       "7         0.000000   44.018526  \n",
       "6         0.000000   44.018526  \n",
       "5         0.000000   44.018526  \n",
       "4         0.000000   44.018526  \n",
       "3         0.000000   44.018526  \n",
       "2         0.000000   44.018526  \n",
       "1         0.000000   44.018526  \n",
       "147       0.000000   37.039608  \n",
       "0         0.000000   44.018526  \n",
       "154       0.000000   37.039608  \n",
       "153       0.000000   37.039608  \n",
       "152       0.000000   37.039608  \n",
       "151       0.000000   37.039608  \n",
       "150       0.000000   37.039608  \n",
       "149       0.000000   37.039608  \n",
       "148       0.000000   37.039608  \n",
       "181      -0.002519   43.386331  \n",
       "181      -0.002519   43.386331  \n",
       "236      -0.004816   47.684825  \n",
       "236      -0.004816   47.684825  \n",
       "205      -0.005355   45.412066  \n",
       "205      -0.005355   45.412066  \n",
       "267      -0.005893   46.347040  \n",
       "267      -0.005893   46.347040  \n",
       "\n",
       "[620 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_scores.sort_values(by='quality_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:02:10.804242Z",
     "start_time": "2017-07-21T22:02:10.786575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>20151219</td>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.642419</td>\n",
       "      <td>0.377946</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>20151219</td>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.580883</td>\n",
       "      <td>0.301438</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>20151219</td>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.551074</td>\n",
       "      <td>0.202147</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>20151219</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.534907</td>\n",
       "      <td>0.153544</td>\n",
       "      <td>43.386331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>20151219</td>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.519605</td>\n",
       "      <td>0.122619</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>20151224</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>20151230</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>20151230</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.621339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>20151204</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.997144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>20151231</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key  train_score  test_score  \\\n",
       "no_of_features hidden_layers                                      \n",
       "4              3              20151219     0.892065    0.642419   \n",
       "16             3              20151219     0.971171    0.580883   \n",
       "8              3              20151219     0.951516    0.551074   \n",
       "1              3              20151219     0.940550    0.534907   \n",
       "42             3              20151219     0.989413    0.519605   \n",
       "1              1              20151224     0.004345    0.500000   \n",
       "4              1              20151230     0.002897    0.500000   \n",
       "8              1              20151230     0.003862    0.500000   \n",
       "16             1              20151204     0.003793    0.500000   \n",
       "42             1              20151231     0.004035    0.500000   \n",
       "\n",
       "                              quality_score  time_taken  \n",
       "no_of_features hidden_layers                             \n",
       "4              3                   0.377946   45.412066  \n",
       "16             3                   0.301438   46.347040  \n",
       "8              3                   0.202147   47.684825  \n",
       "1              3                   0.153544   43.386331  \n",
       "42             3                   0.122619   53.214450  \n",
       "1              1                   0.000000   44.018526  \n",
       "4              1                   0.000000   35.370012  \n",
       "8              1                   0.000000   34.621339  \n",
       "16             1                   0.000000   33.997144  \n",
       "42             1                   0.000000   37.039608  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg = past_scores.sort_values(by='quality_score', ascending=False).groupby(by=['no_of_features', 'hidden_layers'])\n",
    "psg.first().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:02:14.041117Z",
     "start_time": "2017-07-21T22:02:10.805654Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key_nof_hidden '20151201_16_1'\n",
    "Train.predictions = pd.read_pickle(\"dataset/tf_vae_dense_trained_together_nsl_kdd_predictions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:04:58.611342Z",
     "start_time": "2017-07-21T22:04:58.586169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = Train.predictions['20151219_4_3'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:04:59.141803Z",
     "start_time": "2017-07-21T22:04:59.136053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train.predictions['20151219_42_1'].loc[:,'Prediction']\n",
    "df.loc[:,'Prediction'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:04:59.478749Z",
     "start_time": "2017-07-21T22:04:59.441843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84316392907000925"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as me\n",
    "me.f1_score(df.loc[:,'Actual'].values.astype(int),\n",
    "            df.loc[:,'Prediction'].values.astype(int) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:04:59.833208Z",
     "start_time": "2017-07-21T22:04:59.822139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actual\n",
       "0.0     67713\n",
       "1.0    149915\n",
       "Name: Actual, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=\"Actual\").Actual.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:05:00.561625Z",
     "start_time": "2017-07-21T22:05:00.106447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGhCAYAAADiGPptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmcTfUfx/HXx9iXZE/2JWRn7PuWfd8iFRH1a08qpFRa\naJVKRSpUIlKyJiUiO9EqKkkSkTIYM3x/f9wz4xpzxxh3hnu9nz3Owz3f8z3nfM6l+cz3e77ne8w5\nh4iISDhJd74DEBERCTYlNxERCTtKbiIiEnaU3EREJOwouYmISNhRchMRkbCj5CYiImFHyU1ERMKO\nkpuIiISd9Oc7ABEROXsRlxRzLvZI0I7njuxd5JxrHbQDnmdKbiIiIcjFHiFT2Z5BO97RTS/nDdrB\nLgDqlhQRCUkGli54y5nOZvaGmf1lZt8ksu0eM3NmltevbJiZbTOzH82slV95pJlt8baNMzPzyjOZ\n2XSvfLWZFffbp6+Z/eQtfZPz7Si5iYhIcrwFnNZtaWZFgJbAb35l5YFeQAVvn/FmFuFtfgUYCFzh\nLXHHHAAccM6VBp4HxnjHyg2MBGoDtYCRZpbrTMEquYmIhCIDzIK3nIFzbhmwP5FNzwP3Af6vmOkE\nvOeci3bO/QJsA2qZWUHgEufcKud7Jc0UoLPfPpO9zzOB5l6rrhWw2Dm33zl3AFhMIkk2Id1zExEJ\nVcnoTkzV05t1AnY55762UxNkIWCV3/rvXlmM9zlhedw+OwGcc7FmdhDI41+eyD4BKbmJiAhAXjNb\n57c+wTk3IVBlM8sKDMfXJXnBUXITEQlVyehOPAv7nHM1zqJ+KaAEENdqKwxsMLNawC6giF/dwl7Z\nLu9zwnL89vndzNIDOYG/vfImCfZZeqbgdM9NRCQkpe1oyYScc1ucc/mdc8Wdc8XxdRdWd879CcwB\nenkjIEvgGziyxjm3G/jXzOp499OuBz7yDjkHiBsJ2R34zLsvtwhoaWa5vIEkLb2yJKnlJiIiZ2Rm\n0/C1oPKa2e/ASOfcpMTqOue+NbMZwHdALHCrc+64t/kWfCMvswALvAVgEjDVzLbhG7jSyzvWfjMb\nBaz16j3qnEtsYMup8foSo4iIhJJ02S5zmSpeF7TjHV3zzPqz7Ja8oKnlJiISiozzPlryQqZvRkRE\nwo5abiIiISl5D19frJTcRERClbolA9I3IyIiYUctNxGRUKVuyYCU3EREQpKpWzIJ+mZERCTsqOUm\nIhKK4l55I4lSy01ERMKOWm4iIqFK99wCUnITEQlJGlCSFH0zIiISdtRyExEJVek0oCQQJTcRkVCk\ntwIkSd+MiIiEHbXcRERClZ5zC0jJTUQkJGm0ZFL0zYiISNhRy01EJFSpWzIgJTcRkVClbsmA9M2I\niEjYUctNRCQUmalbMglquYmISNhRy01EJFTpnltASm4iIqFK3ZIBKe2LiEjYUctNRCQkaYaSpCi5\niYiEKnVLBqS0LyIiYUfJTcKGmWUxs4/N7KCZvX8Ox+ljZp8EM7bzxcwamtmP5zsOSQVx73ML1hJm\nwu+K5IJnZteY2TozO2Rmu81sgZk1CMKhuwMFgDzOuR4pPYhz7h3nXMsgxJOqzMyZWemk6jjnljvn\nyqZVTJKWTMktCeF3RXJBM7PBwFjgCXyJqCjwMtAxCIcvBmx1zsUG4Vghz8x0T10uWkpukmbMLCfw\nKHCrc+4D51yUcy7GOTfXOXefVyeTmY01sz+8ZayZZfK2NTGz383sHjP7y2v13eBtewR4CLjaaxEO\nMLOHzextv/MX91o76b31fmb2s5n9Z2a/mFkfv/Iv/farZ2Zrve7OtWZWz2/bUjMbZWYrvON8YmZ5\nA1x/XPz3+cXf2czamtlWM9tvZsP96tcys6/M7B+v7ktmltHbtsyr9rV3vVf7Hf9+M/sTeDOuzNun\nlHeO6t765Wa218yanNNfrJw/cVNwBWMJM0pukpbqApmB2UnUeQCoA1QFqgC1gBF+2y8DcgKFgAHA\ny2aWyzk3El9rcLpzLrtzblJSgZhZNmAc0MY5lwOoB2xKpF5uYJ5XNw/wHDDPzPL4VbsGuAHID2QE\nhiRx6svwfQeF8CXjicC1QCTQEHjQzEp4dY8DdwN58X13zYFbAJxzjbw6Vbzrne53/Nz4WrGD/E/s\nnNsO3A+8bWZZgTeByc65pUnEKxcydUsGFH5XJBeyPMC+M3Qb9gEedc795ZzbCzwCXOe3PcbbHuOc\nmw8cAlJ6T+kEUNHMsjjndjvnvk2kTjvgJ+fcVOdcrHNuGvAD0MGvzpvOua3OuSPADHyJOZAY4HHn\nXAzwHr7E9YJz7j/v/N/hS+o459Y751Z55/0VeA1onIxrGumci/biOYVzbiKwDVgNFMT3y4RI2FFy\nk7T0N5D3DPeCLgd2+K3v8Mrij5EgOR4Gsp9tIM65KOBq4GZgt5nNM7NyyYgnLqZCfut/nkU8fzvn\njnuf45LPHr/tR+L2N7MyZjbXzP40s3/xtUwT7fL0s9c5d/QMdSYCFYEXnXPRZ6grFzJ1Swak5CZp\n6SsgGuicRJ0/8HWpxSnqlaVEFJDVb/0y/43OuUXOuavwtWB+wPdD/0zxxMW0K4UxnY1X8MV1hXPu\nEmA4vgHgSXFJbTSz7PgG9EwCHva6XSUUmUZLJiX8rkguWM65g/juM73sDaTIamYZzKyNmT3lVZsG\njDCzfN7AjIeAtwMd8ww2AY3MrKg3mGVY3AYzK2Bmnbx7b9H4ujdPJHKM+UAZ7/GF9GZ2NVAemJvC\nmM5GDuBf4JDXqvxfgu17gJJnecwXgHXOuRvx3Ut89ZyjFLkAKblJmnLOPQsMxjdIZC+wE7gN+NCr\n8hiwDtgMbAE2eGUpOddiYLp3rPWcmpDSeXH8AezHdy8rYfLAOfc30B64B1+36n1Ae+fcvpTEdJaG\n4Bus8h++VuX0BNsfBiZ7oyl7nulgZtYJaM3J6xwMVI8bJSohSN2SAZlzSfZiiIjIBShdruIuU9MH\ng3a8o7NvXO+cqxG0A55neshTRCREWRi2uIJFyU1EJAQZSm5J0T03EREJO2q5iYiEIuPMD4ZcxC76\n5JY7T15XuEjCx5hEkhYde/zMlUQSsfXbr/c55/Kd+5FM3ZJJuOiTW+EixZi7ZMX5DkNCzPa9Uec7\nBAlRza7Mm3DGG0kFF31yExEJVWq5BabkJiISopTcAtNoSRERCTtquYmIhCi13AJTy01EJBRZkJcz\nnc7sDe8N8t/4lT1tZj+Y2WYzm21ml/ptG2Zm28zsRzNr5VceaWZbvG3jzMvQZpbJzKZ75avNrLjf\nPn3N7Cdv6Zucr0fJTUREkuMtfBNv+1sMVHTOVQa24r15w8zKA72ACt4+480swtvnFWAgcIW3xB1z\nAHDAOVcaeB4Y4x0rNzASqA3UAkaaWa4zBavkJiISgsx7zi1Yy5k455bhe4OGf9knfi8PXgUU9j53\nAt7z3gj/C763v9cys4LAJd4b5h0whZPvd+wETPY+zwSae626VsBi59x+59wBfAk1YZI9je65iYiE\nqCDfc8trZuv81ic45yacxf79OflapkL4kl2c372yGO9zwvK4fXYCOOdizewgkMe/PJF9AlJyExER\ngH0pfeWNmT0AxALvBDeklFO3pIhIiErLbskkYuiH74W+fdzJF4TuAor4VSvsle3iZNelf/kp+5hZ\neiAnvhcEBzpWkpTcREQkRcysNb6303d0zh322zQH6OWNgCyBb+DIGufcbuBfM6vj3U+7HvjIb5+4\nkZDdgc+8ZLkIaGlmubyBJC29siSpW1JEJESl5XNuZjYNaILv3tzv+EYwDgMyAYu9WFY55252zn1r\nZjOA7/B1V97qnIubbfwWfCMvswALvAVgEjDVzLbhG7jSC8A5t9/MRgFrvXqPOudOGdiSGCU3EZFQ\nlMavvHHO9U6keFIS9R8HHk+kfB1QMZHyo0CPAMd6A3gj2cGibkkREQlDarmJiIQoTb8VmJKbiEgI\nMr2sNEnqlhQRkbCjlpuISIhSyy0wJTcRkVCl3BaQuiVFRCTsqOUmIhKKTN2SSVFyExEJUUpugalb\nUkREwo5abiIiIUott8CU3EREQpAe4k6auiVFRCTsqOUmIhKq1HALSC03EREJO2q5iYiEIj3nliQl\nNxGREKXkFpi6JUVEJOyo5SYiEqLUcgtMyU1EJFQptwWkbkkREQk7armJiIQodUsGpuQmIhKCzDT9\nVlLULSkiImFHLTcRkRCllltgSm4iIiFKyS0wdUuKyGmOHz/OoK5NGX5z79O2zXjzZZpdmZeDB/4G\nIDYmhtFDb2VAx4b0a1eXdyeMBeDokcMMu6kXfdvW4Yb29Znw7KPxx/h67UoGdW1Ki4oF+GLRnLS5\nKLmoKLmJyGk+mPoaRUtecVr5X7t3sW7FUvIXLBxf9sWij4g5Fs2kOct5deYSPp4+mT93/QZAz/63\nMnn+KiZ88DnfblzD6mWfAlDg8sLc/+RLNG/XLW0uKFxZEJcwo+QmIqfY++cfrPpiMW27X3vatvGj\nR3DTkJGndoeZceTIYY7HxhJ99CgZMmQga7YcZM6SlWq1GwKQIWNGrihfmb1//gHAZYWKUqpsBdKl\n048gSR36lyUip3j5yQe4acjI0xLPiiXzyVugIKXKVTylvHHLjmTJkpXujSrQu3lVeva/lUsuzXVK\nnUP/HuSrzxdRvW6jVI//YhL3OEAwlnCj5CYi8b76fBGX5s5LmQpVTyk/euQw70wYS7/bh562zw9b\nNpAuIoL3v/iGdxavZ8ab4/lj56/x24/HxvLYkEF0uXYglxcpnspXcBExJbekaLSkiMT7ZuMaVn6+\nkNXLPuXYsWgOH/qPJ+67mV433sGfv//GwM6NAdi75w9u6taM8dM/YcncWdRs0Jz0GTKQK08+Klav\nzdZvNsUnsmdHDqZQsZJ073vzebwyudgouYlIvIGDH2Tg4AcB2LTmS2a88TLDn3oVgA9W/BBfr3fz\narw681Ny5spD/oKF2bh6OS079eTI4Si+/3od3a6/CYBJY58g6r9/GTJqbNpfTJgzIAwbXEGjbkkR\nOSedr+nPkcNR3NC+Prf0vIpWXXpTqmwF9v75B++89hw7tv/ITd2aMbBLE+a9PxXwdWX2bFKJLxbN\n4fmR93BD+/rn+SpCUfC6JMOxW9Kcc+c7hvOqctVIN3fJivMdhoSY7XujzncIEqKaXZl3vXOuxrke\nJ/NlZVyR68YFIyQAtj3TJihxXSjULSkiEqLCsMEVNEpuIiIhKhy7E4NF99zC1B+7dnJ1p1Y0r1eN\nFvWr88ZrL8Vve3zkMJrVqUKrRjUZdH1PDh78B4CYmBgG33ojLRvWoFndqrw89un4fT6e/T6tGtWk\nRf3qPPnIA/HlE8e/QPN61WjVqCa9u7Th95070u4iJV7v5tUY0LEhA7s0YWCXJnyzcU2S9dtGFjvn\nc44Zdhs9Glfk2LFoAA4e+Jvezaud83ET+vLT+fy67cf49TfHPcn6lV8E/TwSXpTcwlRERHpGPDqa\nJSs38uHCL5gy6TW2/vg9AA2bNOeTL9ezaNlaSpS6gvFeEpv30SyORUfzyfJ1zFuykncnv87O33Zw\nYP/fPPHwcN79YD6frtjA3r/28OWyzwGoUKkqcz9dwaJla2nboQtPPvxAwJgkdT03+UMmzl7KxNlL\nqVitVpqcMyIiggWz3knVc6xYMp8d208mtxvuGEZkvcapes6QYL5uyWAt4UbJLUwVuKwglar4fovO\nniMHpcuUY89u39RHjZq2IH16X490tRq12P3HLsDXxXH48GFiY2M5evQIGTJkJEeOHPz26y8UL1ma\nPHnzAdCgcTMWfPwhAPUaNiZL1qwnj7V7V5pepwR2JOoQ99zQhUFdmzKgY0NWLJl/Wp2///qTO69t\nz8AuTejfoQGb130FwNoVn3Nbr9YM6tqUh+/qz5GoQ4meo9v1NzFr8qscj409bdt7k17kfz1acGOn\nRrz14uj48qnjn+H6NrW5o087Rt0zkOlv+HoV5s6Y4qvfuTEj7+jH0SOH45+7e+3phxnYpQm7fvuF\nMcNu44tFc1izfAkP39U//rib1nwZP9FzcuMPZQakS2dBW8KN7rldBHb+toNvt2yiamTN07bNeGcK\n7Tt3B6Btx64sXjCXmhVKcOTIYR4a9RSX5sqNmfHztq3s/G0HBS8vxKL5c4g5FnPasaa/8xZNmrdK\n7cuRAAb37Uy6iAgyZMzI+OmfkDFTZh59cQrZsufg4IG/ubVXa+o1a3PKfZol82ZRs0Ezrr15MMeP\nHyf66GEOHvibt195lqffmEWWrNmYNnEc77/1Ctffeu9p58xfsDAVI+vwyZwZ1Gt68u9+7YrP2bXj\nZ8bPWIxzjhG39OHrtSvJlDkzyxbP5fUPvyA2NoabujajTIUqADS8qj3te14P+J6Pmz/rHbpeO5B6\nTVtTp0lLGrfqeMq5I+s25rmRgzlyOIosWbPx+fwPadq2y1nFL+FLyS3MRR06xM39evPQ40+TI8cl\np2x78bkxpE8fQZcevQDYtGEt6SIiWPPNzxz85wA92regQeNmFC1egsefHsdtN16LpUtHZM06/Pbr\nz6cc64MZ09iyaQPT5yxOs2uTUz03+UNy5soTv+6c4/XnH2PLuq+wdOnYt2c3B/b9Re58BeLrlK1Y\njadH3ElsbAwNmrel9JWV+GrNInZs38odfdoBEBtzjPJVTv/FKM41A+/kwduuo07jq+LL1q34nHUr\nljKoa1MAjhyOYteOnzkcdYj6zVqTMVNmMmbKTF2/hPjrT9/zxrgnOfTvQY4cjqJmg6ZJXm9E+vTU\nbNCMrz5fRONWHVm9bDE33TuSr9esPKv4Q1k4dicGi5JbGIuJieHmG3rTufvVtGnf+ZRt70+bypJP\n5jPtgwXxv8l/NGsGTZq3JEOGDOTNl5/I2nXZvGk9RYuXoEXrdrRo7fth8e7kSURERMQf68svPuOl\n58cwY84nZMqUKe0uUJL06dyZHNy/j1dnLiF9hgz0bl4tfvBHnCo16zF26hxWLV3MmOG306Pv/8ie\nMyeR9Rrz4LMTk3WewsVLUapcRZYu/OhkoXNcM+hOOlzd75S6Mye/GvA4Y4bfzqiXplCqXEUWzp7G\n12vO/Pxp07Zd+PCdSeS4NBdlKlQla7YcONxZxR/KNFoyMN1zC1POOe6782ZKlynLwFvuPGXb0iWf\n8OqLzzHp7Znx98sAChUuzMrlSwE4HBXFxnVrKHVFWQD27f0LgIP/HGDqmxPode0NAHyzeRPD7rmN\nSW/PJG++/GlwZZJcUf/9y6V58pE+QwY2rl7Onj92nlbnz107yZUnP+17Xk/b7tey9bvNlK9Sg283\nrmHXDl/r/MjhKHb+si3Jc/W5aTAz3ng5fr1Gg2Ys+ODd+Htde/fs5sDfe6lYvRYrP1/EseijHIk6\nxKqln8TvczjqELnzFSA2JoYlH8+ML8+SLTuHA9wzq1KzPj99t5l570+lWdsuACmKX8KPWm5hat3q\nlXww413Kla9Imya1Abj3gUdodlVrHhp6N8eio7m2e3sAqkXW4olnX+T6/jcz5I5BtKhfHeccPXpf\nx5UVKgHwyPAhfPftFgDuHDKMkqV9L7J84uHhHI6K4pYBfQC4vFARJr0zM2E4ch606NCdB/7XhwEd\nG1KmYtVEXz769doVTJ/0EukzZCBL1mwMHf0yl+bOy31PvMhjQwYRc+wYADfcOYwiJUoHPFeJK8px\nRfnK/PTdZgBq1m/Kb9u3clvvNgBkyZqNYU+9QrlK1anXrDU3dmpErrz5KVGmPNmy+7rLb7hjKLde\n3YqcufNwZeXI+MTYrG0Xnn3obma/PZGRY9845bwRERHUadKSRR++x9AnfQNTUhJ/SArTUY7Boum3\nNP2WpICm30q5I1GHyJItO0ePHOau6zow+JHn4geVXAyCNf1WlsvLuNIDXj5zxWT65rGWmn5LRCSl\nnh05mB3bt3Is+iitOve6qBJbMPneCqCmWyBKbiKSpkY8M+F8hxAmwnM2/2DRgBIREQk7armFgE4t\nG3Ls2DH+ObCfo0ePclnBywGYMGUGRYqe+xyBcX79eTstG9WgVOkyHDt2jLoNGjNqzPNn/dvhdT06\n8Mob7xIbG8PcD2dx7Q0DAd98l48/NIyXJ70dtJjlzG65uiUxx47x38EDRB89St4CBQEY9dIULitU\nNOjnmzT2CXLmyk33vjfzxH0306hlRxq0aHtKnSfuu5lvNqwhm/fsZdZs2Xnh7blBjyXcqeEWmJJb\nCPjok+WA79m0zZvWM2pM4m81Pn78+CnPn6VEqdJlWLB0NTExMVzdqSWfLpzHVW3an9Uxpr7/MeBL\nlu9Mfj0+uV1eqIgS23kwfrpvuP3C2dP48ZtN3PngmPMckc8tQx87Len5Ox4bS0T69AHXk7tfOEvL\nbkkzewNoD/zlnKvoleUGpgPFgV+Bns65A962YcAA4Dhwh3NukVceCbwFZAHmA3c655yZZQKmAJHA\n38DVzrlfvX36AiO8UB5zzk0+U7zqlgxhsbGxVCp5GY88MIRWjWqyacNaalcqFT/L/4Z1q7mmq++H\nR9ShQ9xz20A6XtWANk3r8OnCeUkeO0OGDFSvUZtff9nOiRMneHTEfVzVIJKWDWswb84HAPy5exfd\n2jWjTZPaXNUgknVrfPMSxsUwetQItm/bSpsmtRn96Ah+/Xl7/GMJ7ZvXZ/tPW+PP161dM77d8vVZ\nxykpN3fGZF556qH49Y+mvcGrT49k146fuaF9fUYNvpF+7eryyN0DiD56BPC9Qfuu6zpwU7dmDB10\nNfv3/RXUmCaNfYIn77+F269py5jhtzHv/ak8eNt1DO7bifsG9uDEiROMHz2C/h0aMKBjQ75YNAeA\n9Su/4O7rOzL85t4M6NQwqDFJvLeA1gnKhgJLnHNXAEu8dcysPNALqODtM97M4n7zfgUYCFzhLXHH\nHAAccM6VBp4HxnjHyg2MBGoDtYCRZpbrTMEquYW4f/89SK26DVi0bC2RNesErPfCM0/QuNlVzFn8\nJe/NXsBjDw3l6NGjAesfjopi5fIvKFe+IvM+msW2n35k4RdreHvmXEaNuI99e/9i9vvTaNGqLQuW\nrmbhF2u4snylU44x9MHH4luCQx967JRt7Tt3Y+5HswDY/cfv/HPgABUqVTnrOCXlmrbtypefzo+f\n9Hjh7Gm06ep7XnHH9h/pdv3NvDXvKzJmzMTH0ydz7Fg0Lz3xAI+Me4vXZn1Giw7deXPckyk+//jR\nI+Jf0fPk/bfEl//2y088++YHDH/KN5vJT99t4ZFxk3n2zdl8sfAjfvv5JyZ++AVPT5rJ+NEjOPD3\nXgB+/PZr7nzoad6a91WKYwopafxWAOfcMmB/guJOQFwrajLQ2a/8PedctHPuF2AbUMvMCgKXOOdW\nOd9zaFMS7BN3rJlAc/M1TVsBi51z+71W4WJOT7KnSbW2u5k54Dnn3D3e+hAgu3Pu4dQ6ZyIxvAXM\ndc6F7VPFGTNmpHW7Tmest3zpEpYu+YRXxj0LQHT0Uf74fWf8w9hx4lpaZulo1a4jDZs056Ghd9Op\na08iIiLIX+Ayataux+ZNG6hctQbD77mN6KPRtGzbgfIVKyc77vaduzGgT3fuHDKMj2fPpF3HrmcV\np5y7bNlzULlGXVYv/5SChYsTkS6CYqXKsGvHzxQsXIzyVX2PPLXo2IN5M6ZQtVZ9dmz7gSH9uwFw\n4vhx8l12eYrPH6hbsn6zNmTMlDl+vUb9JuTIeSkAWzasplnbrkRERJA7XwEqVq/Nj99sIkOGjFSo\nUoMClxdOcTyhJhUeBchrZuv81ic45840tLWAc2639/lPIG7i0kLAKr96v3tlMd7nhOVx++wEcM7F\nmtlBII9/eSL7BJSaHdPRQFcze9I5t+9sdzaz9M6509+jIafInDnLKf/A06dPjztxAoDooyfnEXTO\nMXHKDIqVKJnk8eJaWslRv1ETps9ZxGefLGTwrTdy021306VH72TtW7hIMbJmy8bWH79n7oczeeal\niWcVpwRHu+7X8v5br3BZoSK07ur3d5fwh6YZzjlKlq2Q6gM/MmfJmmA9W/L2y5r1zJUkKfvO5SFu\n777ZBTMrSGp2S8YCE4C7E24ws+Jm9pmZbTazJWZW1Ct/y8xeNbPVwFNm9rCZTTaz5Wa2w8y6mtlT\nZrbFzBaaWQZvv4fMbK2ZfWNmE+wifvijcJFibPl6IwAL5s6OL2/UtAVvvT4+fv2bzZuSfcxadeoz\nZ/b7nDhxgr1/7WHdmq+oXLU6v+/cQb78l3FN3wH06H0d3275+pT9smXPzqFD/wU8bofO3Rk/9mmO\nHTtGmbJXnnOccvYqVq/NHzt/5YtFc2jSpkt8+Z+/7+CHLRsA+GzuLCpVr02x0mXZt2c332/2lccc\nO8YvP/2QpvFWjqzDZ/Nnc+LECfbv+4tvN66hbMWqaRrDheQCeFnpHq+rEe/PuJuwu4AifvUKe2W7\nvM8Jy0/Zx8zSAznxDSwJdKwkpfY9t5eBPmaWM0H5i8Bk51xl4B1gnN+2wkA959xgb70U0AzoCLwN\nfO6cqwQcAdp5dV5yztX0RvBkwTeiJyAzG2Rm68xs3X6vvz5c3HXfCEbceycdWtQnQ4aMJ8vvfYDD\nUYdp2bAGLepXZ+xTjyf7mG07dqVU6TK0alSTPt3aMWLUGPLmy8+KZUtp3bgWbZrWYeHcj+g38H+n\n7JcvfwEqValGy4Y1GP3oiNOO265jVz6aNZ12nboFJU5JmUYtO1C5Rl2y+70SqWjJMrz/1iv0a1eX\no0eP0L7n9WTMmImRY9/glTEPcmOnRgzq2pQfNq9P8Xn977kN7NKE48ePnznWVh0pWrI0N3ZqxL39\nu/G/+0eRK0++FMcQ6swsaEsKzQH6ep/7Ah/5lfcys0xmVgLfwJE1Xhfmv2ZWx2uEXJ9gn7hjdQc+\n8+7LLQJamlkubyBJS68s6e8mteaWNLNDzrnsZvYovn7WI3j33MxsH1DQORfjtb52O+fyevfIPo8b\n5mlmDwMxzrnHzSydd4zMXvP3UWC/c26smXUD7gOyArmBF51zo5Nzz01zS0pKhNPckvcP7Mk1A++k\nSq36AOzhUoqTAAAgAElEQVTa8TMP39WfibOXnt/AwlSw5pbMVqisK3/La8EICYB1I5omGZeZTQOa\nAHmBPfhGMH4IzACKAjvwPQqw36v/ANAfXy/eXc65BV55DU4+CrAAuN37mZ4ZmApUwzdwpZdz7mdv\nn/7AcC+Ux51zb57petLiYZCxwAbgjMF4Ev7UiAZwzp0wsxh3MhufANJ7X8h4oIZzbqeXEDMjIkk6\neGA/t/VqRZmKVeMTm4SWtLwB45wLdEO9eYD6jwOndb0459YBFRMpPwr0CHCsN4A3EtsWSKonN+fc\nfjObge8ZhrjgVuJ7BmIq0AdYfg6niEtk+8wsO77mbNiOjhQJlpy5cjN10drTygsVK6lWWygwTZyc\nlLR6zu1ZfE3ZOLcDN5jZZuA64M5E90oG59w/wETgG3z9sKf/3yoiIheVVGu5Oeey+33eg+9+WNz6\nDnyDRBLu0y/B+sNJHPNhv88jODk1S8DjiYiEC99zbuc7iguXZigREZGwc3HMLioiEnb0PrekKLmJ\niIQo5bbA1C0pIiJhRy03EZEQpW7JwJTcRERC0bnNCRn21C0pIiJhRy03EZEQlArvcwsrSm4iIiFK\nyS0wdUuKiEjYUctNRCREqeEWmJKbiEiIUrdkYOqWFBGRsKOWm4hIKNJzbklSy01ERMKOWm4iIiHI\n9FaAJCm5iYiEKOW2wNQtKSIiYUctNxGREJVOTbeAlNxEREKUcltg6pYUEZGwo5abiEgIMtMMJUlR\nchMRCVHplNsCUrekiIiEHbXcRERClLolA1NyExEJUcptgalbUkREwo5abiIiIcjwzS8piVNyExEJ\nURotGZi6JUVEJOyo5SYiEopMr7xJilpuIiISdtRyExEJUWq4BabkJiISggy98iYp6pYUEZGwo5ab\niEiIUsMtMCU3EZEQpdGSgalbUkREwo5abiIiIcj3stLzHcWFS8lNRCREabRkYOqWFBGRsKOWm4hI\niFK7LbCAyc3MLklqR+fcv8EPR0REkkujJQNLquX2LeA49ZeDuHUHFE3FuERERFIsYHJzzhVJy0BE\nRCT5fNNvne8oLlzJGlBiZr3MbLj3ubCZRaZuWCIikiTvlTfBWsLNGZObmb0ENAWu84oOA6+mZlAi\nIiLnIjktt3rOuZuAowDOuf1AxlSNSkREzijuQe5gLGc+l91tZt+a2TdmNs3MMptZbjNbbGY/eX/m\n8qs/zMy2mdmPZtbKrzzSzLZ428aZ12w0s0xmNt0rX21mxc/lu0lOcosxs3T4BpFgZnmAE+dyUhER\nCR1mVgi4A6jhnKsIRAC9gKHAEufcFcASbx0zK+9trwC0BsabWYR3uFeAgcAV3tLaKx8AHHDOlQae\nB8acS8zJSW4vA7OAfGb2CPDluZ5URETOXRrfc0sPZDGz9EBW4A+gEzDZ2z4Z6Ox97gS855yLds79\nAmwDaplZQeAS59wq55wDpiTYJ+5YM4HmlszAAgWbJOfcFDNbD7Twino4575J6QlFROTcpcJoybxm\nts5vfYJzbgKAc26XmT0D/AYcAT5xzn1iZgWcc7u9+n8CBbzPhYBVfsf63SuL8T4nLI/bZ6d3vlgz\nOwjkAfal5GKSO0NJhBeUQ1N2iYiEo33OuRqJbfDupXUCSgD/AO+b2bX+dZxzzsxc6oeZPMkZLfkA\nMA24HCgMvGtmw1I7MBERSVoadku2AH5xzu11zsUAHwD1gD1eVyPen3959XcB/s9KF/bKdnmfE5af\nso/X9ZkT+DuFX02yWmHXAzWdcyOccw8AtYB+KT2hiIgEhwVxOYPfgDpmltW7D9Yc+B6YA/T16vQF\nPvI+zwF6eSMgS+AbOLLG68L818zqeMe5PsE+ccfqDnzm3ZdLkeR0S+5OUC+9VyYiIhcB59xqM5sJ\nbABigY3ABCA7MMPMBgA7gJ5e/W/NbAbwnVf/Vufcce9wtwBvAVmABd4CMAmYambbgP34RlumWFIT\nJz+P7x7bfuBbM1vkrbcE1p7LSUVE5NyYpe373JxzI4GRCYqj8bXiEqv/OPB4IuXrgIqJlB8Fepx7\npD5JtdziRkR+C8zzK1+VSF0REUljYThrVtAkNXHypLQMREREJFjOeM/NzErha1qWBzLHlTvnyqRi\nXCIicgbhOOFxsCRntORbwJv4BtS0AWYA01MxJhERSYa0nFsy1CQnuWV1zi0CcM5td86NwJfkRERE\nLkjJeRQg2ps4ebuZ3YzvQbscqRuWiIgkxbA0HS0ZapKT3O4GsuGbEfpxfE+N90/NoERERM5FciZO\nXu19/I+TLywVEZHzKUzvlQVLUg9xz8Z7h1tinHNdUyWiNJYhwsifM/OZK4r4KdtiyPkOQUSjJZOQ\nVMvtpTSLQkREJIiSeoh7SVoGIiIiZ0fvHwssue9zExGRC4ihbsmkKPGLiEjYSXbLzcwyOeeiUzMY\nERFJvnRquAWUnDdx1zKzLcBP3noVM3sx1SMTEZEkpbPgLeEmOd2S44D2eK/7ds59DTRNzaBERETO\nRXK6JdM553YkuHF5PFBlERFJfb4Jj8OwyRUkyUluO82sFuDMLAK4HdiaumGJiMiZhGN3YrAkp1vy\nf8BgoCiwB6jjlYmIiFyQkjO35F9ArzSIRUREzoJ6JQNLzpu4J5LIHJPOuUGpEpGIiJyRgV55k4Tk\n3HP71O9zZqALsDN1whERETl3yemWnO6/bmZTgS9TLSIREUkWTTEVWEq+mxJAgWAHIiIiEizJued2\ngJP33NIB+4GhqRmUiIicmW65BZZkcjPfE4JVgF1e0QnnXMAXmIqISNowMw0oSUKS3ZJeIpvvnDvu\nLUpsIiJywUvOPbdNZlYt1SMREZGz4puCKzhLuAnYLWlm6Z1zsUA1YK2ZbQei8D1e4Zxz1dMoRhER\nSYSm3wosqXtua4DqQMc0ikVERCQokkpuBuCc255GsYiISDJphpKkJZXc8pnZ4EAbnXPPpUI8IiKS\nTMptgSWV3CKA7HgtOBERkVCRVHLb7Zx7NM0iERGR5DMNKEnKGe+5iYjIhcn0YzqgpJ5za55mUYiI\niARRwJabc25/WgYiIiLJ5xsteb6juHAl531uIiJyAVJyC0yvAxIRkbCjlpuISIgyPegWkFpuIiIS\ndtRyExEJQRpQkjQlNxGRUBSmr6oJFnVLiohI2FHLTUQkROmtAIEpuYmIhCDdc0uauiVFRCTsqOUm\nIhKi1CsZmJKbiEhIMtLprQABqVtSRETCjpLbRWDrjz9SO7Jq/JI/9yW8+MJYADZ//TWNG9SlRtVK\ndOvcgX///ReAmJgYbryhLzWqVqJqpSt5esyT8ccb+eADlC5RhLyXZj8v1yMivgElZsFbzng+s0vN\nbKaZ/WBm35tZXTPLbWaLzewn789cfvWHmdk2M/vRzFr5lUea2RZv2zjz5hAzs0xmNt0rX21mxc/l\n+1FyuwiUKVuW1es3sXr9JlauWU/WrFnp2LkLAP+76UYee2I06zZtoWOnLjz/7NMAzJr5PtHHolm3\naQsrV6/n9YmvsePXXwFo264Dy1euOV+XIyIQ/ybuYC3J8AKw0DlXDqgCfA8MBZY4564AlnjrmFl5\noBdQAWgNjDezCO84rwADgSu8pbVXPgA44JwrDTwPjDmXr0fJ7SLz+WdLKFGyFMWKFQNg209badCw\nEQDNWlzFh7NnAb4JWQ9HRREbG8uRI0fImDEjOS65BIDadepQsGDB83MBIpLmzCwn0AiYBOCcO+ac\n+wfoBEz2qk0GOnufOwHvOeeinXO/ANuAWmZWELjEObfKOeeAKQn2iTvWTKC5ncPM0EpuF5n3p79H\nz6t7x69fWb4CH8/5CIAPZr7P7zt3AtC1W3eyZstGiSIFKVOyKHfdPYTcuXOfl5hFJHHpzIK2AHnN\nbJ3fMsjvVCWAvcCbZrbRzF43s2xAAefcbq/On0AB73MhYKff/r97ZYW8zwnLT9nHORcLHATypPi7\nSemOEnqOHTvGvLlz6Nq9R3zZaxPfYMKr46lXK5JDh/4jY8aMAKxds4aIdBH8/NsffP/TL7ww9ll+\n+fnn8xW6iKS+fc65Gn7LBL9t6YHqwCvOuWpAFF4XZByvJebSLtyk6VGAi8iihQuoWq06BQoUiC8r\nW64ccxd8AsBPW7eyYP48AGa89y4tW7UmQ4YM5M+fn7p167N+/TpKlCx5XmIXkVPFDShJI78Dvzvn\nVnvrM/Eltz1mVtA5t9vrcvzL274LKOK3f2GvbJf3OWG5/z6/m1l6ICfwd0oDVsvtIjJj+rRTuiQB\n/vrL92/xxIkTjH7iMQYOuhmAwkWLsvTzzwCIiopizZpVlC1bLm0DFpEkBblbMiDn3J/ATjMr6xU1\nB74D5gB9vbK+wEfe5zlAL28EZAl8A0fWeF2Y/5pZHe9+2vUJ9ok7VnfgM681mLLvJqU7SmiJiori\ns08X06lL11PKZ7w3jUrly1ClYjkKXn451/e7AYCb/3crh6IOUb1KBRrUrcl1fW+gUuXKAAwfeh+l\nihfm8OHDlCpemMcefTitL0dE0t7twDtmthmoCjwBjAauMrOfgBbeOs65b4EZ+BLgQuBW59xx7zi3\nAK/jG2SyHVjglU8C8pjZNmAwCbo9z5adQ2IMC5GRNdyK1evOdxgSYnLVvO18hyAh6uiml9c752qc\n63GKX1nZPTRlbjBCAmBArWJBietCoXtuIiIhyFDXW1L03YiISNhRcrtAlS1dnBpVK8VPmfXVypVJ\n1g/GVFgD+/ej3BUlqB1Zlbo1q7Pqq6/O+hhzP57D00+NBmDORx/y/XffxW979OGH+GzJp+ccpwTf\nqyP7sGPJk6x7f3ii2++8rhlHNr5EnkuzAZA7ZzYWTriDvSue5fn7e5xSt3vL6qyZPoz1Mx/gsTs6\nxZff2L0Ba2cMZ9V7Q1nyxt2UK3nZKfvlyJaZbQtHnXY8CcB8ky0Eawk36pa8gC389HPy5s2bpud8\nYvTTdO3WnU8Xf8Ltt9zE2o2bz2r/9h060r5DRwA+/uhD2rRrz5XlywPw0MOPBj1eCY6pH6/i1elf\n8Pqo60/bVrjApTSvcyW/7d4fX3Y0OoZHx8+lfOnLqVDq5Gw1uXNm44m7OlOvz1PsO3CIiY9eR5Na\nZVi6ZivTF6zj9ZlfAtCucSXGDO5Kp9vGx+878pZ2fLlheypeZfgJv5QUPGq5hZBDhw7RpmVz6tas\nTo2qleJnFvG3e/duWjRtRO3IqkRWrciXXy4H4NPFn9C4QV3q1qzONb16cOjQoSTP1aBhI7Zv3wbA\n15s20ah+HWpWq0zP7l04cOAAAC+/OI5qlctTs1plruvTC4Cpk9/irjtu46uVK5k3dw7Dh95L7ciq\n/Lx9OwP79+ODWTP5ZNFCrul18rfzZV8spWun9imKU4JjxYbt7D94ONFtTw3pxgMvfIj/4LPDR4+x\nctPPHI2OOaVuiUJ52PbbXvYd8P29fbb6Bzo3rwrAf1FH4+tly5IR5/e8b7Uri5A/zyV8+tX3Qbsm\nubgpuV3AWrdoSu3IqjSsVxuAzJkzM33mbL5au4GFn37O0PvuIeFo1+nvvctVLVuxev0m1qz/mipV\nqrJv3z5GP/EY8xd9yldrN1A9sgbjxj6X5Lnnzf2YChUrAXDjDdfz+JNjWLtxMxUrVuLxUY8A8MzT\no1m1diNrN27mxZdfPWX/uvXq0a59R54Y/TSr12+iZKlS8duaNW/B2jWriYqKAmDmjOn06NkrRXFK\n6mrfpBJ//PUPW7buOnNlYPvOvZQpnp+iBXMTEZGOjk2rULhA/ETx3NSzEd/OGcnjd3bmnqdmAr6u\ntdGDuzLsudmpcg3hyki759xCkbolL2AJuyWdczw0Yjgrli8jXbp0/LFrF3v27OGyy07eu6hRoyY3\nDexPTEwMHTp2pkrVqixf9gU/fP8dzRrVB+BYzDFq166b6DmHD72XMU88Rt58+Xh1wiQOHjzIPwf/\noWGjxgBce11f+nitrkqVKtPv+j507NiZDp06J3q8xKRPn56WLVszb+7HdO3WnQUL5vH46KfOKk5J\nfVkyZ+C+/q1of8tLyd7nn/+OcMcT03l7TH9OOMeqr3+mZOGT/4Zfm7GM12Ys4+rWNRh6Y2sGPjSV\nm3o2ZNGX37Lrr39S4zLCWvilpOBRcgsh7737Dvv27WXlmvVkyJCBsqWLE3306Cl1GjRsxOLPlrFw\n/jwGDejHHXcN5tJcuWjW4iqmvD3tjOeIu+cW5+DBgwHrzp4zjy+XL2Pe3I8ZM/px1m3ckuxr6XF1\nL14Z/xK5c+ememQNcuTIgXMu2XFK6itZOB/FCuVhzfRhABTKfylfvXs/Da97mj1//xdwv/nLvmH+\nsm8A6N+1PsePnzitzoxF63lh+NUA1K5cgvrVSjGoZ0OyZclExgwRHDoSzYPj5qTCVcnFQt2SIeTg\nwYPky5efDBky8MXSz/ltx47T6uzYsYMCBQrQ/8aB9Ot/Ixs3bqBW7Tp8tXIF27f57qFFRUXx09at\nyTpnzpw5yXVprvh7d+++M5UGjRpz4sQJft+5k8ZNmvL4k2M4ePDgaffHsufIwaH/Ev8h2LBRYzZt\n3MAbkybSo6fvft25xCnB9+22PyjWfBjl2o2kXLuR7PrrH+peMybJxAaQL5dv5O6lObIwqGdD3pzt\nG3Vbqmi++DptGlZg2869ANzwwGTKtH2Icu1GMuz52bw7d40SWzKl5ctKQ41abiGk1zV96Na5AzWq\nVqJ6ZA3Kljt9rsflXyzl+eeeJkP6DGTLnp1Jb04hX758TJz0Ftdf25tj0dEAjHz0Ma4oUyZZ5534\nxmRuv/Vmjhw+TPGSJZnw+pscP36cG/pey78HD+Jw3HLbHVx66aWn7NejZy9u/d9Axr80jnenzzxl\nW0REBG3atuftKW/x+hu+Vzida5yScpOf7EfDyCvIe2l2ti0cxahX5zP5w6QfBflh3iPkyJaZjBnS\n06FpZdrf8jI//Pwnz9zXnUplfG8xeXLCQrb95pu/9H9XN6Jp7XLExB7nn38PM/DBKal+XeEtPIfw\nB4um39L0W5ICmn5LUipY02+VLF/FPf7O/GCEBMA11Qtr+i0RETm/NP1W0vTdiIhI2FHLTUQkROme\nW2BquYWYhvVqUzuyKleULEqRgvni557c8euvqXK+hx8awYsvjE20vGSxQvHnrx1Zlf8CjIyUtLNs\nyhBWvTeUrfMf5bfPnmTVe0NZ9d5QihbMHdTzlCySlyMbX2JgjwbxZeMe6EWvtjWDep5cl2Tlxu4n\nz1G4wKVMHX1DUM8RyiyIS7hRyy3ELF/pe8v71MlvsX79OsaOS/4DtsF29+B7uf3OuwJuj42NJX36\n9AHXA3HO4ZwjXTr97nW2Gl3/DADXdqhNZPmi3D3m/UTrpUtnnDhxboPJ/tz3L7f3acYbH6xM9Fm2\nYMiV05fc4uak/H3PP1w39M1UOZeEF/30CBOTJk5g6H1D4tcnvPoKw+6/l+3btlG9SgWu69OLqpWu\npE/vnhw5cgSAdWvXclWzxtSrFUmn9m3Ys2fPOcfx5qTX6dGtM61aNKVD21Z8tuRTWjZvQtdO7alR\nzTed17PPPEVk1YpEVq3I+JdeBGD7tm1Uq1yeftf1oXqVCuzevfucY5GTIiLSsXvZUzw9pBtrpg+j\nZsXibFs4ipzZswBQq1Jx5r3qGwGaLUtGJjxyLcunDuGraffTtlHFRI+55+9/WbFxG9e0q3XatlJF\n8zHn5VtZ8c59LJ50F6WL5o8vXzZlCGtnDOfhWzuwe9lTgO+NAAteu52V797PmunDaNPQd87H7uhE\nmWL5WfXeUEbd0ZGSRfKy6j3fC5q/fOc+riiWP/6cS964m8plCiU7/pCntwIkScktTPS4uhdzPppN\nbGwsAFMmv0nffv0B+P6777jt9rvYtOV7MmfKzOsTXiM6Opohg+9k2oxZrFyznl7XXMujIx88q3M+\n/9zT8V2SbVu1iC//etNG3nv/AxZ8sgSADevXMfbF8Wza8j1rVq9m+rvv8OVXa1m6/CsmvDaeb7b4\nZjb58YcfuP3Ou9m4+TsKFSoUjK9F/FyaIytfbthGraufZPXmXwLWGz6oDYtXfk/D656hzaBxjB7c\nlUwZE29xP/PmYu7u2/y0H44vj+jNnU9Op36fp3ho3ByeH+qbsu25+3owdsoSavZ8gj/3nZz95kj0\nMXoOnki9a8bQ7uYXeWpIVwBGjPuIrTv+ok6v0ac92D1r0Xq6tawO+GZPyZUzK5u37jqr+ENZ3GjJ\nYC3hJs3/xs2sMzAbuNI594OZFQfqOefe9bZXBS53zqXoAQ4z+xWo4ZzbF5yIQ8Mll1xCgwaNWLRw\nASVKlCQiIoJyV17J9m3bKF6iBLXr1AGgd59rmfT6BBo1bsL3331LOy8pHT9+nEKFC5/VOQN1S7Zo\n0ZJcuU5Ollu7Tl2KFi0KwMqVX9K5azeyZPG1GDp07MyKL5fT4qqWlCxVisgaYfOYzQUn+lgMH332\n9RnrNa97JS3rV+CeG64CIHPG9BS5LHf8w9j+tv+2l80/7qJHq+rxZTmzZ6FWpeJMe+bG+LL0Eb4f\nnzUrFafz7a8AMH3BOkbe6nsbhGGMuqMj9aqW4oRzFC6QK/7dcYHMWryBmWNvZvTEhXRvVZ0PFm88\n6/glfJ2PX2d6A196f44EigPXAO9626sCNYDgPZ14kejX/0bGvfAcxYoV5/q+J2+6J/yt2sxwzlGx\nUmWWLF0e9DiyZsuW5Hog2bImr56kzJEEr6eJPX6CdOl8/zYyZcwQX24GPQdP4Jffk/f74ZjXF/LW\nk/1Ys/nX+P3//ieKOr1GJzu2Ph1qkTN7FupeM4bjx0+wbeEoMvvFlJjfdh8g6kg05UpeRveW1Rk4\n8u0UxR/KwrE7MVjStDVqZtmBBsAAoJdXPBpoaGabzOx+4FHgam/9ajOrZWZfmdlGM1tpZmW9Y0WY\n2TNm9o2ZbTaz2xOcK4uZLTCzgWl4iedVvfr1+WX7dj6Y9T7de14dX/7rL7+wbu1aAKZPe5d69Rpw\nZfny/PHHLtauWQPAsWPH+O7bb1M9xvr1GzLnw9kcOXKEQ4cOMffjj6jfoGGqn1dOt+OP/VS70tei\n7tKianz5pyu/55ZejePXq5RNukX//c9/8svOfbSq73sp7T//HeHPfQfp2LQy4PsBHDcd17pvdtCp\nWRUAerSKjD9GzuxZ2Lv/P44fP0Gz2uUo5L0m51BUNDmyZgp47pmLNnDvDS3JmDE9P/z8Z4riD2Ua\nLRlYWne1dgIWOue2An+bWSQwFFjunKvqnBsDPARM99anAz8ADZ1z1bxtT3jHGoSv1VfVOVcZeMfv\nPNmBj4FpzrmJCYMws0Fmts7M1u3dtzd1rvQ86dKtOw0aNCJnzpzxZeWuvJJxLzxH1UpXcvjIYQYM\nHESmTJl4972Z3H/vYGpWq0ydmtVYu2b1WZ3L/55b7ciq7Ny584z71KxVix69etOgbk0aN6jDwEH/\no2KlSmd9nXLuHnt1Pi8M78mXb9/LsZjY+PLHX1tA1iwZWTtjOOtnPsADN7c947FGv76QIn6PG1w3\n9E1u7N6Q1dOHsmHmA/EDRO556n3u6deCNdOHUbxQHv495Hurxbtz11CnSknWzhhOj9bV+WmHrwvx\nr/3/sfH7naydMZxRd3Q87bwffLqRq9vUYNYnG88pfgk/aTq3pJnNBV5wzi02szuAosBcYIhzrr1X\npx++e2a3eetFgHHAFYADMjjnypnZLOBV59ziBOf4FTgIPOWc8094iQq3uSU7tmvNvfcPi3//2vZt\n27jm6u6sXr/pPEcWXjS3ZMpkzZyRw0ePAdCrbU06NatC7yGvn+eo0law5pYsXaGKe/a9RcEICYDO\nlQtqbsmUMLPcQDOgkpk5IAJfspp3hl1HAZ8757p4g0+WJuN0K4DWZvauu0hmhv77779p3KAO1SNr\nxCc2kQtNZIViPH1vN9KZ8c9/hxnk3SeTs+cbLRmOHYrBkZYDSroDU51zN8UVmNkXwAkgh1+9/xKs\n5wTi3nHfz698MXCTmX3unIs1s9zOuf3etoe85WXglqBexQUqT548fPP9T6eVlypdWq02uWAsX//T\nWQ00EUmptLzn1hvfIwD+ZuEbWHLczL42s7uBz4HycQNKgKeAJ81sI6cm49eB34DNZvY1vhGX/u4E\nspjZU6lwLSIi551eVhpYmrXcnHNNEykbF6B6wgnq/N9WOcLbNxYY7C3+xyzut6pJ6EQkTBmmbsmA\nwvHBdBERuciF35w0IiIXiXDsTgwWtdxERCTsqOUmIhKC9ChA0pTcRERCUZiOcgwWdUuKiEjYUctN\nRCREqeUWmJKbiEiI0nNugalbUkREwo5abiIiIciAdGq4BaTkJiISotQtGZi6JUVEJOyo5SYiEqI0\nWjIwJTcRkRClbsnA1C0pIiJhRy03EZEQpNGSSVPLTUREwo5abiIiIUlv4k6KkpuISCjSWwGSpG5J\nEREJO2q5iYiEKDXcAlNyExEJQb7RkkpvgahbUkREwo6Sm4hIiLIgLsk6n1mEmW00s7neem4zW2xm\nP3l/5vKrO8zMtpnZj2bWyq880sy2eNvGmfman2aWycyme+Wrzax4ir8YlNxEREJXWmc3uBP43m99\nKLDEOXcFsMRbx8zKA72ACkBrYLyZRXj7vAIMBK7wltZe+QDggHOuNPA8MCbZUSVCyU1ERM7IzAoD\n7YDX/Yo7AZO9z5OBzn7l7znnop1zvwDbgFpmVhC4xDm3yjnngCkJ9ok71kygeVyrLiWU3EREQpQF\n8b9kGAvcB5zwKyvgnNvtff4TKOB9LgTs9Kv3u1dWyPv8//buPNrOqj7j+PcBg8yBggIFNIwqKlMC\nBFBXKshgQZAFNEEEJAtkkApaJrUuXUqFuqrVhUBBWnAoggUl1FKKWEWQMKXMs2VuGIIYBFFIePrH\n3nd5csm53CSHnPu+5/lknXVP3nHfu846v/e339+79/DlC+xjex4wF1hj1H+MYRLcIiIaSurdC1hT\n0sgyKtcAAA5hSURBVE0dr8P/dB7tATxl++ZubamZmF/3X3qU8ihAREQAzLE9qcu6HYEPSfogsDyw\nqqTvAU9KWsf27Nrl+FTd/nFg/Y7916vLHq/vhy/v3OcxSW8AxgPPLO4vk8wtIqKhllY9ie2Tba9n\newKlUORntg8EZgAH180OBi6t72cAU2sF5AaUwpEbahfmc5Im1/tpBw3bZ+hY+9ZzLHYmmMwtIqKp\n+v8M96nARZKmAw8D+wPYvlPSRcBdwDzgaNvz6z5HAecBKwCX1xfAucB3JT0A/IYSRBdbgltERIya\n7Z8DP6/vnwF26rLdKcApC1l+E/CuhSz/A7Bfr9qZ4BYR0UClO7H/qdtYlXtuERHROsncIiKaKPO5\njSjBLSKioRLbuku3ZEREtE4yt4iIpkrq1lWCW0REI416TMiBlG7JiIhonWRuERENlWrJ7hLcIiIa\naNHmGB086ZaMiIjWSeYWEdFUSd26SnCLiGioVEt2l27JiIhonWRuERENlWrJ7hLcIiIaKrGtu3RL\nRkRE6yRzi4hoojzoNqJkbhER0TrJ3CIiGiqPAnSX4BYR0UAi1ZIjSbdkRES0TjK3iIiGSuLWXYJb\nRERTJbp1lW7JiIhonWRuERENlWrJ7hLcIiIaKtWS3aVbMiIiWieZW0REQyVx6y7BLSKiqRLdukq3\nZEREtE4yt4iIBiqTAiR16yaZW0REtM7AZ26zZt08Z4Vxerjf7RjD1gTm9LsR0Tj53HT31p4cRXkU\nYCQDH9xsv6nfbRjLJN1ke1K/2xHNks/N0pHY1l26JSMionUGPnOLiGispG5dJbjFazm73w2IRsrn\n5nWnVEuOIN2SMSLb+ZKKRZbPTfRbMreIiIZKtWR3CW4REQ0kcsttJOmWjIiI1klwi8Um6R2S3i9p\nXL/bEmOflE60nlMPXy2TbslYElOB9YH5kn5l++V+NyjGLtsGkDQZeMj2E31uUuOlWrK7ZG6xJL4I\nPAT8FfCeZHCxMJK2krRcfb8RcAowr7+tirZLcItF0tm1ZPsVyhfVbBLgorsvAJfVAPcgMBd4CUDS\nMpKW7WPbGk3q3attEtxi1CSpo2tpF0lTgNWALwOPUALcDglwASVwAdjeC3gWuAhYmZLtr1jXvQIs\n16cmNl5uuXWXe24xah2B7VPAh4G7gMOAb9v+O0knAocD84Fr+tbQ6Lt6IfRKff8m21MlXQpcR/l8\nrCNpPjAOmC3pZNsv9rHJ0TIJbrFIJO0M/IXt90r6CrAtME0Stk+TdBzwQH9bGf3WcSH018AkSUfa\n3kvSWcBOwN8Dy1Iy/3sT2BZDS7sTeyXBLUbU2RVZPQocI+kQYBvgg8DXgS9IGmf7631oZoxBkj4M\nHAzsYfsFANtHSPoh8CVgb9spLInXRe65RVfD7rFtJ2l14EHbDwGbAGfang3cBtwK3NK3xsZYtCEw\nw/ZsSeOG7sXa3g94EvjzvrauFZbOXTdJ60v6b0l3SbpT0ifr8j+TdKWk++vP1Tv2OVnSA5LulbRr\nx/KJkm6v6745VKQm6Y2SLqzLr5c0YUn+Mglu0VVHYDsC+FfgfOBwSWsCdwDnSzoF+CjwDdtP9a2x\n0VddHtB+HHizpFVtv2z7ZUn7S9rB9nTbjyztdraJWKrVkvOAT9veDJgMHC1pM+Ak4CrbmwBX1f9T\n100F3gnsBpzRURV7JuVe/Sb1tVtdPh141vbGlN6g05bk75PgFq/S+UUl6c3A5pR7a98CJlA+hDOA\n/Skf+n1s/3rptzTGgmEZ/j6SPiBpe+C/gLcAh0raXdI04POUR0eiQWzPtj2rvv8dcDewLrAX5aKX\n+nPv+n4v4Ae2/2j7Qcp9+G0lrQOsantm/cx8Z9g+Q8f6N2CnJRnVJvfcYgHDvqg+AawNvNP2M8AV\ntbx7Z+AESrb2H/1rbYwFw4pHDqDM5XYCcBSlevYTlIuj5YFp9csueqAf9SS1u3Ar4HpgrXprAuAJ\nYK36fl1gZsduj9VlL9f3w5cP7fMogO15kuYCawBzFqedydxiAZ1X4JRigBuA9SRdWNdfDlxNKeFO\nrVYAZRQSypX3FGA94Cng28B2tj9r+wDgINu396+V7dPjbsk1Jd3U8Tr81efTysDFwLG2n+tcV787\nPHyffklwC+BVXZETgY8AZ9ueAWwMbCrpAgDblwKn1GwuBpCk1epQWkjaHHgRmEYJcB+w/T7gHOBC\nSQcC2H6+X+2NUZlje1LHa4EJZ2tB0MXA921fUhc/WbsaqT+H7rs/Thl3dsh6ddnj9f3w5QvsI+kN\nwHhgsb9jEtxieFfkvpTy/meBKZK2qOu2BiZLOg9gqLQ7Bk/94tkUOFjSOZQxRh+pBUWrU4qPAH4D\nfI0Fu6eih9TDfyOep1z8ngvcbftrHatmUHp4qD8v7Vg+tVZAbkApHLmhdmE+J2lyPeZBw/YZOta+\nwM+GPYa0SHLPLTq7Inej3CfZFXgHcCDwIUmv1O6kDeoHNQZUvRCaJ+le4DPA9sAJtn9fN3kDsKuk\ntwF7AlNsP9qn5rbf0rsxsCOlKvp2SUOP/HwGOBW4SNJ04GFKkRm275R0EWUUo3nA0bbn1/2OAs4D\nVgAury8owfO7kh6gXBhNXZIGawkCY7SIyjiRxwH32D6xLtuR8gVl4Hu27+xfC6Pf6jNMk21fLmkb\nStf1byg3/X9q+7K63cfqLjfkM/P62WKrib7iF71LitcZv9zNtif17IB9lsxtQC1k5JEHKSXaG9au\nyFttX1v72d9Peeg2Bts4YEdJnwewvX195vEAYE9Jv6UMqfUScMHQ2JLx+klFV3cJbgNo2D22PSnd\nBr8FjgG+Aew31BVp++eSrs/Yf4NL0tq2n7D9lKQngc2ASwBsz5F0GeUzdCKwBbBTAtvrr61T1fRK\nCkoGmKSjKMUA7wH+mdIteRxlMNtD6igDJLANLklvB/5P0j9KOgA4i1IR+bSkM+qF0oPAlcChlG7L\n+/rY5AggwW2gSHqLpJVsu448sj/wEdufBXYAjgD2o0xAuix/KuuNwfU88CtKl/V0ytBJ44ErgOeA\n0yV9lHJR9Jztx7sdKHpvaVVLNlGC24CQtBbwaeBISSvXsu051BmRbT8LHAu8u5brHm97sUYGiPaw\n/RjlQf6tKVW0V1Gq5r4EXEYpJjkEON32H/rUzMGV2Uq7SnAbHE8DN1JGYv9YfcbkAeAH9bklgLdS\nRiNZlnIPJQZYx4P9J1EqZtekZHATgdsp92gfAw62fVdfGhnRRQpKWk7SJsAytu+V9H1gLrA7cJjt\nkySdCVwt6TZgO0o35fwRDhkDonZfDwW4+4F/oAS242z/uN6Pe7Jm/dEHLUy4eibBrcUkrQHcC8yR\n9EVgPmVQ2/HAxpI+bvtISdtRBrU9LYPaRqdaVfuSpO8BvwC+ZfvHdd09fW1cxAgS3FrM9jOSdgZ+\nSumC3gK4kFIk8BLw7npl/i+2/9i/lsZYVzP/k4AJklbsGJEk+iiPAnSX4NZytn+mMgvuNynBbS3K\nQ9lTKdOQvA24AEhwi9cyE9in342IIe2scuyVBLcBYPtKSX9DmT17su3zJc2gjDixou25/W1hNIHt\neyRNTdYWTZDgNiBs/0TSK8BMSdtnuppYHAlsY4dIt+RIEtwGSB3wdjngp5ImZoikiGirPOc2YOpE\no+9NYIuINkvmNoAyI3JEO6RbsrsEt4iIhkq1ZHfployIiNZJ5hYR0USZz21ECW4REQ3U0sH8eybd\nktEIkuZLukXSHZJ+KGnFJTjWFEn/Xt9/qA4r1W3b1eqkrot6ji/UB+dHtXzYNudJ2ncRzjVB0h2L\n2saINktwi6Z40faWtt9FGRfziM6VKhb582x7hu1TR9hkNWCRg1vEUpH53LpKcIsm+iVlVoMJku6V\n9B3K0GLrS9pF0nWSZtUMb2UASbtJukfSLDrGR5R0iKTT6/u1JP1I0q31tQNwKrBRzRq/Wrc7XtKN\nkm6rsy0MHeuzku6TdA1lzM4RSTqsHudWSRcPy0Z3lnRTPd4edftlJX2149wfX9I/ZERbJbhFo9SJ\nVXenTJYJsAlwhu13Ai8AnwN2tr01cBPwKUnLA+cAe1LmI1u7y+G/CfzC9haUmafvpEzU+euaNR4v\naZd6zm2BLYGJkt4naSJlMOotgQ8C24zi17nE9jb1fHcD0zvWTajn+EvgrPo7TAfm2t6mHv8wSRuM\n4jzRUurhv7ZJQUk0xQqSbqnvfwmcS5lV/GHbM+vyycBmwLV1js3lgOuAtwMP2r4foM5NdvhCzvF+\n4CCAOmHrXEmrD9tml/r6n/r/lSnBbhXgR0NjL9aBqV/LuyR9mdL1uTJwRce6i+ooMvdL+t/6O+wC\nbN5xP258Pfd9ozhXtFCqJbtLcIumeNH2lp0LagB7oXMRcKXtacO2W2C/JSTgK7b/adg5jl2MY50H\n7G37VkmHAFM61nnYtq7nPsZ2ZxBE0oTFOHdEq6VbMtpkJrCjpI0BJK0kaVPgHsokmxvV7aZ12f8q\n4Mi677KSxgO/o2RlQ64ADu24l7eupDcDVwN7S1pB0iqULtDXsgowW9I44CPD1u0naZna5g0pM6pf\nARxZt0fSppJWGsV5oqVST9JdMrdoDdtP1wzoAklvrIs/Z/s+SYcDP5H0e0q35ioLOcQngbMlTQfm\nA0favk7StbXU/vJ63+0dwHU1c3weOND2LEkXArcCTwE3jqLJfwtcDzxdf3a26RHgBmBV4Ajbf5D0\nbcq9uFl1BvWngb1H99eJVmpjVOoR2cN7PyIiYqzbeuIkXzNzNNdQo7PScsvcbHtSzw7YZ8ncIiIa\nqo1Vjr2S4BYR0UCZiXtk6ZaMiGggSf8JrNnDQ86xvVsPj9dXCW4REdE6eRQgIiJaJ8EtIiJaJ8Et\nIiJaJ8EtIiJaJ8EtIiJaJ8EtIiJaJ8EtIiJaJ8EtIiJaJ8EtIiJa5/8BVTwIjz6jJNUAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbeb67639b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(actual_value = df.loc[:,'Actual'].values.astype(int),\n",
    "     pred_value = df.loc[:,'Prediction'].values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:05:00.577732Z",
     "start_time": "2017-07-21T22:05:00.563298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <td>0.892065</td>\n",
       "      <td>0.590326</td>\n",
       "      <td>0.101743</td>\n",
       "      <td>45.412066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>3</th>\n",
       "      <td>0.971171</td>\n",
       "      <td>0.541010</td>\n",
       "      <td>0.072127</td>\n",
       "      <td>46.347040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <td>0.951516</td>\n",
       "      <td>0.535129</td>\n",
       "      <td>0.056104</td>\n",
       "      <td>47.684825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>3</th>\n",
       "      <td>0.989413</td>\n",
       "      <td>0.510446</td>\n",
       "      <td>0.030122</td>\n",
       "      <td>53.214450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>3</th>\n",
       "      <td>0.940550</td>\n",
       "      <td>0.514903</td>\n",
       "      <td>0.027124</td>\n",
       "      <td>43.386331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.018526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.370012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.621339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.997144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.039608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                           \n",
       "4              3                 0.892065    0.590326       0.101743   \n",
       "16             3                 0.971171    0.541010       0.072127   \n",
       "8              3                 0.951516    0.535129       0.056104   \n",
       "42             3                 0.989413    0.510446       0.030122   \n",
       "1              3                 0.940550    0.514903       0.027124   \n",
       "               1                 0.004345    0.500000       0.000000   \n",
       "4              1                 0.002897    0.500000       0.000000   \n",
       "8              1                 0.003862    0.500000       0.000000   \n",
       "16             1                 0.003793    0.500000       0.000000   \n",
       "42             1                 0.004035    0.500000       0.000000   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "4              3               45.412066  \n",
       "16             3               46.347040  \n",
       "8              3               47.684825  \n",
       "42             3               53.214450  \n",
       "1              3               43.386331  \n",
       "               1               44.018526  \n",
       "4              1               35.370012  \n",
       "8              1               34.621339  \n",
       "16             1               33.997144  \n",
       "42             1               37.039608  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.mean().sort_values(by='quality_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:05:01.304455Z",
     "start_time": "2017-07-21T22:05:01.290453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>quality_score</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_of_features</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010462</td>\n",
       "      <td>0.030403</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044799</td>\n",
       "      <td>0.081343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.042807</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015772</td>\n",
       "      <td>0.058082</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">42</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.028223</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              train_score  test_score  quality_score  \\\n",
       "no_of_features hidden_layers                                           \n",
       "1              1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.010462       0.030403   \n",
       "4              1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.044799       0.081343   \n",
       "8              1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.017920       0.042807   \n",
       "16             1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.015772       0.058082   \n",
       "42             1                      0.0    0.000000       0.000000   \n",
       "               3                      0.0    0.005929       0.028223   \n",
       "\n",
       "                              time_taken  \n",
       "no_of_features hidden_layers              \n",
       "1              1                     0.0  \n",
       "               3                     0.0  \n",
       "4              1                     0.0  \n",
       "               3                     0.0  \n",
       "8              1                     0.0  \n",
       "               3                     0.0  \n",
       "16             1                     0.0  \n",
       "               3                     0.0  \n",
       "42             1                     0.0  \n",
       "               3                     0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-21T22:05:13.350317Z",
     "start_time": "2017-07-21T22:05:13.266682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1907: RuntimeWarning: invalid value encountered in multiply\n",
      "  lower_bound = self.a * scale + loc\n",
      "/home/ritesh_malaiya/anaconda3/envs/p3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1908: RuntimeWarning: invalid value encountered in multiply\n",
      "  upper_bound = self.b * scale + loc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "no_of_features  hidden_layers\n",
       "1               1                                         (nan, nan)\n",
       "                3                (-0.0324640103685, 0.0867127258719)\n",
       "4               1                                         (nan, nan)\n",
       "                3                 (-0.0576868704905, 0.261172648992)\n",
       "8               1                                         (nan, nan)\n",
       "                3                   (-0.0277953672687, 0.1400032965)\n",
       "16              1                                         (nan, nan)\n",
       "                3                 (-0.0417109765887, 0.185965314488)\n",
       "42              1                                         (nan, nan)\n",
       "                3                (-0.0251932386019, 0.0854370292005)\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fn(x):\n",
    "    #print(x)\n",
    "    return stats.norm.interval(0.95, loc=x.quality_score.mean(), scale=x.quality_score.std())\n",
    "psg.apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/7d1ace18a82178e15ece8fc5252fce88"
  },
  "anaconda-cloud": {},
  "gist": {
   "data": {
    "description": "Hyper parameter tuning",
    "public": false
   },
   "id": "7d1ace18a82178e15ece8fc5252fce88"
  },
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
